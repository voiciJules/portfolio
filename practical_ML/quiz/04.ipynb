{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb59e668-4e51-4685-9359-98cecef2144b",
   "metadata": {},
   "source": [
    "# Quiz 4\n",
    "\n",
    "### Q1. (Multiple Selection)Which of the following statements about RNNs is correct?\n",
    "- Recurrent layers cannot be stacked\n",
    "- Recurrent layers can be stacked into deep RNNs\n",
    "- A recurrent layer only contains one recurrent neuron\n",
    "- A recurrent layer contains one or more recurrent neurons\n",
    "\n",
    "âœ… A recurrent layer contains one or more recurrent neurons\n",
    "RNN ì¸µì—ëŠ” ì—¬ëŸ¬ ìœ ë‹›ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©°, ê° ìœ ë‹›ì€ ì‹œí€€ìŠ¤ ì…ë ¥ì„ ë°˜ë³µì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "âœ… Recurrent layers can be stacked into deep RNNs\n",
    "ì—¬ëŸ¬ RNN ê³„ì¸µì„ ìŒ“ì•„ ë” ê¹Šì€ íŒ¨í„´ê³¼ ë³µì¡í•œ ì‹œí€€ìŠ¤ë¥¼ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### Q2. At each time step t (t>1), a recurrent neuron receives ____.\n",
    "- The input for time step t\n",
    "- The neuron's output at time step t-1\n",
    "- The input for time step t and the neuron's output at time step t-1\n",
    "\n",
    "âœ… ì •ë‹µ: The input for time step t and the neuron's output at time step t-1\n",
    "**Recurrent Neural Networks (RNNs)**ëŠ” ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ë‹¤ë£¨ê¸° ë•Œë¬¸ì—, ê° ì‹œê°„ ë‹¨ê³„(time step)ì—ì„œ ì´ì „ ìƒíƒœì˜ ì •ë³´ë¥¼ í˜„ì¬ ì…ë ¥ê³¼ í•¨ê»˜ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "- ì‹œê°„ ë‹¨ê³„ tì—ì„œ RNNì˜ ë™ì‘:\n",
    "\n",
    "$\n",
    "h_t \\;=\\; f\\!\\bigl(W\\,x_t \\;+\\; U\\,h_{t-1} \\;+\\; b\\bigr)\n",
    "$\n",
    "\n",
    "| ê¸°í˜¸ | ì˜ë¯¸ |\n",
    "|------|------|\n",
    "| $x_t$ | í˜„ì¬ ì‹œì  $t$ì˜ ì…ë ¥ ë²¡í„° |\n",
    "| $h_{t-1}$ | ì´ì „ ì‹œì  $t-1$ì˜ hidden state (ë˜ëŠ” ì¶œë ¥) |\n",
    "| $h_t$ | í˜„ì¬ ì‹œì  $t$ì˜ hidden state (ë˜ëŠ” ì¶œë ¥) |\n",
    "| $W$ | ì…ë ¥ ê°€ì¤‘ì¹˜ í–‰ë ¬ |\n",
    "| $U$ | recurrent ê°€ì¤‘ì¹˜ í–‰ë ¬ |\n",
    "| $b$ | bias ë²¡í„° |\n",
    "| $f$ | í™œì„±í™” í•¨ìˆ˜ (ì˜ˆ: $\\tanh$, $\\text{ReLU}$ ë“±) |\n",
    "\n",
    "â¡ï¸ ì¦‰, í˜„ì¬ ì…ë ¥ê³¼ ì´ì „ ì¶œë ¥ì´ ëª¨ë‘ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "\n",
    "### Q3. (Multiple Selection)Which of the following statements about recurrent layers is incorrect?\n",
    "- A recurrent layer can only output a scalar value\n",
    "- A recurrent layer can only output a vector. The length of the output vector equals the length of an input instance\n",
    "- A recurrent layer can only output a sequence of vectors. The length of the sequence equals the number of time steps configured. The length of each vector equals the length of an input instance\n",
    "\n",
    "âœ”ï¸ ì •ë‹µìœ¼ë¡œ í‹€ë¦° ì§„ìˆ  (Incorrect)\n",
    "âŒ 1. A recurrent layer can only output a scalar value  \n",
    "- í‹€ë¦¼: RNNì˜ ì¶œë ¥ì€ ë³´í†µ ë²¡í„° ë˜ëŠ” ì‹œí€€ìŠ¤ì˜ ë²¡í„°ì…ë‹ˆë‹¤.  \n",
    "- ìŠ¤ì¹¼ë¼(scalar) ì¶œë ¥ì€ ì¼ë°˜ì ì¸ ì„¤ì •ì´ ì•„ë‹ˆë©°, ë³´í†µì€ FC ë ˆì´ì–´ë‚˜ í›„ì†  ì²˜ë¦¬ì—ì„œ ìƒê¹ë‹ˆë‹¤.  \n",
    "\n",
    "âŒ 2. A recurrent layer can only output a vector. The length of the output vector equals the length of an input instance  \n",
    "- í‹€ë¦¼: ì¶œë ¥ ë²¡í„°ì˜ ê¸¸ì´ëŠ” ì…ë ¥ ë²¡í„°ì™€ ë¬´ê´€í•˜ê²Œ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
    "- ì˜ˆ: ì…ë ¥ì´ 10ì°¨ì›ì´ë¼ë„ hidden sizeê°€ 128ì´ë©´ ì¶œë ¥ì€ 128ì°¨ì› ë²¡í„°ì…ë‹ˆë‹¤.  \n",
    "- ë˜í•œ, return_sequences=Trueë¡œ ì„¤ì •í•˜ë©´ ì‹œí€€ìŠ¤ ì „ì²´ ë²¡í„°ë¥¼ ì¶œë ¥í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.  \n",
    "  \n",
    "âœ”ï¸ ì •ë‹µìœ¼ë¡œ ë§ëŠ” ì§„ìˆ  (Correct)\n",
    "âœ… 3. A recurrent layer can only output a sequence of vectors. The length of the sequence equals the number of time steps configured. The length of each vector equals the length of an input instance\n",
    "- ëŒ€ì²´ë¡œ ë§ìŒ (ë‹¨, ì„¤ëª…ì´ ì¡°ê¸ˆ ì• ë§¤í•˜ê¸´ í•¨).\n",
    "- return_sequences=Trueì¸ ê²½ìš°, ê° time stepë§ˆë‹¤ ë²¡í„° ì¶œë ¥.\n",
    "- ê·¸ëŸ¬ë‚˜, ì¶œë ¥ ë²¡í„°ì˜ ê¸¸ì´ëŠ” ë³´í†µ RNNì˜ hidden sizeë¡œ ì„¤ì •ë©ë‹ˆë‹¤.\n",
    "- ì…ë ¥ ê¸¸ì´ì™€ ê°™ì§€ ì•Šì„ ìˆ˜ ìˆìŒ â†’ ì‚´ì§ ë¶€ì •í™•í•˜ì§€ë§Œ ì·¨ì§€ëŠ” ë§ìŒ\n",
    "\n",
    "### Q4. Which of the following statements about hidden state h(t) is correct?\n",
    "- h(t) always equals the output of the recurrent layer at time step t-1\n",
    "- h(t) always equals the output of the recurrent layer at time step t\n",
    "- h(t) always equals the output of the recurrent layer at time step t+1\n",
    "- h(t) could be different from the output of the recurrent layer at time step t\n",
    "\n",
    "âœ… ì •ë‹µ: h(t) always equals the output of the recurrent layer at time step t\n",
    "\n",
    "ğŸ” ì„¤ëª…\n",
    "ğŸ’¡ Hidden state, h(t) ë€?  \n",
    "- RNNì—ì„œ ì‹œê°„ ë‹¨ê³„ tì—ì„œì˜ ë‚´ë¶€ ìƒíƒœ (ë˜ëŠ” ë©”ëª¨ë¦¬)  \n",
    "- ë³´í†µ ë‹¤ìŒ ë‹¨ê³„ ê³„ì‚°ì— ì‚¬ìš©ë¨ (ì¦‰, $h_{t+1}$ ê³„ì‚°ì‹œ ì‚¬ìš©)  \n",
    "- ë™ì‹œì—, í•´ë‹¹ ì‹œì ì˜ ì¶œë ¥ìœ¼ë¡œ ê°„ì£¼ë˜ê¸°ë„ í•¨  \n",
    "\n",
    "ğŸ“Œ ë”°ë¼ì„œ:   \n",
    "- $h_t$ëŠ” recurrent layerì˜ ì¶œë ¥ì´ì, ë‹¤ìŒ ë‹¨ê³„ë¡œ ì „ë‹¬ë˜ëŠ” hidden state  \n",
    "  \n",
    "### Q5. What is the Short-Term Memory Problem for RNNs?\n",
    "- Information gets lost at each time step\n",
    "- RNNs cannot learn meaningful information when trained with limited number of epochs\n",
    "- RNN cannot learn meaningful information on short sequences\n",
    "\n",
    "ğŸ“Œ What is the Short-Term Memory Problem in RNNs?  \n",
    "- RNNì€ ê³¼ê±°ì˜ ì •ë³´ë¥¼ hidden stateë¡œ ë„˜ê¸°ë©° ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµí•˜ì§€ë§Œ, ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ê³¼ê±° ì •ë³´ê°€ ì ì  í¬ë¯¸í•´ì§€ëŠ” ë¬¸ì œê°€ ìƒê¹ë‹ˆë‹¤.\n",
    "- ì´ ë¬¸ì œë¥¼ \"short-term memory problem\" ë˜ëŠ” \"vanishing gradient problem\"ì´ë¼ê³ ë„ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ğŸ“Œ ì™œ ì´ëŸ° ë¬¸ì œê°€ ë°œìƒí•˜ë‚˜ìš”?  \n",
    "- RNNì´ ì˜¤ëœ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•  ë•Œ, ê° time stepì„ ê±°ì¹˜ë©° ì •ë³´ê°€ ë®ì–´ì“°ì´ê³  ì•½í•´ì§‘ë‹ˆë‹¤.\n",
    "- íŠ¹íˆ ê³¼ê±°ì˜ ì¤‘ìš”í•œ ì •ë³´ê°€ ê¸´ ì‹œí€€ìŠ¤ ì¤‘ê°„ì¯¤ì— ë¬»í˜€ë²„ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- ì´ëŠ” ì—­ì „íŒŒ ì¤‘ ê·¸ë˜ë””ì–¸íŠ¸ê°€ ì ì  ì‘ì•„ì ¸ì„œ í•™ìŠµì´ ë˜ì§€ ì•ŠëŠ” í˜„ìƒìœ¼ë¡œ ì—°ê²°ë©ë‹ˆë‹¤.\n",
    "\n",
    "### Q6 - Q8. In an LSTM cell, what does Gate A/B/C do? \n",
    "- An input gate. It decides how much the new input should be incorporated\n",
    "- A forget gate. It decides how much past information should be forgotten\n",
    "- An output gate. It decides what the next short-term state h(t) should be\n",
    "\n",
    "#### ğŸ§  LSTMì˜ 3ê°€ì§€ ê²Œì´íŠ¸: Gate A, B, C ì •ë¦¬\n",
    "##### ğŸ”µ Gate A â€“ **Forget Gate**\n",
    "- ì—­í• : ê³¼ê±°ì˜ ì…€ ìƒíƒœ $ C_{t-1} $ ì¤‘ **ì–¼ë§ˆë‚˜ ìŠì„ì§€ ê²°ì •**\n",
    "- ìˆ˜ì‹:  \n",
    "  $\n",
    "  f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "  $\n",
    "- ì…€ ìƒíƒœ ì—…ë°ì´íŠ¸ì— ë°˜ì˜:\n",
    "  $\n",
    "  C_t = f_t \\cdot C_{t-1} + i_t \\cdot \\tilde{C}_t\n",
    "  $\n",
    "\n",
    "##### ğŸŸ¢ Gate B â€“ **Input Gate**\n",
    "- ì—­í• : í˜„ì¬ ì…ë ¥ $ x_t $ ì¤‘ì—ì„œ **ì–¼ë§ˆë‚˜ ê¸°ì–µì— ë°˜ì˜í• ì§€ ê²°ì •**\n",
    "- êµ¬ì„±:\n",
    "  - ìƒˆë¡œìš´ í›„ë³´ ìƒíƒœ:  \n",
    "    $\n",
    "    \\tilde{C}_t = \\tanh(W_c \\cdot [h_{t-1}, x_t] + b_c)\n",
    "    $\n",
    "  - ì…ë ¥ ê²Œì´íŠ¸ í™œì„±í™”:  \n",
    "    $\n",
    "    i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
    "    $\n",
    "- ê¸°ì–µì— ë°˜ì˜:  \n",
    "  $\n",
    "  i_t \\cdot \\tilde{C}_t\n",
    "  $\n",
    "\n",
    "##### ğŸŸ£ Gate C â€“ **Output Gate**\n",
    "- ì—­í• : ì…€ ìƒíƒœ \\( C_t \\)ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ì¶œë ¥ $ h_t $** ë¥¼ ê²°ì •\n",
    "- ìˆ˜ì‹:\n",
    "  $\n",
    "  o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
    "  $\n",
    "  $\n",
    "  h_t = o_t \\cdot \\tanh(C_t)\n",
    "  $\n",
    "- ë‹¤ìŒ ì‹œì ìœ¼ë¡œ ì „ë‹¬ë˜ëŠ” short-term memoryê°€ ë¨\n",
    "\n",
    "##### âœ… ìš”ì•½í‘œ\n",
    "\n",
    "| ê²Œì´íŠ¸ | ì´ë¦„ | ì—­í•  |\n",
    "|--------|------|------|\n",
    "| Gate A | Forget Gate | ì´ì „ ê¸°ì–µ ì¤‘ ë¬´ì—‡ì„ **ìŠì„ì§€** ê²°ì • |\n",
    "| Gate B | Input Gate | ìƒˆë¡œìš´ ì…ë ¥ ì¤‘ ë¬´ì—‡ì„ **ê¸°ì–µí• ì§€** ê²°ì • |\n",
    "| Gate C | Output Gate | ì…€ ìƒíƒœë¥¼ ë°”íƒ•ìœ¼ë¡œ **ì¶œë ¥í•  ê°’** ê²°ì • |\n",
    "\n",
    "### Q9. How to train RNNs?\n",
    "- Using Backpropagation\n",
    "- Using Batch Normalization\n",
    "- Using Dropout\n",
    "\n",
    "- âœ… **Using Backpropagation**  \n",
    "  â†’ RNNs are trained using **Backpropagation Through Time (BPTT)**, which is an extension of the standard backpropagation algorithm for sequential data. It allows the model to learn temporal dependencies.  \n",
    "- âŒ Using Batch Normalization   \n",
    "  â†’ Batch normalization is **rarely used in RNNs** due to difficulties in applying it consistently across time steps. Alternatives like Layer Normalization are more commonly used for RNNs.   \n",
    "- âŒ Using Dropout   \n",
    "  â†’ Dropout is a **regularization technique**, not a training algorithm. It helps prevent overfitting but is not how the model is trained.  \n",
    "\n",
    "### Q10. RNN ì–´ë””ì— ì‚¬ìš©ë˜ëŠ”ì§€? Time series..ë¼ê³  ë‹µí•¨. \n",
    "  \n",
    "## ìµœì¢…ì ìˆ˜ 8.3 / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a3333d-dbe5-4a9b-ad32-c349a0608adf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
