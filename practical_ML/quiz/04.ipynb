{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb59e668-4e51-4685-9359-98cecef2144b",
   "metadata": {},
   "source": [
    "# Quiz 4\n",
    "\n",
    "### Q1. (Multiple Selection)Which of the following statements about RNNs is correct?\n",
    "- Recurrent layers cannot be stacked\n",
    "- Recurrent layers can be stacked into deep RNNs\n",
    "- A recurrent layer only contains one recurrent neuron\n",
    "- A recurrent layer contains one or more recurrent neurons\n",
    "\n",
    "✅ A recurrent layer contains one or more recurrent neurons\n",
    "RNN 층에는 여러 유닛이 포함되어 있으며, 각 유닛은 시퀀스 입력을 반복적으로 처리합니다.\n",
    "✅ Recurrent layers can be stacked into deep RNNs\n",
    "여러 RNN 계층을 쌓아 더 깊은 패턴과 복잡한 시퀀스를 학습시킬 수 있습니다.\n",
    "\n",
    "### Q2. At each time step t (t>1), a recurrent neuron receives ____.\n",
    "- The input for time step t\n",
    "- The neuron's output at time step t-1\n",
    "- The input for time step t and the neuron's output at time step t-1\n",
    "\n",
    "✅ 정답: The input for time step t and the neuron's output at time step t-1\n",
    "**Recurrent Neural Networks (RNNs)**는 시퀀스 데이터를 다루기 때문에, 각 시간 단계(time step)에서 이전 상태의 정보를 현재 입력과 함께 사용합니다.\n",
    "- 시간 단계 t에서 RNN의 동작:\n",
    "\n",
    "$\n",
    "h_t \\;=\\; f\\!\\bigl(W\\,x_t \\;+\\; U\\,h_{t-1} \\;+\\; b\\bigr)\n",
    "$\n",
    "\n",
    "| 기호 | 의미 |\n",
    "|------|------|\n",
    "| $x_t$ | 현재 시점 $t$의 입력 벡터 |\n",
    "| $h_{t-1}$ | 이전 시점 $t-1$의 hidden state (또는 출력) |\n",
    "| $h_t$ | 현재 시점 $t$의 hidden state (또는 출력) |\n",
    "| $W$ | 입력 가중치 행렬 |\n",
    "| $U$ | recurrent 가중치 행렬 |\n",
    "| $b$ | bias 벡터 |\n",
    "| $f$ | 활성화 함수 (예: $\\tanh$, $\\text{ReLU}$ 등) |\n",
    "\n",
    "➡️ 즉, 현재 입력과 이전 출력이 모두 사용됩니다.\n",
    "\n",
    "### Q3. (Multiple Selection)Which of the following statements about recurrent layers is incorrect?\n",
    "- A recurrent layer can only output a scalar value\n",
    "- A recurrent layer can only output a vector. The length of the output vector equals the length of an input instance\n",
    "- A recurrent layer can only output a sequence of vectors. The length of the sequence equals the number of time steps configured. The length of each vector equals the length of an input instance\n",
    "\n",
    "✔️ 정답으로 틀린 진술 (Incorrect)\n",
    "❌ 1. A recurrent layer can only output a scalar value  \n",
    "- 틀림: RNN의 출력은 보통 벡터 또는 시퀀스의 벡터입니다.  \n",
    "- 스칼라(scalar) 출력은 일반적인 설정이 아니며, 보통은 FC 레이어나 후속  처리에서 생깁니다.  \n",
    "\n",
    "❌ 2. A recurrent layer can only output a vector. The length of the output vector equals the length of an input instance  \n",
    "- 틀림: 출력 벡터의 길이는 입력 벡터와 무관하게 설정할 수 있습니다.  \n",
    "- 예: 입력이 10차원이라도 hidden size가 128이면 출력은 128차원 벡터입니다.  \n",
    "- 또한, return_sequences=True로 설정하면 시퀀스 전체 벡터를 출력할 수도 있습니다.  \n",
    "  \n",
    "✔️ 정답으로 맞는 진술 (Correct)\n",
    "✅ 3. A recurrent layer can only output a sequence of vectors. The length of the sequence equals the number of time steps configured. The length of each vector equals the length of an input instance\n",
    "- 대체로 맞음 (단, 설명이 조금 애매하긴 함).\n",
    "- return_sequences=True인 경우, 각 time step마다 벡터 출력.\n",
    "- 그러나, 출력 벡터의 길이는 보통 RNN의 hidden size로 설정됩니다.\n",
    "- 입력 길이와 같지 않을 수 있음 → 살짝 부정확하지만 취지는 맞음\n",
    "\n",
    "### Q4. Which of the following statements about hidden state h(t) is correct?\n",
    "- h(t) always equals the output of the recurrent layer at time step t-1\n",
    "- h(t) always equals the output of the recurrent layer at time step t\n",
    "- h(t) always equals the output of the recurrent layer at time step t+1\n",
    "- h(t) could be different from the output of the recurrent layer at time step t\n",
    "\n",
    "✅ 정답: h(t) always equals the output of the recurrent layer at time step t\n",
    "\n",
    "🔍 설명\n",
    "💡 Hidden state, h(t) 란?  \n",
    "- RNN에서 시간 단계 t에서의 내부 상태 (또는 메모리)  \n",
    "- 보통 다음 단계 계산에 사용됨 (즉, $h_{t+1}$ 계산시 사용)  \n",
    "- 동시에, 해당 시점의 출력으로 간주되기도 함  \n",
    "\n",
    "📌 따라서:   \n",
    "- $h_t$는 recurrent layer의 출력이자, 다음 단계로 전달되는 hidden state  \n",
    "  \n",
    "### Q5. What is the Short-Term Memory Problem for RNNs?\n",
    "- Information gets lost at each time step\n",
    "- RNNs cannot learn meaningful information when trained with limited number of epochs\n",
    "- RNN cannot learn meaningful information on short sequences\n",
    "\n",
    "📌 What is the Short-Term Memory Problem in RNNs?  \n",
    "- RNN은 과거의 정보를 hidden state로 넘기며 순차적으로 학습하지만, 시간이 지남에 따라 과거 정보가 점점 희미해지는 문제가 생깁니다.\n",
    "- 이 문제를 \"short-term memory problem\" 또는 \"vanishing gradient problem\"이라고도 합니다.\n",
    "\n",
    "📌 왜 이런 문제가 발생하나요?  \n",
    "- RNN이 오랜 시퀀스를 처리할 때, 각 time step을 거치며 정보가 덮어쓰이고 약해집니다.\n",
    "- 특히 과거의 중요한 정보가 긴 시퀀스 중간쯤에 묻혀버릴 수 있습니다.\n",
    "- 이는 역전파 중 그래디언트가 점점 작아져서 학습이 되지 않는 현상으로 연결됩니다.\n",
    "\n",
    "### Q6 - Q8. In an LSTM cell, what does Gate A/B/C do? \n",
    "- An input gate. It decides how much the new input should be incorporated\n",
    "- A forget gate. It decides how much past information should be forgotten\n",
    "- An output gate. It decides what the next short-term state h(t) should be\n",
    "\n",
    "#### 🧠 LSTM의 3가지 게이트: Gate A, B, C 정리\n",
    "##### 🔵 Gate A – **Forget Gate**\n",
    "- 역할: 과거의 셀 상태 $ C_{t-1} $ 중 **얼마나 잊을지 결정**\n",
    "- 수식:  \n",
    "  $\n",
    "  f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "  $\n",
    "- 셀 상태 업데이트에 반영:\n",
    "  $\n",
    "  C_t = f_t \\cdot C_{t-1} + i_t \\cdot \\tilde{C}_t\n",
    "  $\n",
    "\n",
    "##### 🟢 Gate B – **Input Gate**\n",
    "- 역할: 현재 입력 $ x_t $ 중에서 **얼마나 기억에 반영할지 결정**\n",
    "- 구성:\n",
    "  - 새로운 후보 상태:  \n",
    "    $\n",
    "    \\tilde{C}_t = \\tanh(W_c \\cdot [h_{t-1}, x_t] + b_c)\n",
    "    $\n",
    "  - 입력 게이트 활성화:  \n",
    "    $\n",
    "    i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
    "    $\n",
    "- 기억에 반영:  \n",
    "  $\n",
    "  i_t \\cdot \\tilde{C}_t\n",
    "  $\n",
    "\n",
    "##### 🟣 Gate C – **Output Gate**\n",
    "- 역할: 셀 상태 \\( C_t \\)를 바탕으로 **출력 $ h_t $** 를 결정\n",
    "- 수식:\n",
    "  $\n",
    "  o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
    "  $\n",
    "  $\n",
    "  h_t = o_t \\cdot \\tanh(C_t)\n",
    "  $\n",
    "- 다음 시점으로 전달되는 short-term memory가 됨\n",
    "\n",
    "##### ✅ 요약표\n",
    "\n",
    "| 게이트 | 이름 | 역할 |\n",
    "|--------|------|------|\n",
    "| Gate A | Forget Gate | 이전 기억 중 무엇을 **잊을지** 결정 |\n",
    "| Gate B | Input Gate | 새로운 입력 중 무엇을 **기억할지** 결정 |\n",
    "| Gate C | Output Gate | 셀 상태를 바탕으로 **출력할 값** 결정 |\n",
    "\n",
    "### Q9. How to train RNNs?\n",
    "- Using Backpropagation\n",
    "- Using Batch Normalization\n",
    "- Using Dropout\n",
    "\n",
    "- ✅ **Using Backpropagation**  \n",
    "  → RNNs are trained using **Backpropagation Through Time (BPTT)**, which is an extension of the standard backpropagation algorithm for sequential data. It allows the model to learn temporal dependencies.  \n",
    "- ❌ Using Batch Normalization   \n",
    "  → Batch normalization is **rarely used in RNNs** due to difficulties in applying it consistently across time steps. Alternatives like Layer Normalization are more commonly used for RNNs.   \n",
    "- ❌ Using Dropout   \n",
    "  → Dropout is a **regularization technique**, not a training algorithm. It helps prevent overfitting but is not how the model is trained.  \n",
    "\n",
    "### Q10. RNN 어디에 사용되는지? Time series..라고 답함. \n",
    "  \n",
    "## 최종점수 8.3 / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a3333d-dbe5-4a9b-ad32-c349a0608adf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
