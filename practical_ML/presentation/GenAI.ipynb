{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ba64f31-27a4-4d40-8ad5-8fae3267f2f2",
   "metadata": {},
   "source": [
    "# Generative AI(Gen AI)\n",
    "LLMs are large pre-trained models that can be used directly or fine-tuned for specific business datasets. Typically, these models are accessed via APIs, as organizations may not host them locally.\n",
    "  \n",
    "### Key Resources:\n",
    "  - [OpenAI](https://openai.com/) (pay-per-request)\n",
    "  - [Hugging Face Repository] (https://huggingface.co/) (mostly free with a token required)\n",
    "### Frameworks:\n",
    "  - https://python.langchain.com/en/latest/index.html\n",
    "  - https://docs.crewai.com/introduction\n",
    "  - https://langchain-ai.github.io/langgraph/\n",
    "  - https://huggingface.co/docs/transformers/transformers_agents\n",
    "  - https://github.com/jerryjliu/llama_index\n",
    "\n",
    "### Choose Generative AI Models When:\n",
    "  1. Low Confidentiality Concerns:\n",
    "     - Best suited when your data is not very highly sensitive. (If some level of confidentiality is required, ensure third-party providers offer strong encryption and legal agreements such as NDAs or DPAs to safeguard your data.)\n",
    "  2. Large, Complex Datasets:\n",
    "     - Ideal for handling massive or varying datasets when there's limited time or expertise to develop and scale a custom deep learning solution in-house.\n",
    "  3. Multiple Downstream Tasks:\n",
    "     - Effective when supporting various business needs using the same dataset; such as named entity recognition (NER), summarization, classification, etc. where a flexible, general-purpose model streamlines development.\n",
    "    \n",
    "### Generative AI - Search Engines\n",
    "  - https://openai.com/\n",
    "  - https://grok.com/?referrer=website\n",
    "  - https://chat.deepseek.com/\n",
    "  - https://gemini.google.com/app\n",
    "  - https://claude.ai/new\n",
    "  - https://www.phind.com/\n",
    "  - https://www.perplexity.ai/  \n",
    "\n",
    "### AI Coding Assistant\n",
    "  - Cursor AI\n",
    "    - AI-powered integrated development environment (IDE) built to boost developer productivity with cutting-edge artificial intelligence features.\n",
    "    - https://www.cursor.com/en\n",
    "  - Copilot\n",
    "    - GitHub Copilot is an AI-driven coding assistant created through a collaboration between GitHub and OpenAI.\n",
    "    - https://github.com/features/copilot\n",
    "\n",
    "### Generative AI Models\n",
    "#### OpenAI o1 모델\n",
    "- OpenAI에서 제공하는 임베딩(embedding) 모델 중 하나입니다. o1은 OpenAI의 \"OpenAI Ada 001\" 계열 임베딩 모델 중 하나로, 텍스트를 벡터(숫자 배열)로 변환하는 데 최적화되어 있습니다. 빠르고 효율적이면서도, 다양한 자연어 처리 태스크(검색, 분류, 추천 등)에 적합한 범용 임베딩을 제공합니다. 보통 text-embedding-ada-002 같은 이름으로 더 알려져 있는데, o1은 그런 모델의 별칭 또는 내부 코드명으로 쓰이기도 합니다.\n",
    "- 주요 특징\n",
    "  - 고속, 경량화된 임베딩 생성\n",
    "  - 다목적 범용 임베딩 (검색, 의미 비교 등에 사용)\n",
    "  - OpenAI API에서 쉽게 호출 가능\n",
    "\n",
    "#### Instructor Embedding 모델\n",
    "- Instructor Embedding은 HKUNLP 팀에서 개발한 instruction 기반 텍스트 임베딩 모델입니다. 단순히 텍스트만 임베딩하지 않고, **임베딩 목적을 담은 instruction(지시문)**과 텍스트를 같이 넣어 임베딩을 생성합니다. 예를 들어, \"Represent the query for retrieval:\" 와 같은 instruction을 넣고 임베딩을 만들면, 검색용에 최적화된 벡터를 얻을 수 있습니다.\n",
    "- 장점\n",
    "  - 특정 태스크에 맞춘 임베딩 생성 가능 (검색, 요약, 분류 등)\n",
    "  - 다양한 지시문을 활용해 다목적 임베딩\n",
    "  - Hugging Face에서 hkunlp/instructor-xl 등으로 공개되어 있음\n",
    "\n",
    "#### Engage with these LLM models through prompt engineering.\n",
    "- https://learnprompting.org/docs/intro (Recommended)\n",
    "- https://www.promptingguide.ai\n",
    "- https://www.cloudskillsboost.google/paths/118\n",
    "- Prompt Engineering : Design effective prompting techniques that interface with LLMs and other too\n",
    "\n",
    "#### Large Language Model (LLM) :\n",
    "- To access online LLMs, you will need an access token or API key from Hugging Face, Google Cloud and OpenAI.\n",
    "- Obtain a free trial token and/or secure payable API by visiting the following links:\n",
    "  - Hugging Face: https://huggingface.co/docs/hub/security-tokens\n",
    "  - OpenAI: https://platform.openai.com/account/api-keys | https://platform.openai.com/settings/organization/api-keys\n",
    "  - Google Cloud Vertex AI: [Get started for free] https://cloud.google.com/?hl=en\n",
    "\n",
    "- LLMs\n",
    "  - LangChain : https://python.langchain.com/en/latest/index.html\n",
    "  - Crew AI : https://github.com/crewAIInc/crewAI\n",
    "  - Transformers Agent : https://huggingface.co/docs/transformers/transformers_agents\n",
    "  - LlamaIndex : https://github.com/jerryjliu/llama_index\n",
    "  - ChainLit: Build and share LLM apps | https://docs.chainlit.io/overview\n",
    "  - VLLM : Serving LLM | https://github.com/vllm-project/vllm\n",
    "  - H2O LLM Studio : GUI designed for fine-tuning state-of-the-art large language models (LLMs) | https://github.com/h2oai/h2o-llmstudi\n",
    "\n",
    "#### RAG(Retrieval-Augmented Generation)\n",
    "##### RAG is a hybrid AI architecture that combines:\n",
    "- Retrieval systems: Search external knowledge sources (like documents, databases, or the web) to find relevant information.\n",
    "- Generative models: Use that retrieved information to produce accurate, context-aware responses.\n",
    "\n",
    "##### How RAG Works\n",
    "- Input query: User asks a question or provides a prompt.\n",
    "- Retrieval step: The system searches a large corpus (e.g., a knowledge base, documents) to find relevant passages or data.\n",
    "- Augmentation: Retrieved documents are fed into a generative model.\n",
    "- Generation step: The generative model produces an output based on both the input and the retrieved knowledge.\n",
    "- **ppt from prof : Load file -> Split into chunk(Token length) -> Embedding(OpenAI, VertexAI, HuggingFace, InstructorEmbedding) -> Build vectorStore(Chroma, Deep Lake, FAISS, Annoy, etc) -> Retrieval -> Q&A**\n",
    "\n",
    "##### Benefits of RAG\n",
    "- Improved factual accuracy: The generative model can ground responses in real data, reducing hallucinations.\n",
    "- Access to up-to-date info: Retrieval allows use of current data without retraining the model.\n",
    "- Domain adaptability: Easy to swap or update the retrieval corpus for new domains or languages.\n",
    "\n",
    "##### Applications of RAG\n",
    "- Customer support bots that pull answers from product manuals.\n",
    "- Medical assistants referencing medical literature.\n",
    "- Research assistants summarizing scientific papers.\n",
    "- Chatbots with access to proprietary or private databases.\n",
    "\n",
    "##### Example: RAG pipeline in practice\n",
    "- User: “What are the latest updates on climate policy?”\n",
    "- Retrieval: Search recent news and reports.\n",
    "- Generation: Summarize and generate an answer based on retrieved documents.\n",
    "  \n",
    "##### Several different types of document loaders available in LangChain\n",
    "- PDFs\n",
    "- UnstructuredPDFLoader\n",
    "- Social Media\n",
    "- TwitterTweetLoader\n",
    "- Messaging Services\n",
    "- WhatsAppChatLoader\n",
    "- File Types\n",
    "- CSVLoader\n",
    "  \n",
    "##### Embedding\n",
    "- OpenAI (EX :GPT-3.5 Turbo )\n",
    "- HuggingFace Models (Ex: Falcon 40B )\n",
    "- InstructorEmbedding (EX : hkunlp/Instructor-xl)\n",
    "- Google Embedding (EX :Gemini-1.5-Flash-002\n",
    "  \n",
    "##### Types of Chains in LangChain (Stuff, Map-Reduce, Refine and Map-Rerank)\n",
    "- References :\n",
    "  - https://medium.com/@vinusebastianthomas/document-chains-in-langchain-d33c4bdbabd8\n",
    "  - https://medium.com/@minh.hoque/what-are-llm-chains-671b84103ba\n",
    "\n",
    "##### STUFF\n",
    "- It is ideal for applications where documents are small, and only a few are used at a time.\n",
    "- The stuff chain would fail if the document tokens exceed the LLM limit.\n",
    "- Good : It’s quite cheap and it works for all application.\n",
    "- Bad : If we are dealing with a lots of different types of chunks it is not the best practice\n",
    "\n",
    "##### Map-Reduce\n",
    "- Enables the iteration over a list of documents, generating individual outputs for each document, which can later be combined to produce a final result.\n",
    "- Good : Improving efficiency (Parallel processing ) and reducing processing time.\n",
    "  \n",
    "##### Refine Chain\n",
    "- The Refine chain focuses on iterative refinement of the output by feeding the output of one iteration into the next, aiming to enhance the accuracy and quality of the final result\n",
    "- Good: Enhanced accuracy: By refining the output in each iteration, the chain can improve the accuracy and relevance of the final result.\n",
    "- Bad: Increased computational resources | Longer processing time\n",
    "\n",
    "##### Map-Rerank\n",
    "- Can be used for question answering.\n",
    "- It maps over documents, trying to both (a) answer a question, (b) assign a score to how good the answer is. It then picks the answer with the highest score. Map-Rerank is a good choice for question answering tasks where you expect there to be a single\n",
    "simple answer in a single document. For example, you could use Map-Rerank to answer questions about factual topics, such as\n",
    "\"What is the capital of France?\" Map-Rerank is not a good choice for question answering tasks where you expect there to be\n",
    "multiple answers or where the answers are spread out over multiple documents.\n",
    "- For example, you would not use Map-Rerank to answer questions about open-ended topics, such as\n",
    "\"What is the meaning of life?\" or \"What is the best way to solve world hunger?\"\n",
    "  \n",
    "####  RAG - Search Type\n",
    "##### Similarity Search\n",
    "Similarity search selects text chunk vectors that are most similar to the question vector. This is the simplest and most straightforward search method. However, it can sometimes return documents that are not very relevant to the question.\n",
    "\n",
    "##### Maximum Marginal Relevance (MMR)\n",
    "- MMR search optimizes for similarity to query AND diversity among selected documents. This means that MMR search will try to find documents that are both similar to the question and different from each other.\n",
    "- To set search_kwargs in Langchain in search type MMR, you can use the as_retriever() method.\n",
    "- The as_retriever() method takes a dictionary of keyword arguments as its argument. The keyword arguments that you can use are:\n",
    "  - k: The number of documents to return.\n",
    "  - top_k: The number of documents to consider for each iteration of the MMR algorithm.\n",
    "  - alpha: The relevance decay factor.\n",
    "  - beta: The diversity penalty factor\n",
    "\n",
    "##### Multiple PDFs in LLM\n",
    "https://colab.research.google.com/drive/1mIO99-4QWgIKvjgAFj0vvEbQi5xgNCPk?usp=sharing\n",
    "- More info : https://www.youtube.com/watch?v=s5LhRdh5fu4\n",
    "##### LLM - Question Answering on Own Data\n",
    "- https://medium.com/@onkarmishra/using-langchain-for-question-answering-on-own-data-3af0a82789ed\n",
    "##### Levels Of Summarization: Novice to Expert\n",
    "- https://github.com/gkamradt/langchain-tutorials/blob/main/data_generation/5%20Levels%20Of%20Summarization%20%20Novice%20To%20Expert.ipynb\n",
    "##### Advanced - Generative AI with Large Language Models - Deeplearning.AI and Amazon Web Services\n",
    "- https://github.com/amruthaa08/Generative_AI_LLMs/tree/main\n",
    "##### Efficient Large Language Model training with LoRA and Hugging Face\n",
    "- https://www.philschmid.de/fine-tune-flan-t5-peft\n",
    "##### crewAI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf082bd-41fe-4de6-8345-bee02278b196",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
