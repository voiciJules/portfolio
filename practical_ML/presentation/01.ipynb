{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1596b8f7-8817-4ec3-bf1b-ae4c908142a4",
   "metadata": {},
   "source": [
    "# Welcome to Practical ML\n",
    "\n",
    "### What is Deep learning?   \n",
    "\n",
    "AI 라는 큰 범주 안에 머신러닝이 포함되고, 또 머신러닝에 딥러닝이 포함되는데, 각자의 정의를 알아보자.  \n",
    "- AI(Artificial Intelligence) : Any technique that enables computers to mimic human behaviour.\n",
    "- ML(Machine learning) : Ability to learn without explicitly being programmed.\n",
    "- DL(Deep learning) : Extract patterns from data using neural networks.  \n",
    "In other words, AI > ML > DL.\n",
    "\n",
    "#### Conventional ML vs Deep learning  \n",
    "Traditional ML flow : Input -> Feature extraction -> Features -> Traditional ML Algo -> Output  \n",
    "Deep learning flow : Input -> DL Algo -> Output  \n",
    "\n",
    "#### Why Neural Network faded into Deep Learning?  \n",
    "Two main obstacles:  \n",
    "- Vanishing Gradient  Problem\n",
    "- Not approperiate for complex  problems.\n",
    "- Not Many hidden units ( prone to overfiting)\n",
    "즉, 기존의 얕은 신경망 구조는 깊거나 복잡한 문제를 잘 처리하지 못하고, 학습이 잘 이루어지지 않거나 과적합되기 쉬웠기 때문에 딥러닝이 등장하게 되었다는 것.\n",
    "\n",
    "#### Numerical Systems; Deep Learning  \n",
    "- Banknote Authentication and Forgery Detection\n",
    "- Financial Fraud Detection\n",
    "- Bank Embezzlement & Money Laundering\n",
    "- Boost e-commerce Sales\n",
    "- Losing From Disgruntled Customers\n",
    "- Loan Approval Prediction  \n",
    "  \n",
    "#### Image Processing; Deep Learning  \n",
    "- Image Classification\n",
    "- Object Detection\n",
    "- Face Detection and Recognition\n",
    "- Video Summarization\n",
    "- Key Frame Detection in Human Action Videos\n",
    "- Image Super-Resolution  \n",
    "\n",
    "#### Text Understanding; Deep Learning  \n",
    "- Key Word Extraction\n",
    "- Polarity Analysis(감성분석, 긍정 부정등)\n",
    "- Text classification ( Multi classes , Multi Labels)\n",
    "- Multi Documents Summarization\n",
    "- Topic modeling(숨겨진 주제를 자동으로 추출)\n",
    "- Named Entity Recognition(개체명 인식: 사람, 지명 등 고유명사나 특정대상 식별해내는 것)  \n",
    "  \n",
    "####  Object Recognition  \n",
    "https://youtu.be/MPU2HistivI?si=lGAb0xYA7B30QeII  \n",
    "\n",
    "####  Medical imaging and diagnostic  \n",
    "https://youtu.be/toK1OSLep3s?si=tKB0QHR_nb-1WX0g  \n",
    "  \n",
    "####  So, why DL?  \n",
    "1. It works for complex unstructured problems like images, videos, audio, time series and games  \n",
    "2. Performance with more data    \n",
    "3. Auto feature, no/little domain expertise needed\n",
    "\n",
    "####  No Free Lunch Theorem : \n",
    "모든 문제에 대해 항상 잘 작동하는 하나의 최적의 알고리즘은 존재하지 않는다. 어떤 알고리즘이 특정 문제에서는 잘 작동하더라도, 다른 문제에서는 오히려 성능이 나쁠 수 있다는 의미  \n",
    "\n",
    "####  Perceptron  \n",
    "- 인공신경망의 가장 기본적인 단위로, 뇌의 뉴런을 모방한 모델입니다. 즉, 입력값을 받아서 가중치를 곱한 뒤, 그 결과를 더해서 활성화 함수를 통과시켜 출력을 내는 구조입니다.  \n",
    "| 요소                         | 설명                                        |\n",
    "| -------------------------- | ----------------------------------------- |\n",
    "| 입력값 $x_1, x_2, \\dots, x_n$ | 모델에 들어가는 값                                |\n",
    "| 가중치 $w_1, w_2, \\dots, w_n$ | 각 입력에 곱해지는 값                              |\n",
    "| 편향 $b$                     | 출력값을 조정하는 상수                              |\n",
    "| 가중합 $z$                    | $z = \\sum w_i x_i + b$                    |\n",
    "| 활성화 함수                     | 결과를 이진 출력으로 변환 (예: Step, Sigmoid, ReLU 등) |\n",
    "  \n",
    "- 예시  \n",
    "```\n",
    "입력:     x1 = 1, x2 = 0  \n",
    "가중치:   w1 = 0.5, w2 = -0.6  \n",
    "편향:     b = 0.1  \n",
    "가중합:   z = (1 * 0.5) + (0 * -0.6) + 0.1 = 0.6  \n",
    "\n",
    "출력:     활성화 함수 적용 → z > 0 → 출력 1\n",
    "```\n",
    "- 한계  \n",
    "  - 선형적으로 분리 가능한 문제만 해결 가능   \n",
    "  - XOR 문제처럼 비선형 문제는 처리 불가  \n",
    "\n",
    "#### Demo (40 page) - presentation01_demo01_perceptron\n",
    "\n",
    "#### MLP(Multi-Layer Perceptron)  \n",
    "- 다층 퍼셉트론은 하나 이상의 은닉층(hidden layer) 을 포함한 인공신경망으로, 복잡하고 비선형적인 문제도 학습할 수 있습니다\n",
    "- 입력층 -> 은닉층(들) -> 출력층\n",
    "| 구성 요소      | 설명                                  |\n",
    "| ---------- | ----------------------------------- |\n",
    "| 입력값 $x$    | 입력 데이터                              |\n",
    "| 가중치 $w$    | 각 연결마다 곱해지는 학습 가능한 파라미터             |\n",
    "| 편향 $b$     | 출력값 조정                              |\n",
    "| 활성화 함수 $f$ | 비선형성을 부여 (예: ReLU, Sigmoid, Tanh 등) |\n",
    "\n",
    "- 작동 원리\n",
    "  1. 각 층의 뉴런은 이전 층의 출력을 받아 **가중합**을 계산  \n",
    "   `z = ∑ wᵢxᵢ + b`\n",
    "  2. **비선형 활성화 함수**를 적용  \n",
    "   `a = f(z)`\n",
    "  3. 다음 층으로 전달  \n",
    "  4. 출력층에서 **예측값 도출**\n",
    "\n",
    "- 학습 방법\n",
    "  - **순전파 (Forward Propagation)**: 입력값을 통해 예측값 계산\n",
    "  - **오차 계산 (Loss)**: 예측값과 실제값의 차이 계산\n",
    "  - **역전파 (Backpropagation)**: 오차를 기반으로 가중치와 편향을 업데이트\n",
    "    - **경사 하강법 (Gradient Descent)** 사용\n",
    "     \n",
    "- 특징\n",
    "  - XOR 같은 **비선형 문제**도 해결 가능\n",
    "  - **은닉층이 많을수록** 더 복잡한 패턴을 학습 가능  \n",
    "  - **딥러닝(Deep Learning)**의 가장 기본적인 형태  \n",
    "  \n",
    "- 단점\n",
    "  - **과적합(Overfitting)** 위험\n",
    "  - **계산 비용** 증가\n",
    "  - 적절한 **층 수**, **뉴런 수**, **학습률** 설정이 중요\n",
    "\n",
    "#### Loss Functions\n",
    "모델의 예측값과 실제값 사이의 차이(오차)를 계산하는 함수입니다.\n",
    "| 문제 유형                              | 손실 함수 예시                  | 설명                         |\n",
    "| ---------------------------------- | ------------------------- | -------------------------- |\n",
    "| 회귀 (Regression)                    | Mean Squared Error (MSE)  | 예측값과 실제값의 차이 제곱 후 평균       |\n",
    "| 이진 분류 (Binary Classification)      | Binary Cross Entropy      | 두 클래스(0 or 1)의 예측 확률 기반 손실 |\n",
    "| 다중 분류 (Multi-class Classification) | Categorical Cross Entropy | 여러 클래스 중 정답 클래스의 예측 확률에 기반 |\n",
    "  \n",
    "##### Mean Squared Error (MSE)\n",
    "- **사용 분야**: 회귀(Regression) 문제\n",
    "- **공식**:  \n",
    "  `MSE = (1/n) ∑ (yᵢ - ŷᵢ)²`  \n",
    "- **설명**:  \n",
    "  실제값과 예측값의 차이를 제곱하여 평균한 값  \n",
    "- **특징**:  \n",
    "  큰 오차에 더 민감하게 반응 → 이상치(Outlier)에 민감함\n",
    "\n",
    "##### Categorical Cross Entropy\n",
    "- **사용 분야**: 다중 클래스 분류(Classification) 문제\n",
    "- **공식**:  \n",
    "  `Loss = -∑ yᵢ * log(ŷᵢ)`  \n",
    "- **설명**:  \n",
    "  정답 레이블에 해당하는 예측 확률의 로그 값을 음수로 취한 후 평균  \n",
    "- **특징**:  \n",
    "  예측값이 정답과 가까울수록 손실이 작아짐  \n",
    "  One-hot encoding된 레이블에 주로 사용\n",
    "\n",
    "#### Activation functions\n",
    "활성화 함수(Activation Function)는 인공 신경망에서 뉴런이 출력 신호를 결정하는 함수입니다. 즉, 입력된 값을 비선형적으로 변환해 다음 층으로 전달할지를 결정하는 중요한 역할을 한다.\n",
    "\n",
    "##### 왜 필요한가? \n",
    "- 비선형성 도입\n",
    "  - 선형 함수만 사용하면 아무리 층을 깊게 쌓아도 전체 모델은 선형 함수와 같아집니다.\n",
    "    → 복잡한 문제(예: 이미지, 음성, 자연어 처리 등)를 해결할 수 없음\n",
    "- 정보 필터링\n",
    "  - 각 뉴런이 입력값을 걸러내어 모델이 더 유의미한 특징을 학습하도록 도와줌\n",
    " \n",
    "| 이름                               | 수식                                   | 특징                                                           | 사용 예시              |\n",
    "| -------------------------------- | ------------------------------------ | ------------------------------------------------------------ | ------------------ |\n",
    "| **ReLU (Rectified Linear Unit)** | `f(x) = max(0, x)`                   | - 계산 간단<br>- Gradient Vanishing 문제 해결<br>- 음수 영역 출력 0        | CNN, DNN 등 거의 모든 곳 |\n",
    "| **Sigmoid**                      | `f(x) = 1 / (1 + e^(-x))`            | - 출력 범위 (0, 1)<br>- 확률처럼 해석 가능<br>- Gradient Vanishing 문제 있음 | 이진 분류, 출력층         |\n",
    "| **Tanh**                         | `f(x) = (e^x - e^-x) / (e^x + e^-x)` | - 출력 범위 (-1, 1)<br>- Zero-centered                           | 순환 신경망(RNN) 등      |\n",
    "| **Leaky ReLU**                   | `f(x) = x if x>0 else 0.01x`         | - ReLU의 단점인 \"죽은 뉴런\" 문제 보완                                    | 고급 DNN 모델          |\n",
    "| **Softmax**                      | `f(xᵢ) = e^(xᵢ) / Σe^(xⱼ)`           | - 여러 클래스의 확률 출력<br>- 총합 = 1                                  | 다중 분류 출력층          |\n",
    "\n",
    "#### Optimizer - Backpropagation\n",
    "##### Stochastic Gradient Descent (SGD)\n",
    "**Stochastic Gradient Descent(SGD)**는 확률적 경사 하강법으로도 불리며, 점진적(순차적) 경사 하강법이라고도 알려져 있다. 이 방법은 목적 함수(objective function)를 확률적으로 반복하여 근사하는 방식이며, 미분 가능한 함수의 최적값을 찾기 위한 경사 하강법의 한 형태입니다.  \n",
    "일반적인 **경사하강법(Gradient Descent)**은 전체 데이터셋을 사용해 손실 함수의 기울기를 계산하고, 이 기울기를 이용해 파라미터를 업데이트합니다. 하지만 **Stochastic Gradient Descent (SGD)**는 다음과 같은 방식으로 동작합니다:\n",
    "- 전체 데이터가 아니라, 무작위로 선택한 1개의 데이터 샘플만 사용해서 기울기를 계산합니다.  \n",
    "- 그래서 업데이트가 더 자주 일어납니다.  \n",
    "- \"Stochastic\"은 \"확률적인, 무작위의\"라는 뜻입니다.\n",
    "\n",
    "| 방식                | 설명                       | 장점            | 단점           |\n",
    "| ----------------- | ------------------------ | ------------- | ------------ |\n",
    "| **Batch GD**      | 전체 데이터셋 사용               | 안정적, 정확한 방향   | 느리고 메모리 많이 씀 |\n",
    "| **SGD**           | 1개 샘플로 매번 업데이트           | 빠르고 온라인 학습 가능 | 변동이 크고 진동 가능 |\n",
    "| **Mini-batch GD** | 소량 데이터(예: 32, 64개)로 업데이트 | 속도와 안정성의 균형   | 가장 널리 쓰임     |\n",
    "\n",
    "#### 장점\n",
    "- 계산이 빠름 → 대규모 데이터셋에 적합\n",
    "- 온라인 학습 가능 (실시간 데이터 처리)\n",
    "- 종종 더 좋은 일반화 성능을 제공\n",
    "\n",
    "#### 단점\n",
    "- 손실 값이 진동하거나 불안정\n",
    "- 최적점에 정확히 수렴하기 어려움\n",
    "- 이를 보완하기 위해 Momentum, Adam, RMSprop 같은 개선된 알고리즘들이 자주 사용됩니다.\n",
    "\n",
    "#### 딥러닝 학습의 전체 흐름 속에서의 SGD 위치\n",
    "1. 데이터 준비\n",
    "   - 데이터 수집, 정제, 전처리, 나누기 (train/test)\n",
    "2. 모델 설계\n",
    "   - 입력층, 은닉층, 출력층 설계 (신경망 구조)\n",
    "3. 순전파 (Forward Propagation)\n",
    "   - 입력 데이터를 모델에 넣어 예측값(ŷ)을 계산\n",
    "4. 손실 계산 (Loss Calculation)\n",
    "   - 예측값(ŷ)과 실제값(y)의 차이로 손실(loss) 계산\n",
    "   - 예: 회귀 문제 → MSE, 분류 문제 → CrossEntropy  \n",
    "5. **역전파 (Backpropagation) + 최적화(Optimization)**\n",
    "   - 이 단계에서 **SGD**가 사용됨!!!  \n",
    "   - 손실 함수를 기준으로 **가중치 w, b의 기울기(gradient)를 계산**  \n",
    "   - **SGD가 그 기울기를 따라 가중치를 업데이트** 함  \n",
    "      `(w := w - η * ∇Loss)`\n",
    "6. 반복 (Epochs)\n",
    "   - 위 과정을 여러 번 반복하면서 모델이 점점 더 나은 예측을 하게 됨\n",
    "7. 평가 및 테스트\n",
    "   - 학습된 모델을 검증/테스트 데이터로 평가\n",
    "\n",
    "#### Gradient Descent\n",
    "**Gradient Descent**는 손실 함수를 최소화하기 위해 사용되는 최적화 알고리즘입니다.   \n",
    "모델이 예측한 값과 실제 값 사이의 오차(손실)를 줄이기 위해 **기울기(gradient)**를 따라 조금씩 파라미터를 업데이트합니다.  \n",
    "\n",
    "##### Gradient Descent Algorithm\n",
    "1. Initialize weights randomly\n",
    "2. Loop until convergence\n",
    "   - Compute gradient of **a batch** with Backpropagation\n",
    "   - Update weights using -gradient * **learning rate**\n",
    "3. Return weights (halt)\n",
    "\n",
    "##### 수식\n",
    "$\n",
    "\\theta := \\theta - \\eta \\cdot \\nabla_\\theta J(\\theta)\n",
    "$\n",
    "\n",
    "- $ \\theta $ : 모델의 파라미터 (가중치 등)\n",
    "- $ \\eta $ : 학습률 (learning rate)\n",
    "- $ \\nabla_\\theta J(\\theta) $ : 파라미터에 대한 손실 함수 $ J(\\theta) $의 기울기\n",
    "\n",
    "#### Chain Rule (연쇄 법칙)\n",
    "**Chain Rule**은 함수가 여러 개의 함수로 중첩되어 있을 때, 미분을 단계적으로 계산할 수 있게 해주는 법칙입니다. **역전파(Backpropagation)** 과정에서 매우 중요하게 사용됩니다.\n",
    "\n",
    "##### 수식 예시\n",
    "함수 $ z = f(g(x)) $ 일 때,\n",
    "\n",
    "$\n",
    "\\frac{dz}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dx}\n",
    "$\n",
    "\n",
    "예: $ z = \\sin(x^2) $ 이라면,\n",
    "\n",
    "$\n",
    "\\frac{dz}{dx} = \\cos(x^2) \\cdot 2x\n",
    "$\n",
    "\n",
    "딥러닝에서는 복잡한 신경망 구조의 파라미터들을 학습할 때,\n",
    "- **손실(loss)**을 미분해서\n",
    "- 각 층의 가중치들을 업데이트하는 데\n",
    "- **Chain Rule + Gradient Descent** 조합이 핵심적으로 사용됩니다.\n",
    "  \n",
    "#### Keras\n",
    "API - Deep learning for humans\n",
    "\n",
    "##### DEMO 77 page : presentation01_demo02_keras_introduction\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b770b1-8706-4113-8e93-373e4176819d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
