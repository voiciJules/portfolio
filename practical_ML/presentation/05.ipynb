{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28e3ca9b-a480-4f88-9184-ed3b8db0c6ca",
   "metadata": {},
   "source": [
    "# Module 5. Representations, Autoencoders and Generative Models\n",
    "\n",
    "## ✅ Autoencoder가 뭐 하는 거야?\n",
    "\n",
    "**한마디로 말하면:**  \n",
    "👉 *\"입력을 압축해서 다시 똑같이 복원해 보는 네트워크\"*\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 조금 더 풀어서\n",
    "\n",
    "#### 1️⃣ 입력 데이터를 넣는다\n",
    "예: 고양이 사진, 숫자 이미지 등\n",
    "\n",
    "#### 2️⃣ \"잠재 공간(latent space)\"이라고 불리는 아주 작은 공간으로 압축한다\n",
    "- 데이터를 작고 간단하게 표현\n",
    "- 핵심 정보만 남기는 느낌\n",
    "\n",
    "#### 3️⃣ 다시 그 작은 표현으로부터 원래 입력을 복원한다\n",
    "- 압축된 정보를 이용해서 최대한 원래랑 똑같이 출력하려고 함\n",
    "\n",
    "### 🗜️ 왜 압축하나?\n",
    "- 중요한 특징만 뽑아내려고\n",
    "- 노이즈 제거, 데이터 압축, 특징 추출 등에 활용\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ AutoEncoder의 4가지 주요 파트\n",
    "\n",
    "### 1️⃣ Encoder (인코더)\n",
    "- **무엇을 하나요?**  \n",
    "  원래의 입력 데이터를 **점점 작은 차원**으로 압축합니다.\n",
    "- **예를 들어서?**  \n",
    "  고양이 사진(큰 이미지)을 → 작은 벡터 형태로 변환\n",
    "- **왜 필요해요?**  \n",
    "  입력 데이터에서 중요한 특징(핵심 정보)만 추출하기 위해\n",
    "\n",
    "### 2️⃣ Bottleneck (Latent Space, 잠재 공간)\n",
    "- **무엇을 하나요?**  \n",
    "  **가장 작은 차원**으로 압축된 상태\n",
    "- **왜 bottleneck이라고 부르나요?**  \n",
    "  병목 지점처럼 데이터를 강제로 통과시키기 때문에\n",
    "- **역할?**  \n",
    "  데이터를 요약하는 **잠재 표현(latent representation)** 을 담는다\n",
    "- **중요한 점?**  \n",
    "  이 공간을 통해 특징 압축, 노이즈 제거, 데이터 구조 이해 등 다양한 응용 가능\n",
    "\n",
    "### 3️⃣ Decoder (디코더)\n",
    "- **무엇을 하나요?**  \n",
    "  Bottleneck에서 얻은 작은 벡터를 **다시 원래 데이터의 형태로 복원**합니다.\n",
    "- **예를 들어서?**  \n",
    "  작은 벡터 → 원래 고양이 사진으로 재구성\n",
    "- **역할?**  \n",
    "  얼마나 잘 복원할 수 있는지 평가하고, 네트워크를 학습시킨다\n",
    "\n",
    "### 4️⃣ Reconstruction Loss (재구성 손실)\n",
    "- **무엇을 하나요?**  \n",
    "  **원래 입력 데이터와 복원된 데이터 간의 차이**를 계산\n",
    "- **예를 들어서?**  \n",
    "  원본 이미지와 디코더가 복원한 이미지가 얼마나 비슷한지를 비교\n",
    "- **왜 필요해요?**  \n",
    "  차이가 작을수록, 즉 **손실이 작을수록** 네트워크가 잘 학습되고 있다고 판단\n",
    "- **어떻게 사용되나요?**  \n",
    "  네트워크를 업데이트하는 데 사용되는 핵심 지표\n",
    "\n",
    "## 한 줄 요약\n",
    "> Encoder가 데이터를 압축하고, Bottleneck에 저장하며, Decoder가 복원하고, Reconstruction Loss로 복원 성능을 평가한다!\n",
    "\n",
    "- Encoder와 Decoder는 여러 겹(여러 층)으로 쌓여 있지만, 중간에 압축된 Bottleneck 부분(잠재 공간)은 보통 하나의 층만 쓴다.\n",
    "- 이 중간 층(잠재 공간)에 몇 개의 뉴런을 쓸지는 정해진 공식이 없고, 실험을 통해 (예를 들어 reconstruction loss나 downstream task 성능을 보면서) 찾아야 한다는 뜻이다.\n",
    "\n",
    "## 📄 Autoencoder 종류와 설명\n",
    "### ✨ 공통: BackPropagation to minimize reconstruction loss\n",
    "- Autoencoder는 **입력 데이터를 압축하고 다시 복원**하면서, 복원된 출력과 원래 입력의 차이를 최소화하는 **reconstruction loss**를 줄이도록 **BackPropagation (역전파)**을 사용한다.\n",
    "\n",
    "### 🔹 Vanilla Autoencoder\n",
    "- **구조**: 가장 기본적인 형태. Encoder와 Decoder가 각각 완전 연결층 (Fully connected layer)으로만 이루어져 있음.\n",
    "- **목적**: 단순히 데이터 압축과 복원.\n",
    "- **한계**: 복잡한 패턴이나 이미지 같은 공간 정보를 잘 표현하기 어려움.\n",
    "\n",
    "### 🔹 Multilayer Autoencoder\n",
    "- **구조**: Encoder와 Decoder에 여러 층(Deep structure)을 추가.\n",
    "- **장점**: 더 복잡한 비선형 변환을 학습할 수 있음.\n",
    "- **활용**: 기본 autoencoder보다 더 정교한 특징 추출에 사용.\n",
    "\n",
    "### 🔹 Convolutional Autoencoder\n",
    "- **구조**: Encoder와 Decoder에 Convolution layer(합성곱 층) 사용.\n",
    "- **장점**: 이미지나 시각적 데이터의 **공간적(local) 패턴**을 잘 보존.\n",
    "- **활용**: 이미지 노이즈 제거, 이미지 압축 등.\n",
    "\n",
    "### 🔹 Regularized Autoencoder\n",
    "#### ▪ Sparse Autoencoder\n",
    "- **구조**: 숨은 층의 활성화(activation)에 sparsity constraint(희소성 제약)를 추가.\n",
    "- **장점**: 중요한 특징만 표현하도록 유도 → feature selection 효과.\n",
    "\n",
    "#### ▪ Denoising Autoencoder\n",
    "- **구조**: 입력 데이터에 일부러 노이즈를 추가하고, 깨끗한 원본을 복원하도록 학습.\n",
    "- **장점**: 더 견고하고 일반화된 표현(robust representation)을 학습.\n",
    "\n",
    "### 🔹 Recurrent Autoencoder\n",
    "- **구조**: Encoder와 Decoder에 RNN(LSTM, GRU 등)을 사용.\n",
    "- **장점**: 순차 데이터(Sequence data) 처리가 가능.\n",
    "- **활용**:\n",
    "  - **비디오**: 프레임 순서 보존하면서 압축.\n",
    "  - **시계열(Time series)**: 시계열 이상 탐지, 예측 등.\n",
    "\n",
    "## ✅ 요약\n",
    "\n",
    "| 종류              | 특징                           | 주요 활용 예시                |\n",
    "|-----------------|-----------------------------|---------------------------|\n",
    "| Vanilla        | 기본 구조                    | 단순 압축, 시각화       |\n",
    "| Multilayer     | 더 깊은 구조              | 복잡한 데이터 표현   |\n",
    "| Convolutional | 합성곱 층 사용          | 이미지, 비주얼 데이터 |\n",
    "| Regularized   | 제약 조건 추가          | 희소 표현, 노이즈 제거 |\n",
    "| Recurrent    | 순차적 구조              | 영상, 시계열 분석      |\n",
    "\n",
    "---\n",
    "\n",
    "## 📄 Autoencoder 추가 개념 정리\n",
    "### 🔥 Types: Undercomplete vs Overcomplete\n",
    "#### ▪ Undercomplete Autoencoder\n",
    "- **정의**: 잠재 공간(latent space)의 차원이 **입력 차원보다 작은** 경우.\n",
    "- **목적**: 데이터를 압축(차원 축소)하면서 핵심적인 특징만 추출.\n",
    "- **장점**: 과적합(overfitting)을 방지하고, 잡음을 줄이며 중요한 정보만 남김.\n",
    "- **예시**: 이미지나 데이터의 주요 구조만 남기고 나머지는 버리는 경우.\n",
    "\n",
    "#### ▪ Overcomplete Autoencoder\n",
    "- **정의**: 잠재 공간의 차원이 **입력 차원보다 큰** 경우.\n",
    "- **목적**: 더 풍부하고 다양한 표현 학습.\n",
    "- **위험**: 단순히 입력을 복사하게 되어 무의미한 학습이 될 수 있음 → 규제(regularization)가 필요 (예: sparsity, noise 등).\n",
    "- **활용**: 복잡한 패턴이나 고차원 데이터에서 풍부한 feature를 뽑아내고 싶을 때.\n",
    "\n",
    "### 💡 Autoencoder 구현 방식\n",
    "#### ▪ Feedforward Autoencoder (MLP 기반)\n",
    "- **구성**: 완전 연결층(fully connected layer)을 이용한 다층 퍼셉트론 (MLP).\n",
    "- **활용**: 일반적인 tabular data, 간단한 이미지 (flatten된 벡터).\n",
    "\n",
    "#### ▪ LSTM Autoencoder\n",
    "- **구성**: LSTM (Long Short-Term Memory) 또는 다른 RNN 기반 layer 사용.\n",
    "- **활용**: 시계열 데이터, 자연어, 비디오 등 **순차 데이터**.\n",
    "- **특징**: 시점 간의 의존성(temporal dependency)을 학습할 수 있음.\n",
    "\n",
    "#### ▪ CNN Autoencoder\n",
    "- **구성**: 합성곱(Convolution) layer를 Encoder/Decoder에 사용.\n",
    "- **활용**: 이미지, 음성, 2D 패턴 등 공간적 정보를 보존해야 할 때.\n",
    "- **특징**: 국소적(local) 특징 추출에 강함.\n",
    "\n",
    "### ⚡ Autoencoder without non-linearity ≈ PCA\n",
    "- **설명**:\n",
    "  - Autoencoder에서 **activation function (예: ReLU, sigmoid, tanh 등)**을 제거하면, Encoder와 Decoder가 **선형(linear) 변환**만 수행.\n",
    "  - 이 경우, Autoencoder는 **입력 데이터를 선형적으로 압축**하는데, 이는 **PCA (Principal Component Analysis)**와 본질적으로 같음.\n",
    "- **즉**:\n",
    "  - linear autoencoder = PCA\n",
    "  - **PCA**는 데이터의 분산을 최대화하는 축을 찾아 데이터 차원을 축소함.  \n",
    "  - **Autoencoder**는 reconstruction error를 최소화하도록 학습하지만, activation 없이 선형 변환만 쓰면 결국 같은 결과를 냄.\n",
    "\n",
    "Autoencoder types : (not limited to … )\n",
    "https://towardsdatascience.com/deep-inside-autoencoders-7e41f319999f\n",
    "https://towardsdatascience.com/auto-encoder-what-is-it-and-what-is-it-used-for-part-1-3e5c6f017726\n",
    "코드 실행해볼것.\n",
    "\n",
    "아래 페이지에 대한 해석\n",
    "Adding a sparsity constraint on the encoded representations.\n",
    "In Keras, this can be done by adding an activity_regularizer to our Dense layer.\n",
    "Fewer units would \"fire\" at a given time.\n",
    "```python\n",
    "encoded = layers.Dense(encoding_dim, activation='relu',\n",
    "activity_regularizer=regularizers.l1(10e-5))(input_img)\n",
    "```\n",
    "With the added regularization the model is less likely to overfit and can be trained with more epoch number.\n",
    "  \n",
    "## 🧬 Adding a Sparsity Constraint on Encoded Representations\n",
    "### 💡 개념\n",
    "- **Sparsity constraint**는 잠재 공간(latent space)에 희소성 제약을 걸어, **소수의 뉴런만 활성화**되도록 유도합니다.\n",
    "- 이렇게 하면 모델이 **더 간결한 feature 표현**을 학습하고, **과적합(overfitting)**을 줄일 수 있습니다.\n",
    "\n",
    "### ⚡ Keras에서 적용 방법\n",
    "```python\n",
    "from keras import layers, regularizers\n",
    "\n",
    "encoded = layers.Dense(\n",
    "    encoding_dim,\n",
    "    activation='relu',\n",
    "    activity_regularizer=regularizers.l1(10e-5)\n",
    ")(input_img)\n",
    "```\n",
    "- activity_regularizer: layer의 출력(activation)에 대한 정규화 규제 추가.\n",
    "- regularizers.l1(10e-5): L1 penalty를 추가해 뉴런이 0에 가까워지도록 유도 → 희소성 증가.\n",
    "  \n",
    "### 효과\n",
    "- 소수의 뉴런만 \"fire\" → 나머지는 0에 가까움.\n",
    "- 모델의 복잡성을 줄이고, 일반화 성능을 향상.\n",
    "- 과적합 위험 감소 → 더 많은 epoch 동안 안정적으로 학습 가능.\n",
    "\n",
    "## LSTM 기반 Autoencoder 설명\n",
    "### 1️⃣ 입력은 시퀀스 (sequence)\n",
    "- 데이터가 시간에 따라 나열된 연속된 값들 (예: 시계열, 문장, 음성 등)\n",
    "- 예: `[x1, x2, x3, ..., xT]`\n",
    "\n",
    "### 2️⃣ LSTM Encoder\n",
    "- 입력 시퀀스를 받아서 **전체 시퀀스 정보를 담은 하나의 벡터(컨텍스트 벡터)**로 변환\n",
    "- 즉, T개의 시점(time-step)을 거쳐 마지막에 **고정 길이 벡터** 생성\n",
    "\n",
    "### 3️⃣ 벡터 반복 (Repeat Vector)\n",
    "- 이 고정 길이 벡터를 디코더의 입력 시퀀스 길이(n)만큼 **복제(repeat)**\n",
    "- 예: 고정 벡터 → `[v, v, v, ..., v]` (길이 n)\n",
    "\n",
    "### 4️⃣ LSTM Decoder\n",
    "- 반복된 벡터 시퀀스를 받아, 이를 다시 원래 시퀀스 또는 목표 시퀀스로 변환\n",
    "- 즉, **고정 벡터 → 출력 시퀀스 복원**\n",
    "\n",
    "### 요약\n",
    "| 단계           | 역할                                        |\n",
    "|--------------|-------------------------------------------|\n",
    "| 입력           | 시간에 따른 순서가 있는 시퀀스 데이터                      |\n",
    "| LSTM Encoder | 시퀀스 → 고정 길이 벡터 (전체 정보 압축)               |\n",
    "| Repeat Vector | 고정 벡터를 출력 시퀀스 길이만큼 반복                        |\n",
    "| LSTM Decoder | 반복된 벡터 → 원래 시퀀스 또는 복원된 시퀀스로 변환          |\n",
    "\n",
    "### 한마디 정리\n",
    "\n",
    "> LSTM Autoencoder는 시퀀스 데이터를 한 벡터로 압축했다가, 그 벡터를 반복해서 다시 시퀀스로 복원하는 모델입니다.\n",
    "\n",
    "----\n",
    "A Gentle Introduction to LSTM Autoencoders\n",
    "https://machinelearningmastery.com/lstm-autoencoders/\n",
    "  \n",
    "위에 url 내용 요약\n",
    "## LSTM Autoencoders – Machine Learning Mastery 글 요약\n",
    "### 1. LSTM Autoencoder란?\n",
    "- **Autoencoder**는 입력 데이터를 압축하고 다시 복원하는 신경망.\n",
    "- LSTM Autoencoder는 **순차적 시계열 데이터**를 다루기 위해 LSTM(Recurrent Neural Network)을 활용.\n",
    "\n",
    "### 2. LSTM Autoencoder 구조\n",
    "- **Encoder**: 입력 시퀀스를 받아 마지막 타임스텝의 은닉 상태(hidden state)를 생성. 이 벡터는 시퀀스 전체의 요약 정보를 담음.\n",
    "- **Decoder**: 이 고정 길이 벡터를 초기 상태로 사용해, 원래 시퀀스를 재구성하거나 출력 시퀀스를 생성.\n",
    "\n",
    "### 3. 왜 LSTM Autoencoder를 사용하는가?\n",
    "- 시계열 데이터, 텍스트, 음성, 동영상 등 **순차 데이터**에 효과적.\n",
    "- 비지도학습(unsupervised learning) 방식으로 **특징 추출(feature extraction)** 가능.\n",
    "- **이상 탐지(anomaly detection)**에도 활용 가능 (재구성 오류 기반).\n",
    "\n",
    "### 4. LSTM Autoencoder 구현 주요 포인트\n",
    "- 입력 시퀀스 크기(shape)를 명확히 지정해야 함.\n",
    "- Encoder는 시퀀스를 하나의 고정 벡터로 압축.\n",
    "- Decoder는 이 벡터를 반복(repeat)하여 시퀀스로 복원.\n",
    "\n",
    "### 5. 간단한 예제 코드 (Keras)\n",
    "- LSTM layers를 활용해 encoder-decoder 구조 구성.\n",
    "- RepeatVector, TimeDistributed(Dense) 레이어 사용.\n",
    "\n",
    "### 6. 활용 사례\n",
    "- 이상 탐지: 정상 데이터 학습 후 재구성 오류를 통해 비정상 여부 판단.\n",
    "- 시계열 데이터 압축 및 복원.\n",
    "- 특징 벡터 추출 후 다른 모델에 활용.\n",
    "\n",
    "### 요약\n",
    "| 항목                 | 설명                                    |\n",
    "|--------------------|---------------------------------------|\n",
    "| 목적                 | 시계열 데이터를 효과적으로 압축, 복원            |\n",
    "| 구조                 | LSTM encoder + 고정 벡터 + LSTM decoder       |\n",
    "| 주요 활용             | 이상 탐지, 특징 추출, 시계열 데이터 압축          |\n",
    "| 장점                 | 순차 정보 유지, 비지도학습 가능                   |\n",
    "\n",
    "---\n",
    "Loss Function in AutoEncoder\n",
    "“Denoising autoencoders (DAEs) consist of an encoder and decoder which may be trained simultaneously to minimise a loss (function) between an input and the reconstruction of a corrupted version of the input. There are two common loss functions used for training autoencoders, these include the mean-squared error (MSE) and the binary cross-entropy (BCE).\"\n",
    "Scale between 0 and 1 ---- > BCE is highly recommended ( image processing )\n",
    "\n",
    "https://arxiv.org/pdf/1708.08487.pdf\n",
    "\n",
    "--- \n",
    "## Autoencoder with tied weights\n",
    "### 💡 개념\n",
    "- **Tied weights**는 encoder와 decoder가 **동일한 가중치를 공유**하도록 하는 기법입니다.\n",
    "- decoder의 가중치는 encoder 가중치의 **전치(transpose)** 형태로 사용됩니다.\n",
    "\n",
    "### 🟢 Form of parameter sharing\n",
    "- encoder와 decoder가 같은 weight set을 공유.\n",
    "- 일반적인 autoencoder는 encoder와 decoder 각각 별도의 가중치를 사용하지만, tied weights를 사용하면 **파라미터 수가 크게 줄어듦**.\n",
    "\n",
    "### 🔁 Decoder weights are the transpose of the encoder weights\n",
    "- 예를 들어, encoder weight가 \\( W \\)라면, decoder weight는 \\( W^T \\).\n",
    "- decoder는 encoder의 \"역방향\" 역할.\n",
    "\n",
    "### ⚡ 효과\n",
    "#### 📉 파라미터 수 감소\n",
    "- 모델의 복잡성을 줄이고, **메모리 사용량과 저장 공간** 절약.\n",
    " \n",
    "#### 🚀 Speed\n",
    "- 파라미터 수가 줄어 계산 속도가 **더 빨라짐**.\n",
    "\n",
    "#### 🙅‍♂️ Overfitting 방지\n",
    "- 불필요한 파라미터가 줄어들어 **과적합 방지** 효과.\n",
    "\n",
    "### ✅ 한 줄 요약\n",
    "> **\"Tied weights는 encoder와 decoder가 동일한 가중치를 공유하도록 하여 파라미터를 줄이고, 속도를 높이며, 과적합을 방지하는 기법이다.\"**\n",
    "\n",
    "### 🟢 요약 표\n",
    "| 항목                   | 설명                                    |\n",
    "|----------------------|---------------------------------------|\n",
    "| 개념                  | encoder & decoder weight 공유        |\n",
    "| decoder weight 형태 | encoder weight의 전치(transpose)    |\n",
    "| 장점                  | 파라미터 수 감소, 속도 증가, 과적합 방지 |\n",
    "\n",
    "---\n",
    "\n",
    "## Variational Autoencoder (VAE) 설명\n",
    "### 💡 개념\n",
    "- **Variational Autoencoder**는 **확률적 생성 모델(Probabilistic Generative Model)**입니다.\n",
    "- 일반적인 Autoencoder는 단순히 데이터를 압축하고 복원하는 데 초점을 두지만, VAE는 **데이터의 분포를 학습**하여 새로운 데이터를 생성할 수 있도록 설계됩니다.\n",
    "\n",
    "### 🟢 핵심 아이디어\n",
    "- **잠재 공간(latent space)**에 확률 분포(주로 정규분포)를 가정.\n",
    "- Encoder는 입력 데이터를 **분포의 평균(μ)과 표준편차(σ)**로 매핑.\n",
    "- Decoder는 이 분포에서 샘플링된 값을 사용해 데이터를 생성.\n",
    "\n",
    "### 🔥 주요 구성\n",
    "#### 1️⃣ Encoder\n",
    "- 입력 $x$를 받아 잠재 공간의 **평균 벡터 $\\mu$** 와 **표준편차 벡터 $\\sigma$** 를 출력.\n",
    "\n",
    "#### 2️⃣ Reparameterization Trick\n",
    "- $ z = \\mu + \\sigma \\times \\epsilon $\n",
    "- $\\epsilon$은 표준 정규분포에서 샘플링된 noise.\n",
    "- 이 트릭 덕분에 역전파(backpropagation)가 가능.\n",
    "\n",
    "#### 3️⃣ Decoder\n",
    "- 샘플링된 $z$를 이용해 입력 데이터 $ x $를 복원.\n",
    "\n",
    "### ⚖️ 손실 함수\n",
    "#### ✅ 재구성 손실 (Reconstruction Loss)\n",
    "- 원래 데이터와 복원된 데이터 사이의 차이를 최소화.\n",
    "\n",
    "#### ✅ KL Divergence Loss\n",
    "- 잠재 변수 분포 $q(z|x)$가 표준 정규분포 $ p(z) $와 비슷하도록 유도.\n",
    "- 최종 손실:\n",
    "$\n",
    "\\mathcal{L} = \\text{Reconstruction Loss} + D_{KL}\\bigl(q(z|x)\\|p(z)\\bigr)\n",
    "$\n",
    "\n",
    "### 🚀 장점\n",
    "- **데이터 생성 능력**: 새로운 샘플 생성 가능 (예: 얼굴, 글자 등).\n",
    "- **연속적인 잠재 공간**: 벡터 연산이나 interpolation(보간)이 가능.\n",
    "\n",
    "### ⚠️ 단점\n",
    "- 복잡한 데이터 분포를 완벽하게 모델링하기 어려움.\n",
    "- 생성된 샘플의 품질이 GAN보다 떨어질 수 있음.\n",
    "\n",
    "### ✅ 한 줄 요약\n",
    "\n",
    "> **\"VAE는 입력 데이터를 확률 분포로 인코딩하고, 이 분포에서 샘플링해 데이터를 재구성 및 생성할 수 있는 Autoencoder 기반 생성 모델이다.\"**\n",
    "\n",
    "### 🟢 요약 표\n",
    "\n",
    "| 항목                | 설명                                             |\n",
    "|-------------------|------------------------------------------------|\n",
    "| 핵심 개념          | 확률적 잠재 공간, 데이터 분포 학습                  |\n",
    "| Encoder 출력    | 평균(μ), 표준편차(σ)                           |\n",
    "| 샘플링 기법     | Reparameterization Trick 사용                |\n",
    "| 손실 함수         | Reconstruction Loss + KL Divergence        |\n",
    "| 장점              | 데이터 생성 가능, 연속적 latent space       |\n",
    "| 단점              | 샘플 품질 제한, 복잡한 분포 학습 한계            |\n",
    "\n",
    "---\n",
    "## Generative Adversarial Networks (GANs) 설명\n",
    "### 💡 개념\n",
    "- **GANs**는 **생성 모델(Generative Model)**의 한 종류로, 새로운 데이터를 생성할 수 있도록 훈련됩니다.\n",
    "- 2014년 Ian Goodfellow가 처음 제안.\n",
    "\n",
    "### ⚔️ 핵심 아이디어: 두 네트워크의 경쟁\n",
    "- **Generator (생성자)**:\n",
    "  - 무작위 노이즈(z)를 입력받아, **진짜 같은(fake but realistic) 데이터**를 생성.\n",
    "- **Discriminator (판별자)**:\n",
    "  - 진짜 데이터와 Generator가 만든 가짜 데이터를 구별하려고 시도.\n",
    "\n",
    "> **Generator는 판별자를 속이려고 하고, 판별자는 이를 잡아내려고 함 → 서로 경쟁하며 발전!**\n",
    "\n",
    "### 🌀 학습 과정\n",
    "1️⃣ Generator는 랜덤 노이즈를 받아 **가짜 샘플** 생성.  \n",
    "2️⃣ Discriminator는 **진짜 샘플**과 **가짜 샘플**을 입력받아 진짜/가짜 여부를 판별.  \n",
    "3️⃣ Generator는 Discriminator를 속이는 방향으로 학습.  \n",
    "4️⃣ Discriminator는 더 정확히 판별하도록 학습.  \n",
    "5️⃣ 두 네트워크가 동시에 학습하면서 서로 발전 → Generator는 점점 더 진짜 같은 데이터를 생성.\n",
    "\n",
    "### ⚖️ 손실 함수\n",
    "#### ✅ Discriminator의 목적\n",
    "- 진짜 샘플은 1, 가짜 샘플은 0으로 판별하도록 학습.\n",
    "\n",
    "#### ✅ Generator의 목적\n",
    "- 생성한 가짜 샘플을 **진짜라고 판별**하도록 Discriminator를 속이기.\n",
    "- 최종적으로는 다음과 같은 **min-max 게임**:\n",
    "$\n",
    "\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log (1 - D(G(z)))]\n",
    "$\n",
    "\n",
    "### 🚀 장점\n",
    "- 매우 사실적인 데이터(이미지, 음성 등) 생성 가능.\n",
    "- 데이터 증강, 스타일 변환, 이미지 복원 등 다양한 생성적 응용 가능.\n",
    "\n",
    "### ⚠️ 단점\n",
    "- **훈련 불안정**: 균형 잡히지 않으면 Generator나 Discriminator 중 하나가 압도.\n",
    "- **Mode collapse**: Generator가 일부 패턴만 생성하고 다양성이 부족해지는 현상.\n",
    "- Hyperparameter 튜닝이 어렵고 민감.\n",
    "\n",
    "### ✅ 한 줄 요약\n",
    "> **\"GAN은 Generator와 Discriminator가 경쟁하며, 진짜 같은 데이터를 생성하는 모델이다.\"**\n",
    "\n",
    "### 🟢 요약 표\n",
    "| 항목             | 설명                                       |\n",
    "|----------------|----------------------------------------|\n",
    "| 목적            | 새로운 데이터(이미지 등) 생성              |\n",
    "| 구성 요소      | Generator, Discriminator              |\n",
    "| 학습 방식      | 적대적 학습 (Adversarial training)      |\n",
    "| 장점           | 매우 사실적인 샘플 생성 가능            |\n",
    "| 단점           | 학습 불안정, Mode collapse 문제        |\n",
    "\n",
    "### 💬 확장 버전 (예시)\n",
    "- **DCGAN**: CNN 기반 GAN (이미지 생성 특화).\n",
    "- **WGAN**: Wasserstein loss를 사용해 안정성 개선.\n",
    "- **CycleGAN**: 이미지 간 스타일 변환 (예: 겨울 ↔ 여름).\n",
    "- **StyleGAN**: 매우 고해상도의 사실적인 이미지 생성.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f15b910-295b-4cb5-b0a6-7723458c8c85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
