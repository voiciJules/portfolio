{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "205bfad5-5f55-4a8c-9e60-90b5197e7e8a",
   "metadata": {},
   "source": [
    "# Presentation 2\n",
    "\n",
    "## Hyperparameters and Performance\n",
    "\n",
    "### Performance Improvements\n",
    "- Number of Layers\n",
    "- Number of Neurons\n",
    "- Types of Layers\n",
    "- Activation Functions\n",
    "- Batch Sizes\n",
    "- Initialization\n",
    "- Batch Normalization and Layer Normalization\n",
    "- Optimizers\n",
    "- Learning Rate and Scheduling\n",
    "- Regularization\n",
    "  - Dropout\n",
    "   \n",
    "### Improve Algorithm Performance\n",
    "- Get Data (Deep learning get better)\n",
    "- Generate Data. (Data Augmentation / adding noise, call adding jitter)\n",
    "- Rescale Data. (Rescale data to the bounds of activation functions.)\n",
    "- Transform Your Data. (Guesstimate the univariate distribution )\n",
    "- With Algorithms (Steals from literature)\n",
    "- With Algorithm Tuning (tuning hyperparameters)\n",
    "- Improve Performance With Ensembles\n",
    "- 데이터 수집은 직접적으로 딥러닝에 영향을 끼친다. 데이터의 양과 질이 좋을수록 예측성능도 향상되기 때문이다. 데이터 생성 또한 중요한데, 이미지회전, 자르기, 노이즈추가(jitter) 등으로 학습데이터를 다양화시켜 모델의 일반화 성능 향상을 꾀할 수 있기 때문이다. 데이터 정규화, ReLU, Sigmoid 등의 활성화 함수가 기대하는 값 범위에 입력값을 맞추는 것이 중요하다. 데이터 변환은 로그, 제곱근, 박스-콕스 변환 등을 통해 데이터를 정규분포처럼 바꾸는 작업을 말한다. 기존 연구나 논문으로부터 성공적인 알고리즘을 차용해서 적용해보고 하이퍼파라미터들을 조정하여 최적화하는 것이 중요하다. 앙상블(여러모델의 결합, bagging, boosting, stacking)을 사용하여 더 강력한 예측모델을 만드는 것이 최종적으로 성능을 향상시킬 수 있다.\n",
    "\n",
    "#### Generate Data. (Data Augmentation / adding noise, call adding jitter)\n",
    "- Image :\n",
    "  - Perspective Skewing\n",
    "  - Elastic Distortions\n",
    "  - Rotating\n",
    "  - Shearing\n",
    "  - Cropping\n",
    "  - Mirroring\n",
    "  - Colour distortion\n",
    "  - Ex: https://augmentor.readthedocs.io/en/master/\n",
    "- Text:\n",
    "  - Generate conditional synthetic text samples\n",
    "  - Ex: https://github.com/openai/gpt-2\n",
    "\n",
    "#### Rescale Data\n",
    "- Normalized to 0 to 1.\n",
    "- Rescaled to -1 to 1.\n",
    "- Standardized.\n",
    "\n",
    "#### Transform Your Data (Numerical data)\n",
    "- Like a skewed Gaussian, adjusting the skew with a Box-Cox transform.\n",
    "- Like an exponential distribution, adjusting with a log transform\n",
    "\n",
    "#### With Algorithm Tuning (tuning hyperparameters)\n",
    "- Diagnostics.\n",
    "- Weight Initialization.\n",
    "- Learning Rate.\n",
    "- Activation Functions.\n",
    "- Batches and Epochs.\n",
    "- Regularization.\n",
    "- Optimization and Loss.\n",
    "- Early Stopping.\n",
    "\n",
    "##### Diagnostics\n",
    "- Get insight into the learning behavior of your model\n",
    "- Plotting training vs validation over epoches\n",
    "- Overfitting;\n",
    "  - If training is much better than the validation;\n",
    "  - use regularization technique.\n",
    "- Underfitting;\n",
    "  - If training and validation are both low;\n",
    "  - review all network settings\n",
    "  - If an inflection point\n",
    "  - when training goes above the validation;\n",
    "  - use early stopping.\n",
    "##### Weight Initialization\n",
    "- Try taking an existing model (transfer learning) if possible\n",
    "##### Number of Layers and Neurons\n",
    "##### Activation Function\n",
    "Leaky ReLU\n",
    "Parameterised ReLU\n",
    "....\n",
    "Choosing the right Activation Function :\n",
    "- The softmax function is often in the final layer of a deep learning.\n",
    "- ReLU function is only in the hidden layers (intermediate layers)\n",
    "- ReLU function is a general activation function in most cases.\n",
    "- In presence of dead neurons; the leaky ReLU function is the best choice\n",
    "- Always begin with using ReLU function and then move over to other activation functions in case ReLU doesn’t provide with optimum results)\n",
    "##### Batch size\n",
    "- Larger batch sizes result in faster progress in training, but don't converge as fast.\n",
    "- Smaller batch sizes train slower, but can converge faster.\n",
    "- Large batch size ignore details in training while small batch size go to details. It's definitely problem dependent.\n",
    "- Large batch size is good at very distiguable problems and requires enough memomry size while small batch works perfect for very complex and similar classes.\n",
    "- be safe using a batch size of 8,16,32,64 which are pretty standard\n",
    "##### Initializatoin\n",
    "- Glorot Initialization\n",
    "  - Very simple and important trick that speeds up learning considerably. It makes the forward pass more stable and prevents the gradients from dying out, exploding or saturating\n",
    "##### Batch Normalization\n",
    "- Standardizes inputs, then rescales them, then offsets them.\n",
    "- 각 미니배치(mini-batch)의 입력을 정규화(Normalization)하는 과정입니다. 즉, 각 층(layer)의 입력이 평균 0, 분산 1이 되도록 변환합니다.\n",
    "- 왜 사용하는가?\n",
    "  - 내부 공변량 변화(Internal Covariate Shift) 를 줄이기 위해\n",
    "    - 학습 도중 레이어의 입력 분포가 계속 바뀌는 문제\n",
    "  - 학습 속도 향상\n",
    "    - 더 큰 학습률(Learning Rate) 사용 가능\n",
    "  - 초기화 민감도 감소\n",
    "    - 가중치 초기화에 덜 민감해짐\n",
    "  - 과적합 감소 효과\n",
    "    - 일종의 정규화(regularization) 효과가 있어 드롭아웃 없이도 성능 향상 가능\n",
    "- 어디에 사용하는가?\n",
    "  - CNN에서는 보통 Conv Layer 뒤, Activation 이전\n",
    "  - **DNN (Fully connected)**에서는 Affine Layer 뒤, Activation 이전\n",
    "  - RNN/LSTM은 구조적으로 제한적이지만 일부 변형된 BN 사용 가능\n",
    "##### Optimizers\n",
    "- SGD, Adam...\n",
    "##### Learning Rate\n",
    "- Learning rate is coupled with the number of training epochs, batch size and optimization method.\n",
    "- Learning Rate Schedule For Training Models\n",
    "  - Decrease the learning rate gradually based on the epoch. (argument called decay)\n",
    "  - Decrease the learning rate using punctuated large drops at specific epochs.\n",
    "  - Cyclical Learning Rate\n",
    "##### Regularization in Deep Learning :\n",
    "- L2(Ridge) and L1(Lasso) regularization\n",
    "- Dropout\n",
    "  - Cripple Neural Network by removing hidden units stochastically\n",
    "    - Each hidden unit is set to 0 with a probability\n",
    "    - Redundancy of learning\n",
    "    - Noise Robustness\n",
    "    - No co-adaptation\n",
    "- Data augmentation\n",
    "  - The simplest way to lessen overfitting is to increase the size of the training dat\n",
    "- Early Stopping and Patience Parameter\n",
    "  - Early stopping is a type of regularization to monitor the performance of the model on training and a validation datasets, each epoch\n",
    "  - Patience argument is the number of epochs before stopping once your loss increases (stops improving).\n",
    "  - This is a practical parameter that should be set :\n",
    "    - In very small batches or a large learning rate set a large patience argument.\n",
    "    - In very large batches and a small learning set a smaller patience argument\n",
    "\n",
    "#### How to Choose Hyperparameters\n",
    "- Selecting by hand\n",
    "- Grid Search\n",
    "- Random Search\n",
    "- Bayesian Optimization\n",
    "\n",
    "#### What Happens During Learning?\n",
    "- Universal Function Approximator (Universal approximation theorem, 보편 근사 정리)\n",
    "  - 하나의 은닉층(hidden layer)을 가진 신경망(neural network)은 적절한 조건 하에, 임의의 연속 함수(continuous function)를 원하는 정확도로 근사할 수 있다는 이론이다.즉, 충분한 수의 뉴런을 가진 단일 은닉층 MLP(다층 퍼셉트론)는 어떤 복잡한 함수든 거의 완벽하게 표현할 수 있다.\n",
    "  - 이론적으로 신경망은 만능 함수 근사기라는 뜻.\n",
    "  - 하지만 실제로는 학습 데이터, 학습 방법, 일반화 능력 등에 따라 성능이 좌우됨.\n",
    "  - 층이 하나면 가능은 하지만, 학습 효율이 매우 낮고 비현실적일 수 있음 → 그래서 딥러닝이 발전함.\n",
    "- Learning Representations (learning Representations.ipynb)\n",
    "- Example of meaningful embeddings (https://projector.tensorflow.org/)\n",
    "\n",
    "#### Interpreting Learning Curves (Learning Curve.ipynb)\n",
    "- Do I need a new model or more data?\n",
    "- Are the training and testing errors converging as we add more data?\n",
    "  - Yes - More data\n",
    "  - No - New mode\n",
    "\n",
    "#### Hyperparameters and performance.ipynb\n",
    "#### Customizations.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99c90dd-39fc-4f02-b471-3e0ba9b9e028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c22dae-9840-46e6-b6bc-aa15df5530d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37882ba4-7a43-4cfa-a55c-7ee9d7597438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e450384-dcea-45b6-994e-599a6e8c1c42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7929fb-234a-421e-a650-2222192d99b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5c7819-931a-4d9c-abf5-b595b7cda9f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42ec4a5-4806-4486-b1a6-f96d20aa5c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5e1891-6dac-41b7-9f52-0c88785e3771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd330a8-bf11-4c48-b1db-ba6a6cd4eeee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6cdce3-99f7-4ea1-af23-596bcdc67bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcd5a2d-c0f3-4f1b-a92a-fc8dd6984af9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a32a84a-0388-4ca1-946d-2f3be57a8555",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
