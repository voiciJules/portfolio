{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c783ff91-2fb2-43ce-8803-1fb380c6d96f",
   "metadata": {},
   "source": [
    "# Presentation 04. Processing Sequences Using RNNs and CNNs, Time2Vec\n",
    "\n",
    "## ğŸ“ˆ Time Series Components: Systematic vs Non-Systematic\n",
    "### ğŸ”· Systematic Components\n",
    "- ì˜ë¯¸: **ì¼ê´€ì„±(consistency)** ë˜ëŠ” **ë°˜ë³µì„±(recurrence)**ì„ ê°€ì§€ëŠ” êµ¬ì„± ìš”ì†Œ\n",
    "- íŠ¹ì§•: **ëª¨ë¸ë§ì´ ê°€ëŠ¥**í•˜ë©° ìˆ˜í•™ì ìœ¼ë¡œ ì„¤ëª…í•  ìˆ˜ ìˆìŒ\n",
    "- ì˜ˆì‹œ:\n",
    "  - **Trend (ì¶”ì„¸)**: ì‹œê°„ì— ë”°ë¼ ì¦ê°€í•˜ê±°ë‚˜ ê°ì†Œí•˜ëŠ” ê²½í–¥\n",
    "  - **Seasonality (ê³„ì ˆì„±)**: ì£¼ê¸°ì ìœ¼ë¡œ ë°˜ë³µë˜ëŠ” íŒ¨í„´\n",
    "  - **Cyclic patterns (ìˆœí™˜ì„±)**: ì¼ì •í•˜ì§€ëŠ” ì•Šì§€ë§Œ ë°˜ë³µë˜ëŠ” ì¥ê¸°ì  ë³€í™”\n",
    "\n",
    "### ğŸ”¶ Non-Systematic Components\n",
    "- ì˜ë¯¸: **ì§ì ‘ì ìœ¼ë¡œ ëª¨ë¸ë§í•  ìˆ˜ ì—†ëŠ” ë¶ˆê·œì¹™í•œ êµ¬ì„± ìš”ì†Œ**\n",
    "- íŠ¹ì§•: ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•˜ê³  **ë¬´ì‘ìœ„ì„±(randomness)**ì„ ë‚´í¬í•¨\n",
    "- ì˜ˆì‹œ:\n",
    "  - **Irregular component (ë¶ˆê·œì¹™ì„±)** ë˜ëŠ” **noise (ë…¸ì´ì¦ˆ)**: ê°‘ì‘ìŠ¤ëŸ¬ìš´ ì‚¬ê³ , ìì—°ì¬í•´, ì˜ˆì™¸ì  ì‚¬ê±´ ë“±\n",
    "\n",
    "### âœ… ìš”ì•½\n",
    "\n",
    "| êµ¬ë¶„             | ì„¤ëª…                                        | ì˜ˆì‹œ                  |\n",
    "|------------------|---------------------------------------------|------------------------|\n",
    "| Systematic       | ì„¤ëª… ê°€ëŠ¥í•˜ê³  ì˜ˆì¸¡ ê°€ëŠ¥í•œ êµ¬ì¡°ì  ìš”ì†Œ       | Trend, Seasonality     |\n",
    "| Non-Systematic   | ì„¤ëª… ë¶ˆê°€í•œ ë¬´ì‘ìœ„ì  ìš”ì†Œ                   | Noise, Irregularity    |\n",
    "\n",
    "## ğŸ§± Time Series Components\n",
    "Time series data can be broken down into several core components that help explain the structure and behavior of the data over time.\n",
    "\n",
    "### 1ï¸âƒ£ **Level**\n",
    "- ğŸ“Œ ì˜ë¯¸: ì‹œê³„ì—´ì˜ **ê¸°ë³¸ í‰ê· ê°’** ìˆ˜ì¤€\n",
    "- ì˜ˆì‹œ: ì¥ê¸°ì ìœ¼ë¡œ ì£¼ê°€ê°€ 100ë‹¬ëŸ¬ ê·¼ì²˜ì—ì„œ ìœ ì§€ëœë‹¤ë©´, ê·¸ ê°’ì´ level\n",
    "\n",
    "### 2ï¸âƒ£ **Trend**\n",
    "- ğŸ“ˆ ì˜ë¯¸: ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ë°ì´í„°ê°€ **ì§€ì†ì ìœ¼ë¡œ ì¦ê°€í•˜ê±°ë‚˜ ê°ì†Œí•˜ëŠ” ë°©í–¥**\n",
    "- ì˜ˆì‹œ: ì¸êµ¬ìˆ˜, ì£¼íƒ ê°€ê²©, ì£¼ê°€ì˜ ì¥ê¸° ìƒìŠ¹/í•˜ë½ ê²½í–¥\n",
    "\n",
    "### 3ï¸âƒ£ **Seasonality**\n",
    "- ğŸ” ì˜ë¯¸: ì¼ì • ì£¼ê¸°ë§ˆë‹¤ ë°˜ë³µë˜ëŠ” **ë‹¨ê¸°ì ì¸ ì£¼ê¸°ì  íŒ¨í„´**\n",
    "- ì˜ˆì‹œ: ê³„ì ˆì— ë”°ë¥¸ ì—ì–´ì»¨ íŒë§¤ëŸ‰, ìš”ì¼ì— ë”°ë¥¸ ì›¹ì‚¬ì´íŠ¸ ë°©ë¬¸ì ìˆ˜\n",
    "\n",
    "### 4ï¸âƒ£ **Noise (Irregularity)**\n",
    "- ğŸ² ì˜ë¯¸: **ë¬´ì‘ìœ„ì ì¸ ë³€ë™**ìœ¼ë¡œ, ì˜ˆì¸¡ì´ ì–´ë µê³  ì¼ì •í•œ íŒ¨í„´ì´ ì—†ìŒ\n",
    "- ì˜ˆì‹œ: ê°‘ì‘ìŠ¤ëŸ¬ìš´ ë‰´ìŠ¤ë¡œ ì¸í•œ ì£¼ê°€ ê¸‰ë“±/ê¸‰ë½, ì˜ˆì™¸ì ì¸ ê¸°ìƒ ì´ë³€\n",
    "\n",
    "### âœ… ìš”ì•½í‘œ\n",
    "\n",
    "| êµ¬ì„± ìš”ì†Œ     | ì„¤ëª…                                       | ì˜ˆì‹œ                        |\n",
    "|---------------|--------------------------------------------|-----------------------------|\n",
    "| Level         | ì‹œê³„ì—´ì˜ í‰ê· ì ì¸ ê¸°ë³¸ê°’                   | í‰ê·  ì‹¬ë°•ìˆ˜, í‰ê·  ìˆ˜ìš”ëŸ‰     |\n",
    "| Trend         | ì¥ê¸°ì ì¸ ì¦ê°€/ê°ì†Œ ê²½í–¥                    | ë¶€ë™ì‚° ê°€ê²© ìƒìŠ¹             |\n",
    "| Seasonality   | ì§§ì€ ì£¼ê¸°ì˜ ë°˜ë³µì ì¸ íŒ¨í„´                  | ì—¬ë¦„ì²  ì—ì–´ì»¨ ìˆ˜ìš” ì¦ê°€      |\n",
    "| Noise         | ì„¤ëª…ë˜ì§€ ì•ŠëŠ” ë¬´ì‘ìœ„ì  ë³€ë™                | ëŒë°œ ì‚¬ê±´, ìì—°ì¬í•´          |\n",
    "\n",
    "## ğŸ”„ Autocorrelation in Time Series\n",
    "### ğŸ“Œ What is Autocorrelation?\n",
    "- **Autocorrelation** is the **correlation between a time series and a lagged version of itself**.\n",
    "- It shows how current values of the series relate to **past values** at different lags.\n",
    "\n",
    "### ğŸ“ˆ Why is it important?\n",
    "- Helps **identify seasonality**, trends, and repeating patterns in time series data.\n",
    "- Useful for:\n",
    "  - Detecting **repeating cycles** (e.g., weekly, monthly patterns)\n",
    "  - Understanding **persistence** or **momentum** in the data\n",
    "  - Choosing the right number of lags in models like **ARIMA**\n",
    "\n",
    "### Time Series Forecasting with Deep Learning and Attention Mechanism\n",
    "https://towardsdatascience.com/time-series-forecasting-with-deep-learning-and-attention-mechanism-2d001fc871fc/\n",
    "\n",
    "## Disadvantages of Recurrent Neural Network\n",
    "On long time series; suffer from the vanishing gradient or exploding gradient, the parameters either donâ€™t change that much (Vanishing Gradient) or they lead to numeric instability and chaotic behavior (Exploding Gradient).\n",
    "\n",
    "## Case Study in RNN :\n",
    "Training a RNN on a large network; then notice that somewhere in the middle of the 42nd epoch the cost that you monitor becomes\n",
    "NaN. Identify the source of the problem? How would fix it ?\n",
    "- Problem is gradient exploding :\n",
    "  - Gradient errors can accumulate during an update and result in very large gradients.\n",
    "  - Result in large updates to the network weights, and in turn, an unstable network.\n",
    "  - At an extreme, the values of weights can become so large as to overflow and result in NaN values.\n",
    "- Solutions :\n",
    "  - Reducing learning rate\n",
    "  - Using Adam optimizer (Adam is the adaptive optimizers in most of the cases. Good with sparse data)\n",
    "  - Using gradient clipping (minimum or maximum value if the gradient exceeded an expected range)\n",
    "  - Adding regularization.\n",
    "  - Network size reduction.\n",
    "  - Increasing the batch size.\n",
    "  - Using LeakyReLU.\n",
    "  - Checking whether the input are Nan or zeroes.\n",
    "  - !!!!Normalizing ( Transformation) the input data\n",
    "\n",
    "## LSTM (Long Short-Term Memory) ì„¤ëª…\n",
    "LSTMì€ RNNì˜ ì¼ì¢…ìœ¼ë¡œ, **ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œ**(long-term dependency)ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê³ ì•ˆëœ êµ¬ì¡°ì…ë‹ˆë‹¤. ì•„ë˜ëŠ” LSTMì˜ ì£¼ìš” êµ¬ì„± ìš”ì†Œì™€ ì‘ë™ ì›ë¦¬ì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤.\n",
    "\n",
    "<img src=\"./lstm.png\" alt=\"LSTM êµ¬ì¡°\" width=\"50%\">\n",
    "\n",
    "### ì£¼ìš” êµ¬ì„± ìš”ì†Œ\n",
    "#### 1. Forget Gate (ë§ê° ê²Œì´íŠ¸)\n",
    "- ì—­í• : ì´ì „ ì…€ ìƒíƒœ `C(t-1)`ì—ì„œ **ë¬´ì—‡ì„ ìŠì„ì§€ ê²°ì •**í•©ë‹ˆë‹¤.\n",
    "- ê³„ì‚°ì‹:  \n",
    "  `f(t) = Ïƒ(W_fÂ·x(t) + U_fÂ·h(t-1) + b_f)`\n",
    "- ì¶œë ¥ê°’ì€ 0~1 ì‚¬ì´ì´ë©°, 0ì´ë©´ ì™„ì „íˆ ìŠê³  1ì´ë©´ ì™„ì „íˆ ê¸°ì–µí•©ë‹ˆë‹¤.\n",
    "\n",
    "#### 2. Input Gate (ì…ë ¥ ê²Œì´íŠ¸)\n",
    "- ì—­í• : **í˜„ì¬ ì…ë ¥ x(t)**ë¥¼ ì–¼ë§ˆë‚˜ ì…€ ìƒíƒœì— ë°˜ì˜í• ì§€ ê²°ì •í•©ë‹ˆë‹¤.\n",
    "- ê³„ì‚°ì‹:  \n",
    "  `iâ‚(t) = Ïƒ(W_iÂ·x(t) + U_iÂ·h(t-1) + b_i)`  \n",
    "  `iâ‚‚(t) = tanh(W_gÂ·x(t) + U_gÂ·h(t-1) + b_g)`\n",
    "- ìƒˆë¡œìš´ ì •ë³´ëŠ” `iâ‚(t) * iâ‚‚(t)`ë¡œ ê³„ì‚°ë˜ì–´ ì…€ ìƒíƒœì— ì¶”ê°€ë©ë‹ˆë‹¤.\n",
    "\n",
    "#### 3. Cell State Update (ì…€ ìƒíƒœ ê°±ì‹ )\n",
    "- ì…€ ìƒíƒœëŠ” ì•„ë˜ ë°©ì‹ìœ¼ë¡œ ê°±ì‹ ë©ë‹ˆë‹¤:  \n",
    "  `C(t) = f(t) * C(t-1) + iâ‚(t) * iâ‚‚(t)`\n",
    "\n",
    "#### 4. Output Gate (ì¶œë ¥ ê²Œì´íŠ¸)\n",
    "- ì—­í• : ìµœì¢… ì¶œë ¥ê°’ `h(t)`ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.\n",
    "- ê³„ì‚°ì‹:  \n",
    "  `o(t) = Ïƒ(W_oÂ·x(t) + U_oÂ·h(t-1) + b_o)`  \n",
    "  `h(t) = o(t) * tanh(C(t))`\n",
    "- ì¶œë ¥ì€ ì…€ ìƒíƒœì˜ í™œì„±í™”ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ìš”ì•½\n",
    "- **Forget Gate**: ë¬´ì—‡ì„ ìŠì„ì§€\n",
    "- **Input Gate**: ë¬´ì—‡ì„ ê¸°ì–µí• ì§€\n",
    "- **Output Gate**: ë¬´ì—‡ì„ ì¶œë ¥í• ì§€\n",
    "ì´ êµ¬ì¡° ë•ë¶„ì— LSTMì€ **ì¥ê¸°ì ì¸ ì˜ì¡´ì„± ì •ë³´ë¥¼ ë” ì˜ ê¸°ì–µí•˜ê³  í™œìš©**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "## GRU (Gated Recurrent Unit) ì„¤ëª…\n",
    "GRUëŠ” LSTMë³´ë‹¤ êµ¬ì¡°ê°€ ë‹¨ìˆœí•˜ë©´ì„œë„ íš¨ê³¼ì ì¸ **ê²Œì´íŠ¸ ê¸°ë°˜ RNN ì•„í‚¤í…ì²˜**ì…ë‹ˆë‹¤. GRUëŠ” ë‘ ê°œì˜ ê²Œì´íŠ¸ë§Œ ì‚¬ìš©í•˜ì—¬ ê³¼ê±° ì •ë³´ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤: **Reset Gate**ì™€ **Update Gate**.\n",
    "\n",
    "<img src=\"./gru.png\" alt=\"GRU êµ¬ì¡°\" width=\"50%\">\n",
    "\n",
    "### GRU êµ¬ì„± ìš”ì†Œ\n",
    "\n",
    "#### 1. Reset Gate `r(t)`\n",
    "- ì—­í• : ê³¼ê±°ì˜ ì •ë³´ë¥¼ **ì–¼ë§ˆë‚˜ ìŠì„ì§€** ê²°ì •í•©ë‹ˆë‹¤.\n",
    "- ê³„ì‚°ì‹:  \n",
    "  `r(t) = Ïƒ(W_rÂ·x(t) + U_rÂ·h(t-1))`\n",
    "- ì‘ì€ ê°’ì´ë©´ ê³¼ê±° ì •ë³´ë¥¼ ëœ ë°˜ì˜í•˜ê³ , í° ê°’ì´ë©´ ë” ë§ì´ ë°˜ì˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "#### 2. Update Gate `u(t)`\n",
    "- ì—­í• : ê³¼ê±°ì˜ hidden stateë¥¼ **ì–¼ë§ˆë‚˜ ìœ ì§€í• ì§€** ê²°ì •í•©ë‹ˆë‹¤.\n",
    "- ê³„ì‚°ì‹:  \n",
    "  `u(t) = Ïƒ(W_zÂ·x(t) + U_zÂ·h(t-1))`\n",
    "- `u(t)`ê°€ 1ì´ë©´ ì´ì „ hidden stateë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê³ , 0ì´ë©´ ìƒˆë¡œ ê³„ì‚°ëœ ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "#### 3. Candidate Hidden State `hÌƒ(t)`\n",
    "- Reset Gateë¥¼ í†µí•´ ì´ì „ ìƒíƒœ ì •ë³´ë¥¼ ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ **ìƒˆë¡œìš´ ìƒíƒœ í›„ë³´**ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "- ê³„ì‚°ì‹:  \n",
    "  `hÌƒ(t) = tanh(WÂ·x(t) + UÂ·(r(t) * h(t-1)))`\n",
    "\n",
    "#### 4. ìµœì¢… Hidden State `h(t)`\n",
    "- Update Gateë¥¼ í†µí•´ ì´ì „ ìƒíƒœì™€ ìƒˆë¡œìš´ ìƒíƒœë¥¼ **ê°€ì¤‘ í‰ê· **í•˜ì—¬ í˜„ì¬ ìƒíƒœë¥¼ ê²°ì •í•©ë‹ˆë‹¤.\n",
    "- ê³„ì‚°ì‹:  \n",
    "  `h(t) = u(t) * h(t-1) + (1 - u(t)) * hÌƒ(t)`\n",
    "\n",
    "### ìš”ì•½\n",
    "- **Reset Gate** `r(t)`: ê³¼ê±° ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ìŠì„ì§€ ê²°ì •  \n",
    "- **Update Gate** `u(t)`: ê³¼ê±° ìƒíƒœë¥¼ ì–¼ë§ˆë‚˜ ìœ ì§€í• ì§€ ê²°ì •  \n",
    "- **GRUëŠ” ì…€ ìƒíƒœ ì—†ì´ hidden stateë§Œ ìœ ì§€í•˜ë©°**, LSTMë³´ë‹¤ ê³„ì‚°ì´ ê°„ë‹¨í•©ë‹ˆë‹¤.\n",
    "\n",
    "### GRU vs. LSTM\n",
    "\n",
    "| í•­ëª©        | GRU                  | LSTM                     |\n",
    "|-------------|-----------------------|--------------------------|\n",
    "| ê²Œì´íŠ¸ ìˆ˜    | 2ê°œ (Reset, Update)   | 3ê°œ (Forget, Input, Output) |\n",
    "| ì…€ ìƒíƒœ      | ì—†ìŒ (hë§Œ ìœ ì§€)        | ìˆìŒ (h, C ëª¨ë‘ ìœ ì§€)       |\n",
    "| ê³„ì‚° ë³µì¡ë„  | ë‚®ìŒ                  | ë¹„êµì  ë†’ìŒ                 |\n",
    "| ì„±ëŠ¥ ì°¨ì´    | ê±°ì˜ ì—†ìŒ (ë°ì´í„°ì— ë”°ë¼ ë‹¤ë¦„) | ê±°ì˜ ì—†ìŒ (ë°ì´í„°ì— ë”°ë¼ ë‹¤ë¦„) |\n",
    "  \n",
    "### ğŸ“˜ CNN-LSTM ëª¨ë¸ì´ë€?\n",
    "CNN-LSTMì€ **Convolutional Neural Network (CNN)**ê³¼ **Long Short-Term Memory (LSTM)**ë¥¼ ê²°í•©í•œ ëª¨ë¸ë¡œ, ì£¼ë¡œ **ì‹œê³„ì—´ ì˜ˆì¸¡(time series forecasting)**ì— ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "\n",
    "#### âœ… êµ¬ì„±:\n",
    "1. **Conv1D (1D Convolution Layer)**  \n",
    "   - ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì…ë ¥ë°›ì•„, **êµ­ì†Œì ì¸ íŒ¨í„´(local features)**ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.  \n",
    "   - ì˜ˆ: ì£¼ê¸°ì„±, ê¸‰ê²©í•œ ë³€í™” ë“±\n",
    "2. **LSTM Layer**  \n",
    "   - CNNì´ ì¶”ì¶œí•œ ì‹œê³„ì—´ íŠ¹ì§•ë“¤ì„ ì‹œí€€ìŠ¤ë¡œ ë³´ê³ , **ì‹œê°„ì— ë”°ë¥¸ ì˜ì¡´ì„±**ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "3. **Dense Layer (ì¶œë ¥)**  \n",
    "   - ìµœì¢… ì˜ˆì¸¡ ê°’ì„ ì¶œë ¥í•©ë‹ˆë‹¤ (ì˜ˆ: ë¯¸ë˜ì˜ ì£¼ê°€, ì˜¨ë„ ë“±)\n",
    "\n",
    "### ğŸ§  Conv1Dë€?\n",
    "- **Conv1D**ëŠ” 1ì°¨ì› ì‹œê³„ì—´ ë°ì´í„°(ì˜ˆ: `[xâ‚, xâ‚‚, ..., xâ‚™]`)ì— ëŒ€í•´ ì»¤ë„ì„ slidingí•˜ì—¬ **ì—°ì†ì ì¸ êµ¬ê°„ì—ì„œ íŠ¹ì§•(feature)**ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "- ì»¤ë„(kernel)ì€ ì‘ì€ ì°½(window)ì²˜ëŸ¼ ì‘ë™í•˜ë©°, ê° ìœ„ì¹˜ì—ì„œ **ê³±í•˜ê³  ë”í•˜ëŠ” ê³„ì‚°(convolution)**ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "### âš–ï¸ Causal Convolution vs. Standard Convolution\n",
    "| í•­ëª© | Standard Convolution | Causal Convolution |\n",
    "|------|-----------------------|---------------------|\n",
    "| ğŸ“… ì‹œê°„ ìˆœì„œ ê³ ë ¤ | âŒ ë¯¸ë˜ ì‹œì ê¹Œì§€ ì»¤ë„ì´ ë´„ (t+1 ë“±) | âœ… ê³¼ê±° ì‹œì ë§Œ ë´„ (t ì´í•˜) |\n",
    "| ğŸ§  í•™ìŠµ ì •ë³´ | í˜„ì¬ + ë¯¸ë˜ ì •ë³´ ì‚¬ìš© | ì˜¤ì§ ê³¼ê±° ì •ë³´ë§Œ ì‚¬ìš© |\n",
    "| â³ ì‹œê³„ì—´ ì˜ˆì¸¡ì— ì í•© | âŒ | âœ… ë§¤ìš° ì í•© |\n",
    "| ğŸ§© padding ë°©ì‹ | ë³´í†µ `\"same\"` ë˜ëŠ” `\"valid\"` | `\"causal\"` |\n",
    "| ì˜ˆì‹œ | `[t-1, t, t+1]` ì„ ì»¤ë„ì´ ë´„ | `[t-2, t-1, t]` ë§Œ ë´„ (t+1 ê¸ˆì§€) |\n",
    "\n",
    "### ğŸ“Œ ì™œ CNN-LSTMì— Conv1D + Causal Paddingì„ ì“°ë‚˜?\n",
    "- CNNì€ ë°ì´í„°ì˜ **ì§§ì€ íŒ¨í„´(local dependencies)**ì„ ë¹ ë¥´ê²Œ ì¡ì•„ëƒ…ë‹ˆë‹¤.\n",
    "- Causal paddingì€ LSTMì— ì „ë‹¬í•  ë•Œ **ì‹œê°„ ìˆœì„œë¥¼ ë³´ì¡´**í•©ë‹ˆë‹¤.\n",
    "- LSTMì€ ê·¸ í›„ **ì¥ê¸° ì˜ì¡´ì„±(long-term dependency)**ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ ì¡°í•©ì€:  \n",
    "> \"ì§§ì€ íŒ¨í„´ â†’ ì‹œê°„ êµ¬ì¡° ì´í•´ â†’ ì˜ˆì¸¡\"  \n",
    "ì´ë¼ëŠ” ì´ìƒì ì¸ íë¦„ì„ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤.\n",
    "\n",
    "## WaveNetê³¼ ì‹œê³„ì—´ ì˜ˆì¸¡\n",
    "### 1. WaveNet ê°œìš”\n",
    "- WaveNetì€ ì›ë˜ ìŒì„± í•©ì„±ì„ ìœ„í•´ ê°œë°œëœ ë”¥ëŸ¬ë‹ ëª¨ë¸ë¡œ, **ë”¥ ì»¨ë³¼ë£¨ì…”ë„ ë„¤íŠ¸ì›Œí¬(CNN)** ê¸°ë°˜ì´ë‹¤.\n",
    "- ì‹œê³„ì—´ ë°ì´í„°ì˜ **ìˆœì°¨ì  íŠ¹ì„±**ì„ ëª¨ë¸ë§í•˜ëŠ” ë° ê°•ì ì´ ìˆë‹¤.\n",
    "- ì¼ë°˜ì ì¸ RNN/LSTMê³¼ ë‹¬ë¦¬, CNN ê¸°ë°˜ìœ¼ë¡œ **ë³‘ë ¬ì²˜ë¦¬ê°€ ê°€ëŠ¥**í•˜ë©°, ë§¤ìš° ê¸´ ì‹œê³„ì—´ ì˜ì¡´ì„±ë„ ìº¡ì²˜ ê°€ëŠ¥í•˜ë‹¤.\n",
    "\n",
    "### 2. í•µì‹¬ ì•„ì´ë””ì–´: ì¸ê³¼ì (ì¹´ì£¼ì–¼) ì»¨ë³¼ë£¨ì…˜ (Causal Convolution)\n",
    "- ì¶œë ¥ ì‹œì  tì˜ ì˜ˆì¸¡ì— ëŒ€í•´, ë¯¸ë˜ ì •ë³´(t+1, t+2...)ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ì˜¤ì§ ê³¼ê±°(t, t-1, ...) ë°ì´í„°ë§Œ í™œìš©.\n",
    "- ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ 0ìœ¼ë¡œ íŒ¨ë”©í•˜ì—¬ í˜„ì¬ ì‹œì  ì´ì „ ì •ë³´ë§Œ ì»¨ë³¼ë£¨ì…˜ ì—°ì‚°ì— ë°˜ì˜ë˜ë„ë¡ í•œë‹¤.\n",
    "\n",
    "### 3. Dilation Convolution (íŒ½ì°½ ì»¨ë³¼ë£¨ì…˜)\n",
    "- ì»¤ë„ ì‚¬ì´ì˜ ê°„ê²©(ê°„ê²© = dilation rate)ì„ ë‘ì–´, ë„“ì€ ì‹œê³„ì—´ ë²”ìœ„ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í•œ ë²ˆì— ì»¤ë²„.\n",
    "- dilation rateê°€ ì ì  ì»¤ì ¸ì„œ receptive fieldê°€ ì§€ìˆ˜ì ìœ¼ë¡œ í™•ì¥ë¨ â†’ ë§¤ìš° ê¸´ ê³¼ê±° ì‹œì ë„ ë°˜ì˜ ê°€ëŠ¥.\n",
    "- ì˜ˆ: dilation = 1, 2, 4, 8, ...\n",
    "\n",
    "### 4. WaveNetì˜ ì‹œê³„ì—´ ì˜ˆì¸¡ ê³¼ì •\n",
    "- ì…ë ¥ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ causal + dilation convolution ê³„ì¸µì— í†µê³¼ì‹œì¼œ íŠ¹ì§• ì¶”ì¶œ.\n",
    "- ê° ì‹œì ì˜ ë‹¤ìŒ ê°’(ë˜ëŠ” ë‹¤ìŒ í™•ë¥  ë¶„í¬)ì„ ì˜ˆì¸¡.\n",
    "- ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì‹œí€€ìŠ¤ ë‹¨ìœ„ë¡œ ì ì§„ì ìœ¼ë¡œ ìƒì„±í•˜ê±°ë‚˜ ì˜ˆì¸¡ ê°€ëŠ¥.\n",
    "\n",
    "### 5. WaveNetì˜ ì¥ì \n",
    "- ê¸´ ì‹œê³„ì—´ ì˜ì¡´ì„± ìº¡ì²˜ ê°€ëŠ¥ (RNNë³´ë‹¤ ë” íš¨ìœ¨ì )\n",
    "- ë³‘ë ¬ì²˜ë¦¬ ê°€ëŠ¥ (RNN ëŒ€ë¹„ ë¹ ë¦„)\n",
    "- ë¹„ì„ í˜•ì„± ê°•í•œ ì‹œê³„ì—´ ë°ì´í„°ì— íš¨ê³¼ì \n",
    "\n",
    "### more info : https://www.kilians.net/post/convolution-in-autoregressive-neural-networks/\n",
    "\n",
    "### fit_transform vs transform\n",
    "- https://towardsdatascience.com/what-and-why-behind-fit-transform-vs-transform-in-scikit-learn-78f915cf96fe/\n",
    "- ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•  ë•Œ, ì „ì²´ ë°ì´í„°ë¥¼ í•œêº¼ë²ˆì— fit_transform() í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤. ì™œëƒí•˜ë©´, í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í¬í•¨í•´ì„œ í‰ê· ì´ë‚˜ ë¶„ì‚°ì„ ê³„ì‚°í•˜ê²Œ ë˜ë©´, ëª¨ë¸ì´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ì •ë³´ë¥¼ ë¯¸ë¦¬ ì•Œê²Œ ë˜ëŠ” ê²ƒì´ê³ , ê·¸ë ‡ê²Œ ë˜ë©´ í›ˆë ¨ ì„±ëŠ¥ì´ ê³¼ëŒ€í‰ê°€ë˜ê³ , ì‹¤ì œ ì„±ëŠ¥ì€ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ë°ì´í„° ëˆ„ìˆ˜). ë”°ë¼ì„œ í›ˆë ¨ ë°ì´í„°ë¡œë§Œ fit_transform() í•˜ê³ , í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” ê·¸ ê¸°ì¤€ìœ¼ë¡œ transform()ë§Œ í•´ì•¼ ëª¨ë¸ì´ íŠ¹ì • ë°ì´í„° ë¶„í¬ì— ì¹˜ìš°ì¹˜ì§€ ì•Šê²Œ ë˜ê³ , í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ íŒ¨í„´ì„ ëª°ë˜ ë°°ìš°ì§€ ì•Šê²Œ ë©ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ§  Simple RNN ëª¨ë¸ ì„¤ëª…\n",
    "### ğŸ“ ì…ë ¥ í˜•íƒœ (Input Shape)\n",
    "\n",
    "```python\n",
    "X = (n_samples, n_timesteps, n_features)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(1, input_shape=[None, 1])\n",
    "])\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.005)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "h = model.fit(X_train, y_train, epochs=20)\n",
    "```\n",
    "\n",
    "- n_samples: ì „ì²´ ì‹œê³„ì—´ ìƒ˜í”Œ ìˆ˜ (ì˜ˆ: 1000ê°œ)\n",
    "- n_timesteps: ê° ìƒ˜í”Œì—ì„œ ì‹œê°„ ë‹¨ê³„ ìˆ˜ (ì˜ˆ: 30ì¼)\n",
    "- n_features: ê° ì‹œê°„ ë‹¨ê³„ì˜ íŠ¹ì„± ìˆ˜ (ì˜ˆ: 1ê°œ ì„¼ì„œ ê°’)\n",
    "- Sequential: ìˆœì°¨ì  ë ˆì´ì–´ êµ¬ì„±\n",
    "- SimpleRNN(1): ì¶œë ¥ ìœ ë‹› 1ê°œì¸ RNN ì…€\n",
    "- input_shape=[None, 1]:\n",
    "  - None: ìœ ë™ì ì¸ ì‹œê°„ ê¸¸ì´\n",
    "  - 1: ê° íƒ€ì„ìŠ¤í…ì—ì„œ ì…ë ¥ í”¼ì²˜ ìˆ˜\n",
    "- Adam: ì ì‘í˜• í•™ìŠµë¥ ì„ ê°–ëŠ” íš¨ìœ¨ì ì¸ ì˜µí‹°ë§ˆì´ì €\n",
    "- learning_rate=0.005: í•™ìŠµë¥  ì§€ì •\n",
    "- loss=\"mse\": í‰ê· ì œê³±ì˜¤ì°¨ (íšŒê·€ ë¬¸ì œì— ì í•©)\n",
    "- optimizer=optimizer: ìœ„ì—ì„œ ì„¤ì •í•œ ì˜µí‹°ë§ˆì´ì € ì‚¬ìš©\n",
    "- X_train: ì…ë ¥ ì‹œê³„ì—´ ë°ì´í„° (samples, timesteps, features)\n",
    "- y_train: ì˜ˆì¸¡ ëŒ€ìƒ ê°’ (ì˜ˆ: ë‹¤ìŒ ë‚  ì˜¨ë„)\n",
    "- epochs=20: ì „ì²´ ë°ì´í„°ì…‹ì„ 20ë²ˆ ë°˜ë³µ í•™ìŠµ\n",
    "- h: í•™ìŠµ ì¤‘ ì†ì‹¤ê°’ ë“±ì´ ë‹´ê¸´ ê²°ê³¼ ê°ì²´ (History)\n",
    "\n",
    "## ğŸ” Deep RNNê³¼ `return_sequences=True` ì„¤ëª…\n",
    "### ğŸ“Œ ì½”ë“œ êµ¬ì¡°\n",
    "```python\n",
    "model_rec_deep = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20),\n",
    "    keras.layers.Dense(1)  # ë˜ëŠ” SimpleRNN ë ˆì´ì–´ë„ ì‚¬ìš© ê°€ëŠ¥\n",
    "])\n",
    "\n",
    "model_rec_deep.compile(loss='mse', optimizer='adam')\n",
    "```\n",
    "\n",
    "```\n",
    "keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1])\n",
    "```\n",
    "#### ì²«ë²ˆì§¸ ë ˆì´ì–´\n",
    "- ìœ ë‹› 20ê°œë¥¼ ê°€ì§„ RNN ì…€\n",
    "- return_sequences=True ì„¤ì •:\n",
    "  - ê° ì‹œê°„ ë‹¨ê³„ë§ˆë‹¤ì˜ ì¶œë ¥ì„ ì „ì²´ ì‹œí€€ìŠ¤ë¡œ ë°˜í™˜í•¨\n",
    "  - ì¦‰, ì¶œë ¥ í˜•íƒœëŠ” (batch_size, timesteps, 20)\n",
    "  - ë‘ ë²ˆì§¸ RNN ì¸µì´ ì „ì²´ ì‹œí€€ìŠ¤ë¥¼ ë°›ì•„ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •í•¨\n",
    "\n",
    "#### ë‘ë²ˆì§¸ ë ˆì´ì–´\n",
    "- ìœ ë‹› 20ê°œ\n",
    "- **ì—¬ê¸°ì„œëŠ” return_sequences=False (ê¸°ë³¸ê°’)**\n",
    "- ë§ˆì§€ë§‰ ì‹œê°„ ë‹¨ê³„ì˜ ì¶œë ¥ë§Œ ë°˜í™˜í•¨ (batch_size, 20)\n",
    "\n",
    "#### Dense ë ˆì´ì–´\n",
    "- ë§ˆì§€ë§‰ RNN ì¶œë ¥ ë²¡í„°ë¥¼ ì…ë ¥ë°›ì•„, ìµœì¢… ì˜ˆì¸¡ê°’ í•˜ë‚˜ë¥¼ ì¶œë ¥ (íšŒê·€)\n",
    "\n",
    "### í•µì‹¬ ìš”ì : return_sequences=True\n",
    "- ì™œ í•„ìš”í•œê°€ìš”?\n",
    "  - RNNì˜ ì¶œë ¥ì´ ë‹¤ìŒ RNN ë ˆì´ì–´ë¡œ ì—°ê²°ë  ê²½ìš°, ì „ì²´ ì‹œí€€ìŠ¤ë¥¼ ë°˜í™˜í•´ì•¼ ë‹¤ìŒ ë ˆì´ì–´ê°€ ê·¸ê²ƒì„ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "  - ë”°ë¼ì„œ RNNì„ ì—¬ëŸ¬ ì¸µ ìŒ“ì„ ë•ŒëŠ”, ì¤‘ê°„ RNNì—ëŠ” return_sequences=True ê°€ ê¼­ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ìŠ¬ë¼ì´ë“œ ìš”ì•½ ë° ì„¤ëª…\n",
    "#### ğŸ”¹ ë¬¸ì¥ 1:\n",
    "> We must set `return_sequences=True` to propagate a sequence of errors instead of just the last errors\n",
    "\n",
    "**í•´ì„:**  \n",
    "`return_sequences=True`ë¥¼ ì„¤ì •í•´ì•¼, **ì˜¤ì°¨ë¥¼ ì‹œí€€ìŠ¤ ì „ì²´ì— ëŒ€í•´ ì „íŒŒ**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´, **ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì˜ ì¶œë ¥ì— ëŒ€í•œ ì˜¤ì°¨ë§Œ ì „íŒŒ**ë©ë‹ˆë‹¤.\n",
    "\n",
    "**ì„¤ëª…:**  \n",
    "- RNNì„ ì—¬ëŸ¬ ì¸µ ìŒ“ì„ ë•Œ, ì¤‘ê°„ RNN ë ˆì´ì–´ëŠ” ì „ì²´ ì‹œí€€ìŠ¤ë¥¼ ì¶œë ¥í•´ì•¼ ë‹¤ìŒ RNNë„ ì „ì²´ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- ì¦‰, `return_sequences=True` ì—†ìœ¼ë©´, ë‹¤ìŒ RNN ë ˆì´ì–´ëŠ” ì‹œí€€ìŠ¤ê°€ ì•„ë‹ˆë¼ ë‹¨ì¼ ë²¡í„°ë§Œ ë°›ê²Œ ë˜ì–´, **í•™ìŠµí•  ì •ë³´ê°€ ì œí•œ**ë©ë‹ˆë‹¤.\n",
    "\n",
    "#### ğŸ”¹ ë¬¸ì¥ 2:\n",
    "> We are still using a single output at the last layer.\n",
    "\n",
    "**í•´ì„:**  \n",
    "ëª¨ë¸ì˜ ë§ˆì§€ë§‰ ì¶œë ¥ì€ ì—¬ì „íˆ **í•˜ë‚˜ì˜ ì¶œë ¥ ê°’(single output)** ì…ë‹ˆë‹¤.\n",
    "\n",
    "**ì„¤ëª…:**  \n",
    "- ì˜ˆë¥¼ ë“¤ì–´, ë§ˆì§€ë§‰ RNN ë˜ëŠ” Dense ë ˆì´ì–´ì—ì„œ **ë§ˆì§€ë§‰ ì‹œì ì˜ ê°’ë§Œ ì˜ˆì¸¡**í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "- ì´ëŠ” ë‹¨ì¼ íƒ€ê¹ƒ ì˜ˆì¸¡ (ì˜ˆ: ë‹¤ìŒ ë‚ ì˜ ì˜¨ë„ ì˜ˆì¸¡)ì— ì í•©í•©ë‹ˆë‹¤.\n",
    "\n",
    "#### ğŸ”¹ ë¬¸ì¥ 3:\n",
    "> We can also replace the last layer by a Dense layer, it wouldn't change the performance and it would make the training faster. We just need to remove the `return_sequences=True` from the second layer\n",
    "\n",
    "**í•´ì„:**  \n",
    "ë§ˆì§€ë§‰ RNN ë ˆì´ì–´ë¥¼ Dense ë ˆì´ì–´ë¡œ ë°”ê¿”ë„ ì„±ëŠ¥ì—ëŠ” ì˜í–¥ì„ ì£¼ì§€ ì•Šìœ¼ë©°, **í•™ìŠµ ì†ë„ëŠ” ë¹¨ë¼ì§‘ë‹ˆë‹¤**. ì´ë•ŒëŠ” ë‘ ë²ˆì§¸ RNN ë ˆì´ì–´ì˜ `return_sequences=True` ì„¤ì •ì„ **ì‚­ì œí•´ì•¼ í•©ë‹ˆë‹¤**.\n",
    "\n",
    "**ì„¤ëª…:**  \n",
    "- `SimpleRNN`ì€ ë°˜ë³µ ì—°ì‚°ì´ ìˆì–´ ëŠë¦½ë‹ˆë‹¤.\n",
    "- ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ë§Œ ì²˜ë¦¬í•œë‹¤ë©´ êµ³ì´ RNNì´ í•„ìš” ì—†ê³ , **Dense ë ˆì´ì–´ í•˜ë‚˜ë¡œ ì¶©ë¶„**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- ì´ëŸ´ ë• ì¤‘ê°„ ë ˆì´ì–´ê¹Œì§€ë§Œ ì‹œí€€ìŠ¤ ì²˜ë¦¬í•˜ê³ , ë§ˆì§€ë§‰ ì¶œë ¥ë§Œ ë°›ì•„ì„œ Denseë¡œ ì—°ê²°í•˜ë©´ ë” ë¹ ë¥´ê²Œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "#### ğŸ“Œ ìš”ì•½ ë„ì‹\n",
    "| êµ¬ì„± ë°©ì‹ | ì„¤ëª… |\n",
    "|-----------|------|\n",
    "| `return_sequences=True` | ì—¬ëŸ¬ RNN ì¸µì„ ìŒ“ì„ ë•Œ í•„ìš” (ì˜¤ì°¨ë¥¼ ì‹œí€€ìŠ¤ ì „ì²´ë¡œ ì „íŒŒ) |\n",
    "| ë§ˆì§€ë§‰ ì¶œë ¥ì´ RNN | ë³µì¡í•˜ì§€ë§Œ ì‹œí€€ìŠ¤ ê¸°ë°˜ ì²˜ë¦¬ ê°€ëŠ¥ |\n",
    "| ë§ˆì§€ë§‰ ì¶œë ¥ì´ Dense | ë” ë¹ ë¥´ê³  ê°„ë‹¨í•¨ (ë‹¨ì¼ ì¶œë ¥) |\n",
    "| ë§ˆì§€ë§‰ RNNì— `return_sequences=False` | Denseì™€ ì—°ê²° ì‹œ í•„ìš” |\n",
    "\n",
    "## GRU\n",
    "```python\n",
    "model = keras.models.Sequential([\n",
    "            keras.layers.GRU(20, return_sequences=True, input_shape=[None, 1]),\n",
    "            keras.layers.GRU(20, return_sequences=True),\n",
    "            keras.layers.TimeDistributed(keras.layers.Dense(3))\n",
    "        ])\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
    "```\n",
    "\n",
    "### ğŸ“Œ TimeDistributedì™€ GRUì˜ ê´€ê³„ ì •ë¦¬\n",
    "#### ğŸ§  TimeDistributedë€?\n",
    "> `TimeDistributed(layer)`ëŠ” ì…ë ¥ ì‹œí€€ìŠ¤ì˜ **ê° íƒ€ì„ìŠ¤í…(timestep)** ì— ëŒ€í•´ ë™ì¼í•œ `layer`ë¥¼ **ë…ë¦½ì ìœ¼ë¡œ ì ìš©**í•©ë‹ˆë‹¤.\n",
    "\n",
    "- ì¼ë°˜ì ì¸ ë ˆì´ì–´(`Dense`, `Conv2D` ë“±)ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 2D ì…ë ¥ `(batch_size, features)`ë¥¼ ê¸°ëŒ€í•©ë‹ˆë‹¤.\n",
    "- í•˜ì§€ë§Œ ì‹œê³„ì—´/ì‹œí€€ìŠ¤ ë°ì´í„°ëŠ” 3D ì…ë ¥ `(batch_size, timesteps, features)`ì…ë‹ˆë‹¤.\n",
    "- `TimeDistributed`ëŠ” ê° timestepì— ëŒ€í•´ **ê°œë³„ì ìœ¼ë¡œ layerë¥¼ ë°˜ë³µ ì ìš©**í•©ë‹ˆë‹¤.\n",
    "\n",
    "#### âœ… GRUì™€ í•¨ê»˜ ì“°ëŠ” ì˜ˆì‹œ\n",
    "```python\n",
    "from tensorflow.keras.layers import TimeDistributed, Dense, GRU, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "input = Input(shape=(30, 10))  # 30 timesteps, 10 features\n",
    "x = TimeDistributed(Dense(32))(input)  # ê° timestepì— ëŒ€í•´ Dense(32) ì ìš©\n",
    "x = GRU(64)(x)  # ì‹œí€€ìŠ¤ë¥¼ ë”°ë¼ GRU ì²˜ë¦¬\n",
    "model = Model(inputs=input, outputs=x)\n",
    "```\n",
    "#### TimeDistributed ë¥¼ ì“°ëŠ” ì´ìœ \n",
    "| ìƒí™©                                    | í•´ê²°                                                |\n",
    "| ------------------------------------- | ------------------------------------------------- |\n",
    "| ì‹œí€€ìŠ¤ ë°ì´í„°ì— `Dense`, `Conv` ë“±ì„ ì ìš©í•˜ê³  ì‹¶ì„ ë•Œ | `TimeDistributed(layer)`ë¥¼ í†µí•´ ê° timestepì— ë…ë¦½ ì ìš© ê°€ëŠ¥ |\n",
    "| GRUì— ë„£ê¸° ì „ ë°ì´í„° ì „ì²˜ë¦¬                     | `TimeDistributed(Dense)`ë¡œ ê° timestepì„ ë¨¼ì € ì„ë² ë”©/ë³€í˜•   |\n",
    "\n",
    "#### êµ¬ì¡° ë¹„êµ\n",
    "| ë ˆì´ì–´ êµ¬ì„±                           | ì˜ë¯¸                   |\n",
    "| -------------------------------- | -------------------- |\n",
    "| `TimeDistributed(Dense)`         | ê° timestepì— Dense ì ìš© |\n",
    "| `GRU`                            | ì‹œí€€ìŠ¤ë¥¼ ì‹œê°„ ìˆœì„œì— ë”°ë¼ ì²˜ë¦¬    |\n",
    "| `TimeDistributed(Dense)` + `GRU` | Denseë¡œ ì „ì²˜ë¦¬ í›„ ì‹œí€€ìŠ¤ ì²˜ë¦¬  |\n",
    "\n",
    "https://keras.io/api/layers/recurrent_layers/time_distributed/\n",
    "\n",
    "Time Series - Transformers ( Jan 2021 )\n",
    "- https://arxiv.org/pdf/1907.05321\n",
    "  \n",
    "### ğŸ” Recurrent Neurons & Recurrent Layers ì •ë¦¬\n",
    "#### 1ï¸âƒ£ Recurrent Neurons (ìˆœí™˜ ë‰´ëŸ°)\n",
    "##### âœ… ì •ì˜\n",
    "Recurrent Neuronì€ **ê³¼ê±°ì˜ ì •ë³´ë¥¼ í˜„ì¬ ê³„ì‚°ì— í™œìš©í•  ìˆ˜ ìˆëŠ” ë‰´ëŸ°**ì…ë‹ˆë‹¤.  \n",
    "ì´ì „ ì¶œë ¥ê°’(ë˜ëŠ” ìƒíƒœ)ì„ í˜„ì¬ ì…ë ¥ê³¼ í•¨ê»˜ ì²˜ë¦¬í•˜ì—¬, ì‹œí€€ìŠ¤ ë°ì´í„°ì˜ ì‹œê°„ ì˜ì¡´ì„±ì„ ëª¨ë¸ë§í•©ë‹ˆë‹¤.\n",
    "\n",
    "##### âœ… ê³„ì‚° ë°©ì‹\n",
    "ìˆ˜ì‹:\n",
    "$\n",
    "h_t = f(W \\cdot x_t + U \\cdot h_{t-1} + b)\n",
    "$\n",
    "\n",
    "- $ x_t $: í˜„ì¬ ì…ë ¥\n",
    "- $ h_{t-1} $: ì´ì „ ì‹œì ì˜ ì€ë‹‰ ìƒíƒœ\n",
    "- $ h_t $: í˜„ì¬ ì‹œì ì˜ ì€ë‹‰ ìƒíƒœ (ì¶œë ¥)\n",
    "- $ W, U, b $: í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°\n",
    "- $ f $: í™œì„±í™” í•¨ìˆ˜ (tanh ë˜ëŠ” ReLU ë“±)\n",
    "\n",
    "##### âœ… íŠ¹ì§•\n",
    "- ì‹œê°„ ì •ë³´ë¥¼ ê¸°ì–µ (Memory)\n",
    "- ë°˜ë³µ êµ¬ì¡°ë¡œ êµ¬í˜„ë¨\n",
    "- Gradient Vanishing ë¬¸ì œê°€ ìˆìŒ (â†’ LSTM/GRU ë“±ì¥ ë°°ê²½)\n",
    "\n",
    "#### 2ï¸âƒ£ Recurrent Layers (ìˆœí™˜ ê³„ì¸µ)\n",
    "##### âœ… ì •ì˜\n",
    "Recurrent LayerëŠ” ì—¬ëŸ¬ Recurrent Neuronì´ ëª¨ì—¬ì„œ **ì‹œí€€ìŠ¤ ì „ì²´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê³„ì¸µ**ì…ë‹ˆë‹¤.  \n",
    "ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ì‹œê°„ ìˆœì„œëŒ€ë¡œ ìˆœì°¨ ì²˜ë¦¬í•˜ë©°, ë§¤ ì‹œì ë§ˆë‹¤ ë‰´ëŸ°ì´ ê³µìœ ëœ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "##### âœ… ëŒ€í‘œì ì¸ ì¢…ë¥˜\n",
    "| ê³„ì¸µ ì¢…ë¥˜       | íŠ¹ì§• |\n",
    "|----------------|------|\n",
    "| SimpleRNN      | ê¸°ë³¸ ìˆœí™˜ êµ¬ì¡°. ì§§ì€ ì‹œí€€ìŠ¤ì— ì í•© |\n",
    "| LSTM           | ì¥ê¸° ì˜ì¡´ì„± í•™ìŠµ ê°€ëŠ¥. ê²Œì´íŠ¸ 3ê°œ í¬í•¨ |\n",
    "| GRU            | êµ¬ì¡°ê°€ ê°„ë‹¨í•œ LSTM. ê²Œì´íŠ¸ 2ê°œ ì‚¬ìš© |\n",
    "| BidirectionalRNN | ì‹œí€€ìŠ¤ë¥¼ ì •ë°©í–¥ê³¼ ì—­ë°©í–¥ ëª¨ë‘ ì²˜ë¦¬ |\n",
    "\n",
    "#### 3ï¸âƒ£ RNN vs Feedforward NN ë¹„êµ\n",
    "| í•­ëª©              | Feedforward NN       | Recurrent NN         |\n",
    "|-------------------|----------------------|-----------------------|\n",
    "| ì…ë ¥ êµ¬ì¡°         | ê³ ì •ëœ ì…ë ¥ ë²¡í„°     | ì‹œí€€ìŠ¤ (ì‹œê°„ ì˜ì¡´ì ) |\n",
    "| íŒŒë¼ë¯¸í„° ê³µìœ      | X                    | O (ì‹œê°„ì¶• ë”°ë¼ ê³µìœ ) |\n",
    "| ê³¼ê±° ì •ë³´ ì‚¬ìš©    | ë¶ˆê°€ëŠ¥               | ê°€ëŠ¥ (ë©”ëª¨ë¦¬ ì¡´ì¬)   |\n",
    "| ëŒ€í‘œ í™œìš© ë¶„ì•¼    | ì´ë¯¸ì§€ ë¶„ë¥˜ ë“±       | NLP, ì‹œê³„ì—´ ë¶„ì„ ë“±   |\n",
    "\n",
    "\n",
    "#### 4ï¸âƒ£ ì‚¬ìš© ì˜ˆì‹œ\n",
    "- ìì—°ì–´ ì²˜ë¦¬ (ë²ˆì—­, ê°ì„± ë¶„ì„, ì±—ë´‡ ë“±)\n",
    "- ì‹œê³„ì—´ ì˜ˆì¸¡ (ì£¼ê°€, ë‚ ì”¨ ë“±)\n",
    "- ìŒì•…, ìŒì„±, ë™ì‘ ì¸ì‹ ë“± ì‹œê°„ ìˆœì„œê°€ ì¤‘ìš”í•œ ë¬¸ì œ\n",
    "\n",
    "#### 5ï¸âƒ£ ì¶”ê°€ ê°œë…: `return_sequences`\n",
    "| íŒŒë¼ë¯¸í„°            | ì˜ë¯¸ |\n",
    "|---------------------|------|\n",
    "| `return_sequences=False` | ë§ˆì§€ë§‰ ì‹œì ì˜ ì¶œë ¥ë§Œ ë°˜í™˜ (ê¸°ë³¸ê°’) |\n",
    "| `return_sequences=True`  | ëª¨ë“  ì‹œì ì˜ ì¶œë ¥ ë°˜í™˜ (ì‹œí€€ìŠ¤ â†’ ì‹œí€€ìŠ¤) |\n",
    "\n",
    "44page ì´í›„ì—ëŠ” ì „ì²´ì ìœ¼ë¡œ graph ë° code ë“¤ì´ ë‚˜ì™€ ìˆëŠ”ë° ì„¤ëª…ì´ ë˜ì–´ ìˆì§€ ì•Šì•„ì„œ ì •í™•í•œ ì´í•´ê°€ ì–´ë ¤ì›€."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2028dd70-cd68-47e0-b9c5-0438aa04fa22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
