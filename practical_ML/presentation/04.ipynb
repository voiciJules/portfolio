{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c783ff91-2fb2-43ce-8803-1fb380c6d96f",
   "metadata": {},
   "source": [
    "# Presentation 04. Processing Sequences Using RNNs and CNNs, Time2Vec\n",
    "\n",
    "## 📈 Time Series Components: Systematic vs Non-Systematic\n",
    "### 🔷 Systematic Components\n",
    "- 의미: **일관성(consistency)** 또는 **반복성(recurrence)**을 가지는 구성 요소\n",
    "- 특징: **모델링이 가능**하며 수학적으로 설명할 수 있음\n",
    "- 예시:\n",
    "  - **Trend (추세)**: 시간에 따라 증가하거나 감소하는 경향\n",
    "  - **Seasonality (계절성)**: 주기적으로 반복되는 패턴\n",
    "  - **Cyclic patterns (순환성)**: 일정하지는 않지만 반복되는 장기적 변화\n",
    "\n",
    "### 🔶 Non-Systematic Components\n",
    "- 의미: **직접적으로 모델링할 수 없는 불규칙한 구성 요소**\n",
    "- 특징: 예측 불가능하고 **무작위성(randomness)**을 내포함\n",
    "- 예시:\n",
    "  - **Irregular component (불규칙성)** 또는 **noise (노이즈)**: 갑작스러운 사고, 자연재해, 예외적 사건 등\n",
    "\n",
    "### ✅ 요약\n",
    "\n",
    "| 구분             | 설명                                        | 예시                  |\n",
    "|------------------|---------------------------------------------|------------------------|\n",
    "| Systematic       | 설명 가능하고 예측 가능한 구조적 요소       | Trend, Seasonality     |\n",
    "| Non-Systematic   | 설명 불가한 무작위적 요소                   | Noise, Irregularity    |\n",
    "\n",
    "## 🧱 Time Series Components\n",
    "Time series data can be broken down into several core components that help explain the structure and behavior of the data over time.\n",
    "\n",
    "### 1️⃣ **Level**\n",
    "- 📌 의미: 시계열의 **기본 평균값** 수준\n",
    "- 예시: 장기적으로 주가가 100달러 근처에서 유지된다면, 그 값이 level\n",
    "\n",
    "### 2️⃣ **Trend**\n",
    "- 📈 의미: 시간이 지남에 따라 데이터가 **지속적으로 증가하거나 감소하는 방향**\n",
    "- 예시: 인구수, 주택 가격, 주가의 장기 상승/하락 경향\n",
    "\n",
    "### 3️⃣ **Seasonality**\n",
    "- 🔁 의미: 일정 주기마다 반복되는 **단기적인 주기적 패턴**\n",
    "- 예시: 계절에 따른 에어컨 판매량, 요일에 따른 웹사이트 방문자 수\n",
    "\n",
    "### 4️⃣ **Noise (Irregularity)**\n",
    "- 🎲 의미: **무작위적인 변동**으로, 예측이 어렵고 일정한 패턴이 없음\n",
    "- 예시: 갑작스러운 뉴스로 인한 주가 급등/급락, 예외적인 기상 이변\n",
    "\n",
    "### ✅ 요약표\n",
    "\n",
    "| 구성 요소     | 설명                                       | 예시                        |\n",
    "|---------------|--------------------------------------------|-----------------------------|\n",
    "| Level         | 시계열의 평균적인 기본값                   | 평균 심박수, 평균 수요량     |\n",
    "| Trend         | 장기적인 증가/감소 경향                    | 부동산 가격 상승             |\n",
    "| Seasonality   | 짧은 주기의 반복적인 패턴                  | 여름철 에어컨 수요 증가      |\n",
    "| Noise         | 설명되지 않는 무작위적 변동                | 돌발 사건, 자연재해          |\n",
    "\n",
    "## 🔄 Autocorrelation in Time Series\n",
    "### 📌 What is Autocorrelation?\n",
    "- **Autocorrelation** is the **correlation between a time series and a lagged version of itself**.\n",
    "- It shows how current values of the series relate to **past values** at different lags.\n",
    "\n",
    "### 📈 Why is it important?\n",
    "- Helps **identify seasonality**, trends, and repeating patterns in time series data.\n",
    "- Useful for:\n",
    "  - Detecting **repeating cycles** (e.g., weekly, monthly patterns)\n",
    "  - Understanding **persistence** or **momentum** in the data\n",
    "  - Choosing the right number of lags in models like **ARIMA**\n",
    "\n",
    "### Time Series Forecasting with Deep Learning and Attention Mechanism\n",
    "https://towardsdatascience.com/time-series-forecasting-with-deep-learning-and-attention-mechanism-2d001fc871fc/\n",
    "\n",
    "## Disadvantages of Recurrent Neural Network\n",
    "On long time series; suffer from the vanishing gradient or exploding gradient, the parameters either don’t change that much (Vanishing Gradient) or they lead to numeric instability and chaotic behavior (Exploding Gradient).\n",
    "\n",
    "## Case Study in RNN :\n",
    "Training a RNN on a large network; then notice that somewhere in the middle of the 42nd epoch the cost that you monitor becomes\n",
    "NaN. Identify the source of the problem? How would fix it ?\n",
    "- Problem is gradient exploding :\n",
    "  - Gradient errors can accumulate during an update and result in very large gradients.\n",
    "  - Result in large updates to the network weights, and in turn, an unstable network.\n",
    "  - At an extreme, the values of weights can become so large as to overflow and result in NaN values.\n",
    "- Solutions :\n",
    "  - Reducing learning rate\n",
    "  - Using Adam optimizer (Adam is the adaptive optimizers in most of the cases. Good with sparse data)\n",
    "  - Using gradient clipping (minimum or maximum value if the gradient exceeded an expected range)\n",
    "  - Adding regularization.\n",
    "  - Network size reduction.\n",
    "  - Increasing the batch size.\n",
    "  - Using LeakyReLU.\n",
    "  - Checking whether the input are Nan or zeroes.\n",
    "  - !!!!Normalizing ( Transformation) the input data\n",
    "\n",
    "## LSTM (Long Short-Term Memory) 설명\n",
    "LSTM은 RNN의 일종으로, **장기 의존성 문제**(long-term dependency)를 해결하기 위해 고안된 구조입니다. 아래는 LSTM의 주요 구성 요소와 작동 원리에 대한 설명입니다.\n",
    "\n",
    "<img src=\"./lstm.png\" alt=\"LSTM 구조\" width=\"50%\">\n",
    "\n",
    "### 주요 구성 요소\n",
    "#### 1. Forget Gate (망각 게이트)\n",
    "- 역할: 이전 셀 상태 `C(t-1)`에서 **무엇을 잊을지 결정**합니다.\n",
    "- 계산식:  \n",
    "  `f(t) = σ(W_f·x(t) + U_f·h(t-1) + b_f)`\n",
    "- 출력값은 0~1 사이이며, 0이면 완전히 잊고 1이면 완전히 기억합니다.\n",
    "\n",
    "#### 2. Input Gate (입력 게이트)\n",
    "- 역할: **현재 입력 x(t)**를 얼마나 셀 상태에 반영할지 결정합니다.\n",
    "- 계산식:  \n",
    "  `i₁(t) = σ(W_i·x(t) + U_i·h(t-1) + b_i)`  \n",
    "  `i₂(t) = tanh(W_g·x(t) + U_g·h(t-1) + b_g)`\n",
    "- 새로운 정보는 `i₁(t) * i₂(t)`로 계산되어 셀 상태에 추가됩니다.\n",
    "\n",
    "#### 3. Cell State Update (셀 상태 갱신)\n",
    "- 셀 상태는 아래 방식으로 갱신됩니다:  \n",
    "  `C(t) = f(t) * C(t-1) + i₁(t) * i₂(t)`\n",
    "\n",
    "#### 4. Output Gate (출력 게이트)\n",
    "- 역할: 최종 출력값 `h(t)`를 결정합니다.\n",
    "- 계산식:  \n",
    "  `o(t) = σ(W_o·x(t) + U_o·h(t-1) + b_o)`  \n",
    "  `h(t) = o(t) * tanh(C(t))`\n",
    "- 출력은 셀 상태의 활성화값을 기반으로 합니다.\n",
    "\n",
    "### 요약\n",
    "- **Forget Gate**: 무엇을 잊을지\n",
    "- **Input Gate**: 무엇을 기억할지\n",
    "- **Output Gate**: 무엇을 출력할지\n",
    "이 구조 덕분에 LSTM은 **장기적인 의존성 정보를 더 잘 기억하고 활용**할 수 있습니다.\n",
    "\n",
    "## GRU (Gated Recurrent Unit) 설명\n",
    "GRU는 LSTM보다 구조가 단순하면서도 효과적인 **게이트 기반 RNN 아키텍처**입니다. GRU는 두 개의 게이트만 사용하여 과거 정보를 조절합니다: **Reset Gate**와 **Update Gate**.\n",
    "\n",
    "<img src=\"./gru.png\" alt=\"GRU 구조\" width=\"50%\">\n",
    "\n",
    "### GRU 구성 요소\n",
    "\n",
    "#### 1. Reset Gate `r(t)`\n",
    "- 역할: 과거의 정보를 **얼마나 잊을지** 결정합니다.\n",
    "- 계산식:  \n",
    "  `r(t) = σ(W_r·x(t) + U_r·h(t-1))`\n",
    "- 작은 값이면 과거 정보를 덜 반영하고, 큰 값이면 더 많이 반영합니다.\n",
    "\n",
    "#### 2. Update Gate `u(t)`\n",
    "- 역할: 과거의 hidden state를 **얼마나 유지할지** 결정합니다.\n",
    "- 계산식:  \n",
    "  `u(t) = σ(W_z·x(t) + U_z·h(t-1))`\n",
    "- `u(t)`가 1이면 이전 hidden state를 그대로 사용하고, 0이면 새로 계산된 값을 사용합니다.\n",
    "\n",
    "#### 3. Candidate Hidden State `h̃(t)`\n",
    "- Reset Gate를 통해 이전 상태 정보를 선택적으로 사용하여 **새로운 상태 후보**를 만듭니다.\n",
    "- 계산식:  \n",
    "  `h̃(t) = tanh(W·x(t) + U·(r(t) * h(t-1)))`\n",
    "\n",
    "#### 4. 최종 Hidden State `h(t)`\n",
    "- Update Gate를 통해 이전 상태와 새로운 상태를 **가중 평균**하여 현재 상태를 결정합니다.\n",
    "- 계산식:  \n",
    "  `h(t) = u(t) * h(t-1) + (1 - u(t)) * h̃(t)`\n",
    "\n",
    "### 요약\n",
    "- **Reset Gate** `r(t)`: 과거 정보를 얼마나 잊을지 결정  \n",
    "- **Update Gate** `u(t)`: 과거 상태를 얼마나 유지할지 결정  \n",
    "- **GRU는 셀 상태 없이 hidden state만 유지하며**, LSTM보다 계산이 간단합니다.\n",
    "\n",
    "### GRU vs. LSTM\n",
    "\n",
    "| 항목        | GRU                  | LSTM                     |\n",
    "|-------------|-----------------------|--------------------------|\n",
    "| 게이트 수    | 2개 (Reset, Update)   | 3개 (Forget, Input, Output) |\n",
    "| 셀 상태      | 없음 (h만 유지)        | 있음 (h, C 모두 유지)       |\n",
    "| 계산 복잡도  | 낮음                  | 비교적 높음                 |\n",
    "| 성능 차이    | 거의 없음 (데이터에 따라 다름) | 거의 없음 (데이터에 따라 다름) |\n",
    "  \n",
    "### 📘 CNN-LSTM 모델이란?\n",
    "CNN-LSTM은 **Convolutional Neural Network (CNN)**과 **Long Short-Term Memory (LSTM)**를 결합한 모델로, 주로 **시계열 예측(time series forecasting)**에 사용됩니다.\n",
    "\n",
    "#### ✅ 구성:\n",
    "1. **Conv1D (1D Convolution Layer)**  \n",
    "   - 시계열 데이터를 입력받아, **국소적인 패턴(local features)**을 추출합니다.  \n",
    "   - 예: 주기성, 급격한 변화 등\n",
    "2. **LSTM Layer**  \n",
    "   - CNN이 추출한 시계열 특징들을 시퀀스로 보고, **시간에 따른 의존성**을 학습합니다.\n",
    "3. **Dense Layer (출력)**  \n",
    "   - 최종 예측 값을 출력합니다 (예: 미래의 주가, 온도 등)\n",
    "\n",
    "### 🧠 Conv1D란?\n",
    "- **Conv1D**는 1차원 시계열 데이터(예: `[x₁, x₂, ..., xₙ]`)에 대해 커널을 sliding하여 **연속적인 구간에서 특징(feature)**을 추출합니다.\n",
    "- 커널(kernel)은 작은 창(window)처럼 작동하며, 각 위치에서 **곱하고 더하는 계산(convolution)**을 수행합니다.\n",
    "\n",
    "### ⚖️ Causal Convolution vs. Standard Convolution\n",
    "| 항목 | Standard Convolution | Causal Convolution |\n",
    "|------|-----------------------|---------------------|\n",
    "| 📅 시간 순서 고려 | ❌ 미래 시점까지 커널이 봄 (t+1 등) | ✅ 과거 시점만 봄 (t 이하) |\n",
    "| 🧠 학습 정보 | 현재 + 미래 정보 사용 | 오직 과거 정보만 사용 |\n",
    "| ⏳ 시계열 예측에 적합 | ❌ | ✅ 매우 적합 |\n",
    "| 🧩 padding 방식 | 보통 `\"same\"` 또는 `\"valid\"` | `\"causal\"` |\n",
    "| 예시 | `[t-1, t, t+1]` 을 커널이 봄 | `[t-2, t-1, t]` 만 봄 (t+1 금지) |\n",
    "\n",
    "### 📌 왜 CNN-LSTM에 Conv1D + Causal Padding을 쓰나?\n",
    "- CNN은 데이터의 **짧은 패턴(local dependencies)**을 빠르게 잡아냅니다.\n",
    "- Causal padding은 LSTM에 전달할 때 **시간 순서를 보존**합니다.\n",
    "- LSTM은 그 후 **장기 의존성(long-term dependency)**을 학습합니다.\n",
    "\n",
    "이 조합은:  \n",
    "> \"짧은 패턴 → 시간 구조 이해 → 예측\"  \n",
    "이라는 이상적인 흐름을 만들어 줍니다.\n",
    "\n",
    "## WaveNet과 시계열 예측\n",
    "### 1. WaveNet 개요\n",
    "- WaveNet은 원래 음성 합성을 위해 개발된 딥러닝 모델로, **딥 컨볼루셔널 네트워크(CNN)** 기반이다.\n",
    "- 시계열 데이터의 **순차적 특성**을 모델링하는 데 강점이 있다.\n",
    "- 일반적인 RNN/LSTM과 달리, CNN 기반으로 **병렬처리가 가능**하며, 매우 긴 시계열 의존성도 캡처 가능하다.\n",
    "\n",
    "### 2. 핵심 아이디어: 인과적(카주얼) 컨볼루션 (Causal Convolution)\n",
    "- 출력 시점 t의 예측에 대해, 미래 정보(t+1, t+2...)를 사용하지 않고, 오직 과거(t, t-1, ...) 데이터만 활용.\n",
    "- 입력 시퀀스를 0으로 패딩하여 현재 시점 이전 정보만 컨볼루션 연산에 반영되도록 한다.\n",
    "\n",
    "### 3. Dilation Convolution (팽창 컨볼루션)\n",
    "- 커널 사이의 간격(간격 = dilation rate)을 두어, 넓은 시계열 범위를 효과적으로 한 번에 커버.\n",
    "- dilation rate가 점점 커져서 receptive field가 지수적으로 확장됨 → 매우 긴 과거 시점도 반영 가능.\n",
    "- 예: dilation = 1, 2, 4, 8, ...\n",
    "\n",
    "### 4. WaveNet의 시계열 예측 과정\n",
    "- 입력 시계열 데이터를 causal + dilation convolution 계층에 통과시켜 특징 추출.\n",
    "- 각 시점의 다음 값(또는 다음 확률 분포)을 예측.\n",
    "- 시계열 데이터를 시퀀스 단위로 점진적으로 생성하거나 예측 가능.\n",
    "\n",
    "### 5. WaveNet의 장점\n",
    "- 긴 시계열 의존성 캡처 가능 (RNN보다 더 효율적)\n",
    "- 병렬처리 가능 (RNN 대비 빠름)\n",
    "- 비선형성 강한 시계열 데이터에 효과적\n",
    "\n",
    "### more info : https://www.kilians.net/post/convolution-in-autoregressive-neural-networks/\n",
    "\n",
    "### fit_transform vs transform\n",
    "- https://towardsdatascience.com/what-and-why-behind-fit-transform-vs-transform-in-scikit-learn-78f915cf96fe/\n",
    "- 데이터를 전처리할 때, 전체 데이터를 한꺼번에 fit_transform() 하면 안 됩니다. 왜냐하면, 테스트 데이터를 포함해서 평균이나 분산을 계산하게 되면, 모델이 테스트 데이터의 정보를 미리 알게 되는 것이고, 그렇게 되면 훈련 성능이 과대평가되고, 실제 성능은 떨어질 수 있습니다 (데이터 누수). 따라서 훈련 데이터로만 fit_transform() 하고, 테스트 데이터는 그 기준으로 transform()만 해야 모델이 특정 데이터 분포에 치우치지 않게 되고, 테스트 데이터의 패턴을 몰래 배우지 않게 됩니다.\n",
    "\n",
    "## 🧠 Simple RNN 모델 설명\n",
    "### 📐 입력 형태 (Input Shape)\n",
    "\n",
    "```python\n",
    "X = (n_samples, n_timesteps, n_features)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(1, input_shape=[None, 1])\n",
    "])\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.005)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "h = model.fit(X_train, y_train, epochs=20)\n",
    "```\n",
    "\n",
    "- n_samples: 전체 시계열 샘플 수 (예: 1000개)\n",
    "- n_timesteps: 각 샘플에서 시간 단계 수 (예: 30일)\n",
    "- n_features: 각 시간 단계의 특성 수 (예: 1개 센서 값)\n",
    "- Sequential: 순차적 레이어 구성\n",
    "- SimpleRNN(1): 출력 유닛 1개인 RNN 셀\n",
    "- input_shape=[None, 1]:\n",
    "  - None: 유동적인 시간 길이\n",
    "  - 1: 각 타임스텝에서 입력 피처 수\n",
    "- Adam: 적응형 학습률을 갖는 효율적인 옵티마이저\n",
    "- learning_rate=0.005: 학습률 지정\n",
    "- loss=\"mse\": 평균제곱오차 (회귀 문제에 적합)\n",
    "- optimizer=optimizer: 위에서 설정한 옵티마이저 사용\n",
    "- X_train: 입력 시계열 데이터 (samples, timesteps, features)\n",
    "- y_train: 예측 대상 값 (예: 다음 날 온도)\n",
    "- epochs=20: 전체 데이터셋을 20번 반복 학습\n",
    "- h: 학습 중 손실값 등이 담긴 결과 객체 (History)\n",
    "\n",
    "## 🔁 Deep RNN과 `return_sequences=True` 설명\n",
    "### 📌 코드 구조\n",
    "```python\n",
    "model_rec_deep = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20),\n",
    "    keras.layers.Dense(1)  # 또는 SimpleRNN 레이어도 사용 가능\n",
    "])\n",
    "\n",
    "model_rec_deep.compile(loss='mse', optimizer='adam')\n",
    "```\n",
    "\n",
    "```\n",
    "keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1])\n",
    "```\n",
    "#### 첫번째 레이어\n",
    "- 유닛 20개를 가진 RNN 셀\n",
    "- return_sequences=True 설정:\n",
    "  - 각 시간 단계마다의 출력을 전체 시퀀스로 반환함\n",
    "  - 즉, 출력 형태는 (batch_size, timesteps, 20)\n",
    "  - 두 번째 RNN 층이 전체 시퀀스를 받아 처리할 수 있도록 설정함\n",
    "\n",
    "#### 두번째 레이어\n",
    "- 유닛 20개\n",
    "- **여기서는 return_sequences=False (기본값)**\n",
    "- 마지막 시간 단계의 출력만 반환함 (batch_size, 20)\n",
    "\n",
    "#### Dense 레이어\n",
    "- 마지막 RNN 출력 벡터를 입력받아, 최종 예측값 하나를 출력 (회귀)\n",
    "\n",
    "### 핵심 요점: return_sequences=True\n",
    "- 왜 필요한가요?\n",
    "  - RNN의 출력이 다음 RNN 레이어로 연결될 경우, 전체 시퀀스를 반환해야 다음 레이어가 그것을 처리할 수 있습니다.\n",
    "  - 따라서 RNN을 여러 층 쌓을 때는, 중간 RNN에는 return_sequences=True 가 꼭 필요합니다.\n",
    "\n",
    "### 슬라이드 요약 및 설명\n",
    "#### 🔹 문장 1:\n",
    "> We must set `return_sequences=True` to propagate a sequence of errors instead of just the last errors\n",
    "\n",
    "**해석:**  \n",
    "`return_sequences=True`를 설정해야, **오차를 시퀀스 전체에 대해 전파**할 수 있습니다. 그렇지 않으면, **마지막 타임스텝의 출력에 대한 오차만 전파**됩니다.\n",
    "\n",
    "**설명:**  \n",
    "- RNN을 여러 층 쌓을 때, 중간 RNN 레이어는 전체 시퀀스를 출력해야 다음 RNN도 전체 시퀀스를 처리할 수 있습니다.\n",
    "- 즉, `return_sequences=True` 없으면, 다음 RNN 레이어는 시퀀스가 아니라 단일 벡터만 받게 되어, **학습할 정보가 제한**됩니다.\n",
    "\n",
    "#### 🔹 문장 2:\n",
    "> We are still using a single output at the last layer.\n",
    "\n",
    "**해석:**  \n",
    "모델의 마지막 출력은 여전히 **하나의 출력 값(single output)** 입니다.\n",
    "\n",
    "**설명:**  \n",
    "- 예를 들어, 마지막 RNN 또는 Dense 레이어에서 **마지막 시점의 값만 예측**하고 있습니다.\n",
    "- 이는 단일 타깃 예측 (예: 다음 날의 온도 예측)에 적합합니다.\n",
    "\n",
    "#### 🔹 문장 3:\n",
    "> We can also replace the last layer by a Dense layer, it wouldn't change the performance and it would make the training faster. We just need to remove the `return_sequences=True` from the second layer\n",
    "\n",
    "**해석:**  \n",
    "마지막 RNN 레이어를 Dense 레이어로 바꿔도 성능에는 영향을 주지 않으며, **학습 속도는 빨라집니다**. 이때는 두 번째 RNN 레이어의 `return_sequences=True` 설정을 **삭제해야 합니다**.\n",
    "\n",
    "**설명:**  \n",
    "- `SimpleRNN`은 반복 연산이 있어 느립니다.\n",
    "- 마지막 시퀀스만 처리한다면 굳이 RNN이 필요 없고, **Dense 레이어 하나로 충분**할 수 있습니다.\n",
    "- 이럴 땐 중간 레이어까지만 시퀀스 처리하고, 마지막 출력만 받아서 Dense로 연결하면 더 빠르게 학습할 수 있습니다.\n",
    "\n",
    "#### 📌 요약 도식\n",
    "| 구성 방식 | 설명 |\n",
    "|-----------|------|\n",
    "| `return_sequences=True` | 여러 RNN 층을 쌓을 때 필요 (오차를 시퀀스 전체로 전파) |\n",
    "| 마지막 출력이 RNN | 복잡하지만 시퀀스 기반 처리 가능 |\n",
    "| 마지막 출력이 Dense | 더 빠르고 간단함 (단일 출력) |\n",
    "| 마지막 RNN에 `return_sequences=False` | Dense와 연결 시 필요 |\n",
    "\n",
    "## GRU\n",
    "```python\n",
    "model = keras.models.Sequential([\n",
    "            keras.layers.GRU(20, return_sequences=True, input_shape=[None, 1]),\n",
    "            keras.layers.GRU(20, return_sequences=True),\n",
    "            keras.layers.TimeDistributed(keras.layers.Dense(3))\n",
    "        ])\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
    "```\n",
    "\n",
    "### 📌 TimeDistributed와 GRU의 관계 정리\n",
    "#### 🧠 TimeDistributed란?\n",
    "> `TimeDistributed(layer)`는 입력 시퀀스의 **각 타임스텝(timestep)** 에 대해 동일한 `layer`를 **독립적으로 적용**합니다.\n",
    "\n",
    "- 일반적인 레이어(`Dense`, `Conv2D` 등)는 기본적으로 2D 입력 `(batch_size, features)`를 기대합니다.\n",
    "- 하지만 시계열/시퀀스 데이터는 3D 입력 `(batch_size, timesteps, features)`입니다.\n",
    "- `TimeDistributed`는 각 timestep에 대해 **개별적으로 layer를 반복 적용**합니다.\n",
    "\n",
    "#### ✅ GRU와 함께 쓰는 예시\n",
    "```python\n",
    "from tensorflow.keras.layers import TimeDistributed, Dense, GRU, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "input = Input(shape=(30, 10))  # 30 timesteps, 10 features\n",
    "x = TimeDistributed(Dense(32))(input)  # 각 timestep에 대해 Dense(32) 적용\n",
    "x = GRU(64)(x)  # 시퀀스를 따라 GRU 처리\n",
    "model = Model(inputs=input, outputs=x)\n",
    "```\n",
    "#### TimeDistributed 를 쓰는 이유\n",
    "| 상황                                    | 해결                                                |\n",
    "| ------------------------------------- | ------------------------------------------------- |\n",
    "| 시퀀스 데이터에 `Dense`, `Conv` 등을 적용하고 싶을 때 | `TimeDistributed(layer)`를 통해 각 timestep에 독립 적용 가능 |\n",
    "| GRU에 넣기 전 데이터 전처리                     | `TimeDistributed(Dense)`로 각 timestep을 먼저 임베딩/변형   |\n",
    "\n",
    "#### 구조 비교\n",
    "| 레이어 구성                           | 의미                   |\n",
    "| -------------------------------- | -------------------- |\n",
    "| `TimeDistributed(Dense)`         | 각 timestep에 Dense 적용 |\n",
    "| `GRU`                            | 시퀀스를 시간 순서에 따라 처리    |\n",
    "| `TimeDistributed(Dense)` + `GRU` | Dense로 전처리 후 시퀀스 처리  |\n",
    "\n",
    "https://keras.io/api/layers/recurrent_layers/time_distributed/\n",
    "\n",
    "Time Series - Transformers ( Jan 2021 )\n",
    "- https://arxiv.org/pdf/1907.05321\n",
    "  \n",
    "### 🔁 Recurrent Neurons & Recurrent Layers 정리\n",
    "#### 1️⃣ Recurrent Neurons (순환 뉴런)\n",
    "##### ✅ 정의\n",
    "Recurrent Neuron은 **과거의 정보를 현재 계산에 활용할 수 있는 뉴런**입니다.  \n",
    "이전 출력값(또는 상태)을 현재 입력과 함께 처리하여, 시퀀스 데이터의 시간 의존성을 모델링합니다.\n",
    "\n",
    "##### ✅ 계산 방식\n",
    "수식:\n",
    "$\n",
    "h_t = f(W \\cdot x_t + U \\cdot h_{t-1} + b)\n",
    "$\n",
    "\n",
    "- $ x_t $: 현재 입력\n",
    "- $ h_{t-1} $: 이전 시점의 은닉 상태\n",
    "- $ h_t $: 현재 시점의 은닉 상태 (출력)\n",
    "- $ W, U, b $: 학습 가능한 파라미터\n",
    "- $ f $: 활성화 함수 (tanh 또는 ReLU 등)\n",
    "\n",
    "##### ✅ 특징\n",
    "- 시간 정보를 기억 (Memory)\n",
    "- 반복 구조로 구현됨\n",
    "- Gradient Vanishing 문제가 있음 (→ LSTM/GRU 등장 배경)\n",
    "\n",
    "#### 2️⃣ Recurrent Layers (순환 계층)\n",
    "##### ✅ 정의\n",
    "Recurrent Layer는 여러 Recurrent Neuron이 모여서 **시퀀스 전체를 처리하는 계층**입니다.  \n",
    "입력 시퀀스를 시간 순서대로 순차 처리하며, 매 시점마다 뉴런이 공유된 파라미터를 사용합니다.\n",
    "\n",
    "##### ✅ 대표적인 종류\n",
    "| 계층 종류       | 특징 |\n",
    "|----------------|------|\n",
    "| SimpleRNN      | 기본 순환 구조. 짧은 시퀀스에 적합 |\n",
    "| LSTM           | 장기 의존성 학습 가능. 게이트 3개 포함 |\n",
    "| GRU            | 구조가 간단한 LSTM. 게이트 2개 사용 |\n",
    "| BidirectionalRNN | 시퀀스를 정방향과 역방향 모두 처리 |\n",
    "\n",
    "#### 3️⃣ RNN vs Feedforward NN 비교\n",
    "| 항목              | Feedforward NN       | Recurrent NN         |\n",
    "|-------------------|----------------------|-----------------------|\n",
    "| 입력 구조         | 고정된 입력 벡터     | 시퀀스 (시간 의존적) |\n",
    "| 파라미터 공유     | X                    | O (시간축 따라 공유) |\n",
    "| 과거 정보 사용    | 불가능               | 가능 (메모리 존재)   |\n",
    "| 대표 활용 분야    | 이미지 분류 등       | NLP, 시계열 분석 등   |\n",
    "\n",
    "\n",
    "#### 4️⃣ 사용 예시\n",
    "- 자연어 처리 (번역, 감성 분석, 챗봇 등)\n",
    "- 시계열 예측 (주가, 날씨 등)\n",
    "- 음악, 음성, 동작 인식 등 시간 순서가 중요한 문제\n",
    "\n",
    "#### 5️⃣ 추가 개념: `return_sequences`\n",
    "| 파라미터            | 의미 |\n",
    "|---------------------|------|\n",
    "| `return_sequences=False` | 마지막 시점의 출력만 반환 (기본값) |\n",
    "| `return_sequences=True`  | 모든 시점의 출력 반환 (시퀀스 → 시퀀스) |\n",
    "\n",
    "44page 이후에는 전체적으로 graph 및 code 들이 나와 있는데 설명이 되어 있지 않아서 정확한 이해가 어려움."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2028dd70-cd68-47e0-b9c5-0438aa04fa22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
