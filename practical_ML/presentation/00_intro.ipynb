{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "413e77fc-0663-414d-bdb6-8ae1c480b634",
   "metadata": {},
   "source": [
    "Hyperparameter tuning in Deep Learning :  \n",
    "● Number of Layers  \n",
    "● Number of Neurons  \n",
    "● Types of Layers  \n",
    "● Activation Functions  \n",
    "● Batch Sizes  \n",
    "● Initialization  \n",
    "● Batch Normalization and Layer Normalization  \n",
    "● Optimizers  \n",
    "● Learning Rate and Scheduling  \n",
    "● Regularization  \n",
    "● Dropout  \n",
    "  \n",
    "Improve Deep Learning Algorithm Performance  \n",
    "❏ Get Data (Deep learning get better)  \n",
    "❏ Generate Data. (Data Augmentation / adding noise, call adding jitter)  \n",
    "❏ Rescale Data. (Rescale data to the bounds of activation functions.)  \n",
    "❏ Transform Your Data. (Guesstimate the univariate distribution )  \n",
    "❏ With Algorithms ( Steals from literature)  \n",
    "❏ With Algorithm Tuning (tuning hyperparameters)  \n",
    "❏ Improve Performance With Ensembles.  \n",
    "  \n",
    "Generate Data. (Data Augmentation / adding noise, call adding jitter)  \n",
    "Image :  \n",
    "Perspective Skewing  \n",
    "Elastic Distortions  \n",
    "Rotating  \n",
    "Shearing  \n",
    "Cropping  \n",
    "Mirroring  \n",
    "Colour distortion  \n",
    "Ex: https://augmentor.readthedocs.io/en/master/  \n",
    "Text:  \n",
    "Generate conditional synthetic text samples  \n",
    "Ex: https://github.com/openai/gpt-2  \n",
    "  \n",
    "Rescale Data  \n",
    "● Normalized to 0 to 1.  \n",
    "● Rescaled to -1 to 1.  \n",
    "Transform Your Data (Numerical data):  \n",
    "● Like a skewed Gaussian, adjusting the skew with a Box-Cox transform.  \n",
    "● Like an exponential distribution, adjusting with a log transform  \n",
    "  \n",
    "With Algorithm Tuning (tuning hyperparameters):  \n",
    "● Diagnostics.  \n",
    "● Weight Initialization.  \n",
    "● Learning Rate.  \n",
    "● Activation Functions.  \n",
    "● Batches and Epochs.  \n",
    "● Regularization.  \n",
    "● Optimization and Loss.  \n",
    "● Early Stopping.  \n",
    "  \n",
    "Diagnostics:  \n",
    "Get insight into the learning behavior of your model  \n",
    "Plotting training vs validation over epoches  \n",
    "Overfitting;  \n",
    "If training is much better than the validation;  \n",
    "use regularization technique.  \n",
    "Underfitting;  \n",
    "If training and validation are both low;  \n",
    "review all network settings  \n",
    "If an inflection point  \n",
    "when training goes above the validation;  \n",
    "use early stopping  \n",
    "  \n",
    "Choosing the right Activation Function :  \n",
    "➢ The softmax function is often in the final layer of a deep learning.  \n",
    "➢ ReLU function is only in the hidden layers (intermediate layers)  \n",
    "➢ ReLU function is a general activation function in most cases.  \n",
    "➢ In presence of dead neurons; the leaky ReLU function is the best choice  \n",
    "➢ Always begin with using ReLU function and then move over to other activation  \n",
    "functions in case ReLU doesn’t provide with optimum results)  \n",
    "More info here :  \n",
    "https://www.analyticsvidhya.com/blog/2025/01/fundamentals-deep-learning-activation-functions-when-to-use-them/  \n",
    "https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7  \n",
    "https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-netw  \n",
    "  \n",
    "Batch size  \n",
    "❏ Larger batch sizes result in faster progress in training, but don't converge as fast.  \n",
    "❏ Smaller batch sizes train slower, but can converge faster.  \n",
    "❏ Large batch size ignore details in training while small batch size go to details. It's definitely problem dependent.   \n",
    "❏ Large batch size is good at very distinguable problems and requires enough memory size while small batch works perfect for very complex and similar classes.  \n",
    "Be safe using a batch size of 8,16,32,64 which are pretty standar  \n",
    "  \n",
    "Learning rate   \n",
    "Start with a high learning rate then reduce it : perfect.   \n",
    "Learning Rate Schedule For Training Models  \n",
    "Decrease the learning rate gradually based on the epoch. (argument called decay)  \n",
    "Decrease the learning rate using punctuated large drops at specific epochs.  \n",
    "Cyclical Learning Rates  \n",
    "More info here :  \n",
    "https://github.com/mhmoodlan/cyclic-learning-rate  \n",
    "https://brandonmorris.dev/2018/06/24/mastering-the-learning-ra  \n",
    "  \n",
    "Learning rate is coupled with : Number of training epochs, Batch size and Optimization method  \n",
    "  \n",
    "Some Deep Learning Methods : ( But not limited to )  \n",
    "❑ Image processing :  \n",
    "  ❑ Convolutional Neural Network  \n",
    "  ❑ Inception ResNet  \n",
    "  ❑ Fast R-CNN  \n",
    "  ❑ U-Net  \n",
    "  ❑ LayoutLMv3\n",
    "\n",
    "❑ Text Mining : LLMS  \n",
    "  ❑ HAN  \n",
    "  ❑ FastText  \n",
    "  ❑ Transformers (Google BigBird , BERT, GPT-3, RoBERTa, XLM, DistilBert, XLNet…)  \n",
    "  ❑ LLMs  \n",
    "  \n",
    "❑ Numerical Data :  \n",
    "  ❑ TabTransformer : Time2Vec  \n",
    "  \n",
    "Machine learning steps  \n",
    "• Pre Processing Steps (IMPORTANT STEP):  \n",
    "  • Image processing :\n",
    "    ❑ Channel Selection (RGB, HSV , YIQ , ….)  \n",
    "    ❑ Noise Removal & Image Enhancement  \n",
    "    ❑ Image Augmentation ( for Deep Learning solutions)  \n",
    "  • Text Mining :  \n",
    "    ❑ Language Detection  \n",
    "    ❑ Gibberish detection  ( No- sense phrase, un-related topics ,… )  \n",
    "    ❑ Expanding Contractions  \n",
    "    ❑ Remove special characters(Symbols, HTML tags ,... )  \n",
    "    ❑ Text Augmentation(bigscience/T0ppGPT3 OpenAI)  \n",
    "  • Numerical Data :  \n",
    "    ❑ Outlier Detection  \n",
    "    ❑ Missing Values  \n",
    "    ❑ Analysis of Relevance and Redundancy (Efficient Feature Selection)  \n",
    "    ❑ Categorical conversion  \n",
    "    ❑ Convert a Time Series  \n",
    "    ❑ Rescale Your Data ( Fit for activation functions)  \n",
    "    ❑ Transform Your Data ( skewed Gaussian, exponential  distribution)  \n",
    "  \n",
    "Machine learning accuracy  \n",
    "• Improve Learning Precision (IMPORTANT):  \n",
    "  • Improve Performance With Data.  \n",
    "    • More Data, Invent Data  \n",
    "    • Resampling Methods  \n",
    "  • Improve Performance With Algorithms.  \n",
    "    • Spot-Check Algorithms  \n",
    "    • Steal From Literature  \n",
    "  • Improve Performance With Algorithm Tuning**.  \n",
    "    • Diagnostics.  \n",
    "      • Over-fitting , Under-fitting  \n",
    "    • Weight Initialization (Fine- tune; Transfer Learning).  \n",
    "    • Learning Rate.  \n",
    "    • Activation Functions.  \n",
    "    • Batches and Epochs, Early Stopping.  \n",
    "    • Regularization, Optimization and Loss.  \n",
    "  • Improve Performance With Ensembles.  \n",
    "  \n",
    "Machine learning settings  \n",
    "• Efficient Software Settings:  \n",
    "  • Python as Programming Language and Anaconda as data science library  \n",
    "  • Linux Ubuntu as an efficient OS platform  \n",
    "• Machine Learning Packages :  \n",
    "  • Sklearn , skimage (conventional machine learning)  \n",
    "  • Tensorflow (Mostly, in this course !! Building ML models involves this approach)  \n",
    "  • Keras, PyTorch , MxNet  \n",
    "• Efficient Hardware Settings:  \n",
    "  • Training with GPU support (cuDNN, NVIDIA CUDA)  \n",
    "  • Speed up in certain floating point operations (AVX instruction)  \n",
    "  • Using Bazel a build tool to install TensorFlow package dependencie  \n",
    "  \n",
    "Machine learning deployment  \n",
    "• Deploy Models; Production & @Scale : (IMPORTANT)  \n",
    "  • Docker to Build Source code :  \n",
    "    • Docker image, Docker containers  \n",
    "      • Building Dockerfile, Requirement.txt (OS and code dependencies)  \n",
    "    • Pushing Docker Image to a private repository  \n",
    "  • User interface:  \n",
    "    • Up and running  Tensorflow serving (with REST, gRPC)  \n",
    "    • Creating Web APIs ( API Gateway, RESTful API with Flask )  \n",
    "  • Multiple concurrent request :  \n",
    "    • Building hosting services with uWSGI application  \n",
    "      • Nginx to handle actual client requests and proxy them to the uWSGI server.  \n",
    "      • Gunicorn (Python WSGI server that runs Python web application)  \n",
    "  \n",
    "Applied Machine learning  \n",
    "In general, steps in actions would be but not limited to :  \n",
    "1- Search related topics in Gemini / Deepseek / ChatGPT /Grok search engines.  \n",
    "2- Filter out search into resource pages (Below Link) with appropriate phrase  \n",
    "3- Review ( not details ) related article in Google scholar and/or open- access Arxiv page  \n",
    "4- Borrow sample code(s) from GitHub or mostly from resource pages (Below Links)  \n",
    "5- Hyper parameter and Fine-tuning for at least two potential solutions (Keep looking and seeking for pre-trained model in near business problem in different resources like LLMs,TensorFlow Hub , HuggingFace , .... )  \n",
    "6- Having first intuitive primary version as proof of concept (PoC)  \n",
    "7- Code modification to be in professional format (Class- Function , naming, ... )  \n",
    "8- Code optimization (Appropriate data structure like json, hash data, trained model in Protocol buffer, I/O throughput a large scale \"tf.data.Dataset\"  tf.keras.utils.image_dataset_from_directory, tf.io.gfile.glob,..., install and build packages from source,...)  \n",
    "9 - Deploy model at scale in production ( Docker - Flask /FastAPI - Gunicorn - ALB -ELB - TFX - Cloud Computing \" GCP . AWS. Azure\" )  \n",
    "10 - Quality Analysis  \n",
    "  \n",
    "Few Potential mini-project  \n",
    "1 - House price Prediction with Deep learning ( from unclean data from different resources with required preprocessing steps)  \n",
    "Example : merging unclean data NYC Complaint Data Historic and Rolling sales data  \n",
    "from  \n",
    "https://www.kaggle.com/agustinsellanes/nypd-complaint-data-historic  \n",
    "and  \n",
    "https://www1.nyc.gov/site/finance/taxes/property-rolling-sales-data.page  \n",
    "2 One-Shot Learning with Siamese Network ( Recommendation, Learning by comparison )  \n",
    "Example :  \n",
    "https://tech.wayfair.com/data-science/2020/03/the-visual-complements-model-vics-complementary-product-recommendations-from-visual-cues/  \n",
    "3- Histopathological image classification with ResNet and Inception deep learning approaches (From limited and noisy images)  \n",
    "4- Medical Segmentation with U-net deep learning method (Limited data and using Transposed convolution and copy layers)  \n",
    "5- Sentiment Analysis with Transformers by HuggingFace ( preprocessing step included)  \n",
    "6 - Semantic Textual Similarity Using Transformer (HuggingFace)  \n",
    "https://towardsdatascience.com/semantic-similarity-using-transformers-8f3cb5bf66d6  \n",
    "7- Multi-Label Classification using Transormer  \n",
    "https://towardsdatascience.com/transformers-for-multilabel-classification-71a1a0daf5e1  \n",
    "8 - SimpleTransfomers and classification -  \n",
    "https://towardsdatascience.com/multi-label-classification-using-bert-roberta-xlnet-xlm-and-distilbert-with-simple-transformers-b3e0cda12ce5   \n",
    "9- Named Entity Recognition (NER)  \n",
    "- Example   \n",
    "https://skimai.com/how-to-fine-tune-bert-for-named-entity-recognition-ner/  \n",
    "10 - Community Detection in Social Networks with Deep Learning and Statistical methods.  \n",
    "11- Image similarity ML Algorithms and steps to Detect Duplicate image  \n",
    "    1- LayoutLM:  \n",
    "       LayoutLM: Pre-training of Text and Layout for Document Image Understanding  \n",
    "       https://arxiv.org/pdf/1912.13318.pdf  \n",
    "       https://huggingface.co/docs/transformers/model_doc/layoutlm  \n",
    "       https://towardsdatascience.com/fine-tuning-layoutlm-v2-for-invoice-recognition-91bf2546b19e  \n",
    "       SentenceTransformer :  \n",
    "       https://towardsdatascience.com/semantic-similarity-using-transformers-8f3cb5bf66d6  \n",
    "    2- Deep Image Search :  \n",
    "       https://github.com/TechyNilesh/DeepImageSearch  \n",
    "    3- https://github.com/markusressel/py-image-dedup  \n",
    "12- Siamese Network :  \n",
    "    Train (Anchor, Positive, Negative)  \n",
    "      https://keras.io/examples/vision/siamese_network/  \n",
    "    Build three sub-network and its corresponding embedding  \n",
    "      https://www.aboutwayfair.com/tech-innovation/the-visual-complements-model-vics-complementary-product-recommendations-from-visual-cues  \n",
    "    Store all embedding in Database.  \n",
    "    Similarity measurement (Input versus to all ) using For example; ANNOY https://pypi.org/project/annoy/  \n",
    "13- Explore OmniXAI with different use cases  \n",
    "    Interesting Explainable Package:  \n",
    "      https://github.com/salesforce/OmniXAI  \n",
    "    Simple Tabular:  \n",
    "      https://github.com/salesforce/OmniXAI/blob/main/tutorials/tabular_regression.ipynb  \n",
    "    Classification based:  \n",
    "      https://github.com/salesforce/OmniXAI/blob/main/tutorials/tabular_classification.ipynb\n",
    "      https://sfr-omnixai-demo.herokuapp.com/\n",
    "\n",
    "LLMs(Large Language Models)  \n",
    "Engage with these LLM models through prompt engineering.  \n",
    "https://learnprompting.org/docs/intro (Recommended)  \n",
    "https://www.promptingguide.ai  \n",
    "https://www.cloudskillsboost.google/paths/118  \n",
    "Prompt engineering : Design effective prompting techniques that interface with LLMs and other tools.    \n",
    "LangChain :  \n",
    "https://python.langchain.com/en/latest/index.html  \n",
    "Crew AI :  \n",
    "https://github.com/crewAIInc/crewAI  \n",
    "Transformers Agent :  \n",
    "https://huggingface.co/docs/transformers/transformers_agents  \n",
    "LlamaIndex :  \n",
    "https://github.com/jerryjliu/llama_index  \n",
    "ChainLit: Build and share LLM apps.  \n",
    "https://docs.chainlit.io/overview  \n",
    "VLLM : Serving LLM  \n",
    "https://github.com/vllm-project/vllm  \n",
    "H2O LLM Studio : GUI designed for fine-tuning state-of-the-art large  \n",
    "language models (LLMs)  \n",
    "https://github.com/h2oai/h2o-llmstudio  \n",
    "\n",
    "\n",
    "Opinion\n",
    "The following information is the most interesting things for me during the session since I have to learn those things continuously and I thought that there are too much topics to learn...\n",
    "  \n",
    "Data Science Skill   \n",
    "● English Proficiency ( Reading and Listening Skills in Particular )  \n",
    "● Applied Maths , Statistics and Probability Principles (Mostly Undergrad courses)  \n",
    "● Linux Operating System (Ubuntu in particular)  \n",
    "● Python, Rust Programming Language (Basic and Python, Rust in Data Science )  \n",
    "● Applied Machine Learning Concept (Conventional, Deep Learning, Gen AI)  \n",
    "● Hands-on Experience in Sklearn, TensorFlow , PyTorch , Keras (High-level API of TensorFlow 2 ) , MxNet , …, LangChain, LangGraph, Crew AI, Transformers Agent, LlamaIndex, AWS bedrock  \n",
    "● Prompt Engineering (working with LLMs)  \n",
    "● Program Optimization   \n",
    "  ■ Package (TensorFlow ) — > Install from Source  \n",
    "  ■ GPU programming - CUDA / cuDNN  \n",
    "  ■ Server / Client ( TensorFlow Serving - REST & gRPC )  \n",
    "  ■ CPU Execution (SSE & AVX Vectorization)  \n",
    "● Deploy model and Containerization using Docker  \n",
    "  ○ Docker, Docker Compose, Docker swarm , Kubernetes  \n",
    "  ○ Multiple Concurrent Requests (Uvicorn, Unicorn, Gunicorn, ALB, ELB , ….)  \n",
    "  ○ Chainlit, Streamlit, gradio (Web APP, Presentation for Business Sector )  \n",
    "  ○ EntryPoint Interface ( API Gateway , FastAPI, Flask , … )  \n",
    "● Database and Data Storage :  \n",
    "  ○ Cloud Storage (AWS S3 Bucket , ….)  \n",
    "  ○ SQL , NoSQL  \n",
    "● Big Data Framework :  \n",
    "  ○ Hadoop, Spark, Kubernete, SnowFlake, ….  \n",
    "● Cloud Computing :  \n",
    "  ○ Google Cloud Platform (GCP ) , Amazon Web Services (AWS), Azure  \n",
    "● Version Control :  \n",
    "  ○ Git, GitHub, BitBucket, CodeCommit, …  \n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c28f1c1-4823-4648-b31f-bba25bf4af88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19953b67-fa85-4151-a9b8-ae24b6f8fb49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717e4c9f-0cb6-480d-8824-b63a0cad7cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d70f05-c9cb-4faf-9aa0-ca8a8429c456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563a0f73-09d4-475d-8deb-7ac0bae095d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
