{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b8f1284-dfc9-4157-8381-452ab336d4e8",
   "metadata": {},
   "source": [
    "# Lesson 2\n",
    "\n",
    "### huggingface : 트레이닝 된 모델을 다운로드해서 사용할 수 있는 사이트. \n",
    "  \n",
    "### vaex.io, polars, apache spark, modin 등 -> panda는 큰 데이터셋이나 고속처리에는 한계가 있다. \n",
    "| 라이브러리            | 핵심 특징                                          |\n",
    "| ---------------- | ---------------------------------------------- |\n",
    "| **Vaex**         | 메모리에 올리지 않고도 빠르게 처리 가능 (Out-of-Core 처리)        |\n",
    "| **Polars**       | Rust 기반으로 매우 빠름, 병렬 처리에 최적화                    |\n",
    "| **Apache Spark** | 분산 처리, 빅데이터용, 클러스터 기반                          |\n",
    "| **Modin**        | pandas 코드 그대로 사용 가능, 백엔드 병렬화 (`Ray` or `Dask`) |\n",
    "  \n",
    "### Logits Layer란?\n",
    "- Logits는 딥러닝 모델의 **출력층(마지막 레이어)**에서 나오는 정규화되지 않은 출력 값입니다.\n",
    "- 보통 Dense (또는 Linear) 층을 거친 후 Softmax 또는 Sigmoid를 적용하기 직전의 출력값을 말해요\n",
    "- 왜 Logits를 쓰나요?\n",
    "  - Softmax/Sigmoid 함수는 모델 밖에서 적용할 수도 있음\n",
    "  - 예: loss function에서 내부적으로 softmax를 적용하는 경우 (예: CategoricalCrossentropy(from_logits=True))\n",
    "  - 수치적으로 안정성이 높음\n",
    "  - 큰 값들을 softmax에 직접 넣으면 overflow 문제가 생길 수 있어요. 그래서 logits 그대로 쓰는 게 안정적입니다.\n",
    "\n",
    "### CNN에 Input으로 tuple 사용하는 이유\n",
    "- immutable\n",
    "- 데이터를 학습한 후에, 데이터가 변환되면 안되는데, tuple의 특성상 데이터 변경이 불가능하므로.\n",
    "\n",
    "### Learning Rate란?\n",
    "- Learning Rate는 딥러닝 모델이 학습할 때, 가중치를 얼마나 크게 조정할지 결정하는 값입니다.\n",
    "| 항목      | 설명                         |\n",
    "| ------- | -------------------------- |\n",
    "| 역할      | 가중치 업데이트 크기 조절             |\n",
    "| 너무 클 때  | 발산 가능                      |\n",
    "| 너무 작을 때 | 느린 수렴                      |\n",
    "| 보통 값    | 0.01, 0.001 등              |\n",
    "| 향상 기법   | Learning Rate Scheduler 사용 |\n",
    "\n",
    "\n",
    "### Scaling vs Normalization\n",
    "- 머신러닝과 딥러닝에서 **입력 데이터를 전처리**할 때 사용하는 중요한 개념 두 가지입니다.\n",
    "\n",
    "#### Scaling (스케일링)\n",
    "- 데이터의 **크기를 일정한 범위로 조정**하는 작업입니다.\n",
    "\n",
    "##### 대표 기법\n",
    "- **Min-Max Scaling**\n",
    "  $\n",
    "  x' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}\n",
    "  $\n",
    "  → 결과: 0과 1 사이로 조정됨\n",
    "\n",
    "- **Z-score Scaling (Standardization)**\n",
    "  $\n",
    "  x' = \\frac{x - \\mu}{\\sigma}\n",
    "  $\n",
    "  → 평균 0, 표준편차 1 분포로 변환\n",
    "\n",
    "##### 목적\n",
    "- 단위 차이를 제거해 **모델의 학습을 빠르고 안정적으로** 함\n",
    "\n",
    "#### Normalization (정규화)\n",
    "- **각 샘플(행)**의 벡터 크기를 **1로 조정**하는 작업입니다.\n",
    "\n",
    "##### 대표 기법\n",
    "- **L2 Normalization**\n",
    "  $\n",
    "  x' = \\frac{x}{\\|x\\|_2}\n",
    "  $\n",
    "\n",
    "##### 목적\n",
    "- 벡터의 방향성만 남기고 **크기 차이를 제거**  \n",
    "- 텍스트, 이미지 등의 **유사도 기반 분석**에 적합\n",
    "\n",
    "##### 예시\n",
    "\n",
    "| 데이터 | 스케일링 후 (Min-Max) | 정규화 후 (L2) |\n",
    "|--------|------------------------|----------------|\n",
    "| [100, 200, 300] | [0.0, 0.5, 1.0] | [0.27, 0.53, 0.80] |\n",
    "\n",
    "### 수업 중 아래 세 코드파일 리뷰함.\n",
    "session02_classification_regression_modified\n",
    "session02_extra_Mnist_ConvNet_modified\n",
    "session02_keras_introduction_modified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb61e8-30ec-44a2-9d9a-80ba309058ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
