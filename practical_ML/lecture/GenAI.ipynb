{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5c2d6d9-34a9-43df-9d01-2779f41841a5",
   "metadata": {},
   "source": [
    "# Gen AI. 01. \n",
    "\n",
    "### H2O.ai : Enterprises use H2O platform to treat big data instead of sklearn. \n",
    "\n",
    "### What is LangChain, LangGraph, CrewAI, AWS bedrock, Chainlit? \n",
    "These are very important tool at this moment in the field of Gen AI. \n",
    "\n",
    "#### LangChain\n",
    "LangChain is an open-source framework designed to help developers build powerful applications that integrate language models (like GPT-4) with external data sources, tools, and custom workflows. It makes it easier to create complex, AI-powered systems that go beyond basic question-answering.\n",
    "\n",
    "##### What Does LangChain Do?\n",
    "- LangChain helps with:\n",
    "  1. Connecting LLMs to real-world data\n",
    "     - Use documents (PDFs, websites, databases) as context for the model.\n",
    "     - Example: Ask questions about a long report or your company’s knowledge base.\n",
    "  2. Tool use / agent capabilities\n",
    "     - Lets LLMs call tools like a calculator, web search, or even run Python code.\n",
    "     - Example: “What’s 23% of the average of these numbers?” → LLM uses a calculator tool.\n",
    "  3. Multi-step workflows\n",
    "     - Chains together multiple steps (e.g., summarize → analyze → generate report).\n",
    "     - Example: Extract key insights from 10 documents and write an email summary.\n",
    "  4. Memory\n",
    "     - Maintains conversation state (e.g., remembering your name, topic history).\n",
    "- Key components of LangChain\n",
    "| Component      | What It Does                                               |\n",
    "| -------------- | ---------------------------------------------------------- |\n",
    "| **LLMs**       | Interfaces with models like GPT-4, Claude, etc.            |\n",
    "| **Chains**     | Sequences of operations (e.g., LLM call → tool → LLM).     |\n",
    "| **Agents**     | LLMs that decide what action to take next (tool use, etc). |\n",
    "| **Memory**     | Stores past conversations or results.                      |\n",
    "| **Retrievers** | Pulls relevant chunks of data (often from a vector store). |\n",
    "| **Tools**      | External functions (calculator, browser, APIs).            |\n",
    "\n",
    "#### LangGraph\n",
    "LangGraph is a library built on top of LangChain that lets you create stateful, multi-agent, and multi-step workflows using a graph-based architecture. It’s designed for building advanced AI systems where different parts (LLMs, tools, memory, and agents) interact in a controlled, structured way — much like a finite state machine or directed graph.  \n",
    "In Simple Terms  \n",
    "If LangChain is like giving GPT tools and memory, LangGraph is about organizing how they talk to each other, step by step, with branches, loops, and logic.  \n",
    "\n",
    "##### Key concepts\n",
    "| Concept         | Description                                                            |\n",
    "| --------------- | ---------------------------------------------------------------------- |\n",
    "| **Graph**       | Represents a workflow — nodes (steps) and edges (transitions).         |\n",
    "| **Node**        | A unit of computation (e.g. calling a model, running a tool).          |\n",
    "| **Edge**        | A condition that tells LangGraph where to go next based on output.     |\n",
    "| **State**       | A memory-like object that tracks info during the workflow.             |\n",
    "| **Multi-agent** | Supports having several AI agents collaborate, each with its own role. |\n",
    "\n",
    "##### Why Use LangGraph?\n",
    "- You want complex control flows: loops, branching, conditional logic.\n",
    "- You’re building multi-agent systems (e.g., a planner + executor).\n",
    "- You want to manage conversation state or task state explicitly.\n",
    "- You’re building AI systems with well-defined steps and logic flows.\n",
    "\n",
    "##### Example Use Cases\n",
    "- A customer service bot that decides whether to route to billing, tech support, or self-help.\n",
    "- Research agents that plan → search → summarize → report.\n",
    "- Multi-step form filling, like onboarding flows that depend on previous user answers.\n",
    "- AI project manager that delegates tasks to specialized agents (writer, editor, researcher).\n",
    "\n",
    "##### LangGraph vs LangChain\n",
    "| Feature              | LangChain                        | LangGraph                             |\n",
    "| -------------------- | -------------------------------- | ------------------------------------- |\n",
    "| Workflow structure   | Linear or agent-based            | Graph-based (nodes + edges)           |\n",
    "| Control flow         | Limited branching                | Full branching + loops + conditions   |\n",
    "| Suitable for         | Chatbots, retrieval-augmented QA | Multi-agent, decision-based workflows |\n",
    "| Level of abstraction | Higher (easier to start)         | Lower (more control/flexibility)      |\n",
    "\n",
    "#### CrewAI\n",
    "CrewAI is a Python-based open-source framework for orchestrating collaborative AI agents, enabling them to form “crews” that autonomously divide work, share insights, and solve complex tasks together.\n",
    "  \n",
    "##### Core Concepts\n",
    "1. Crews: Collections of role-based agents (e.g. researcher, writer, analyst) assigned to a shared mission. A \"manager\" agent can organize, delegate, and coordinate tasks among team members\n",
    "2. Agents: Autonomous AI actors with:\n",
    "  - Defined roles and goals\n",
    "  - Access to tools (search, code interpreter, image generation, etc.)\n",
    "  - Ability to autonomously make decisions, ask questions, or reassign tasks\n",
    "    \n",
    "3. Tasks/Processes: Structured units of work. Crews support both sequential and hierarchical workflows:\n",
    "   - Sequential: Step-by-step baton-like task passing (e.g. research → writing → editing)\n",
    "   - Hierarchical: Manager delegates subtasks, consolidates results \n",
    "\n",
    "4. Flows: Complement crews by offering low-level, deterministic control of workflows—driven by events, states, and precise control flows—and can be combined with crews for flexibility \n",
    "\n",
    "##### Why Use CrewAI?\n",
    "- Distributed intelligence: Agents specialize and collaborate to handle complex tasks more effectively than one LLM alone .\n",
    "- Enterprise-ready: High performance, observability, tool integration, and support for deployment at scale \n",
    "- Agile and flexible: Mix high-level crew autonomy with explicit control via flows for production-grade reliability and repeatability \n",
    "- Rich tooling support: Over 40 pre-built tools (web scraping, file I/O, database access, image generation, RAG, code interpreters) plus easy custom tool integration \n",
    "\n",
    "#### AWS bedrock\n",
    "Amazon Bedrock is a fully managed service by AWS that lets you build and scale generative AI applications using foundation models (FMs) from leading AI providers — without needing to manage infrastructure or train models from scratch.\n",
    "\n",
    "##### What Does AWS Bedrock Do?\n",
    "With Bedrock, you can:\n",
    "- Access powerful foundation models (like GPT, Claude, Titan, etc.)\n",
    "- Use them via a single API, even across multiple model providers\n",
    "- Customize models securely using your data with techniques like fine-tuning or RAG (Retrieval-Augmented Generation)\n",
    "- Build AI-powered apps for text generation, chatbots, summarization, search, code generation, and more\n",
    "\n",
    "##### Key features\n",
    "| Feature                         | Description                                                                                                                  |\n",
    "| ------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Foundation Models**           | Access models from **Anthropic (Claude)**, **AI21**, **Meta (LLaMA 2)**, **Cohere**, **Stability AI**, and **Amazon Titan**. |\n",
    "| **Model Access via API**        | Use a consistent API across all supported models.                                                                            |\n",
    "| **No infrastructure to manage** | No servers or GPUs to set up — fully managed and scalable.                                                                   |\n",
    "| **Customization**               | Personalize models using **your data** with *RAG* or *fine-tuning*.                                                          |\n",
    "| **Security & compliance**       | Run in your own **AWS environment**, with built-in privacy and governance.                                                   |\n",
    "\n",
    "#### Chainlit\n",
    "Chainlit is an open-source, Python-first framework for creating production-ready conversational AI applications—think ChatGPT-style interfaces—with just a few lines of code .\n",
    "\n",
    "##### Why Chainlit?\n",
    "- Lightning-fast prototyping — fire up a chat app in minutes, thanks to simple decorators like @cl.on_message and @cl.on_chat_start \n",
    "- Frontend stripped away — no need to build UIs with HTML/CSS/JS. You write backend logic in Python and Chainlit handles a full-featured web UI with streaming, rich formats, buttons, file uploads, etc.\n",
    "- Observability & analytics built-in — record user interactions, intermediate steps, LLM API calls, and performance metrics—all without extra setup\n",
    "- Modular integrations — works out of the box with popular AI tools like LangChain, OpenAI, LlamaIndex, Mistral, Semantic Kernel, and even stream-first frameworks like LangGraph\n",
    "\n",
    "##### How it works\n",
    "Chainlit consists of two parts:  \n",
    "| Component                      | Role                                                                                                                                |\n",
    "| ------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Python backend** (your file) | Define event handlers for chat logic (`@cl.on_message`, `@cl.on_chat_start`), tool usage, etc.  |\n",
    "| **Prebuilt React frontend**    | Automatically launched via WebSocket, includes support for streaming responses, message formatting, code blocks, buttons, and more  |\n",
    "\n",
    "### Hallucination in LLMs\n",
    "In the context of LLMs (Large Language Models) like GPT, Claude, or LLaMA, hallucination refers to the phenomenon where a model generates text that sounds plausible but is factually incorrect, made up, or unsupported by real data.\n",
    "\n",
    "### Guardrail\n",
    "GenAI Guardrail(생성형 AI 가드레일)이란, 생성형 AI(Generative AI)가 안전하고 신뢰성 있는 방식으로 동작하도록 제어하거나 제한하는 모든 기술적·논리적 장치를 의미합니다. 쉽게 말하면, “AI가 거짓말하거나 이상한 말 안 하게 안전벨트 채우는 기술”이에요.\n",
    "\n",
    "#### 왜 필요한가?\n",
    "LLMs는 문장을 \"잘\" 만들지만 항상 \"맞는 말\"을 하는 건 아니죠. 그래서 다음과 같은 위험이 있습니다:  \n",
    "| 위험 요소                    | 예시                       |\n",
    "| ------------------------ | ------------------------ |\n",
    "| **Hallucination (환각)**   | “파리가 아프리카 수도예요.” ❌       |\n",
    "| **Toxic output (유해 발언)** | 인종차별, 성차별, 증오 발언 등       |\n",
    "| **Bias (편향)**            | 특정 성별, 지역, 종교에 대한 편견 반영  |\n",
    "| **Prompt injection**     | 악의적인 입력으로 시스템을 속이는 공격    |\n",
    "| **Data leakage**         | 민감 정보 노출 (예: 실명, 전화번호 등) |\n",
    "\n",
    "#### GenAI Guardrail의 핵심 기능\n",
    "| 기능                             | 설명                   | 기술 예시                                         |\n",
    "| ------------------------------ | -------------------- | --------------------------------------------- |\n",
    "| ✅ **Content filtering**        | 욕설, 성적인 내용, 차별 표현 차단 | OpenAI Moderation API, Google Perspective API |\n",
    "| ✅ **Prompt validation**        | 이상하거나 공격적인 프롬프트 감지   | Rebuff, Llama Guard                           |\n",
    "| ✅ **Output moderation**        | AI가 생성한 결과물 검사       | Guardrails AI, NeMo Guardrails                |\n",
    "| ✅ **Fact-checking**            | AI가 말한 사실 검증         | RAG 기반 출처 연결, Graph-based QA                  |\n",
    "| ✅ **Control flow enforcement** | 응답 형식, 길이, 순서 제한     | `Guardrails`, `PromptLayer`, custom schema    |\n",
    "| ✅ **Tool use supervision**     | AI가 사용하는 툴이나 API 제한  | LangChain Tool Guard, tool permission wrapper |\n",
    "\n",
    "### Gen AI information \n",
    "https://thegenairevolution.com/\n",
    "\n",
    "### Gen AI 이 job market\n",
    "- 학위 더이상 필요 없음. 왜? 그냥 chatgpt에 물어보면 되니깐.\n",
    "- 대학교들은 GenAI에 대해 신경쓰고 있지 않지만 Job market은 이미 변하는 중이다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae3ec5f-7caf-4454-9661-212bfc8aa51e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
